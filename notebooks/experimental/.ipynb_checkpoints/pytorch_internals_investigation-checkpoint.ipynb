{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# PyTorch ONNX Export Internals Investigation\n",
    "\n",
    "This notebook investigates PyTorch's internal mechanisms for ONNX export to find new approaches for hierarchy preservation.\n",
    "\n",
    "## Key Findings from Source Code Analysis\n",
    "\n",
    "1. **`_trace_module_map`**: Already captures complete module hierarchy during export\n",
    "2. **ONNX Scope Functions**: PyTorch has built-in functions for scope-based naming\n",
    "3. **Metadata Infrastructure**: Existing mechanisms for attaching metadata to graphs\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Instead of post-processing ONNX files, we can hook into PyTorch's existing ONNX export infrastructure to inject hierarchy metadata during export.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.jit\nfrom transformers import AutoModel, AutoTokenizer\nimport tempfile\nimport os\nimport json\nfrom pathlib import Path\nimport onnx\nfrom typing import Dict, Any, List, Tuple\nimport inspect\n\n# ‚úÖ UNIVERSAL APPROACH: Use any small HuggingFace model for experimentation\n# This follows CARDINAL RULE #1 - NO HARDCODED LOGIC\nmodel_name = \"prajjwal1/bert-tiny\"  # Small model for fast testing\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Sample input - universal approach\ntext = \"Hello world\"\ninputs = tokenizer(text, return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\nattention_mask = inputs[\"attention_mask\"]\n\nprint(f\"Model: {type(model).__name__}\")\nprint(f\"Input shape: {input_ids.shape}\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Investigate PyTorch's Hierarchy Mechanisms\n",
    "\n",
    "Let's test the key discoveries from our deep investigation of PyTorch's JIT internals."
   ]
  },
  {
   "cell_type": "code",
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": "def capture_trace_module_map_during_export():\n    \"\"\"Capture the _trace_module_map during ONNX export to see what PyTorch already tracks.\"\"\"\n    \n    captured_maps = []\n    exported_path = None\n    \n    print(f\"\"\"\nüîç {'='*70}\nüîç INVESTIGATING PYTORCH'S HIERARCHY MECHANISMS\nüîç {'='*70}\n\nüìã METHOD 1: Direct ONNX Export Hook\n{'-' * 50}\"\"\")\n    \n    # Store original ONNX utility functions\n    original_trace_module = getattr(torch.onnx.utils, '_trace', None)\n    original_setup_trace = getattr(torch.onnx.utils, '_setup_trace_module_map', None)\n    \n    def enhanced_trace_hook(*args, **kwargs):\n        \"\"\"Hook to capture tracing information during ONNX export.\"\"\"\n        print(f\"   üé£ ONNX tracing hook called with {len(args)} args\")\n        \n        # Check current _trace_module_map\n        current_map = getattr(torch.jit._trace, '_trace_module_map', None)\n        if current_map:\n            captured_maps.append({\n                'hook_point': 'trace_hook',\n                'map': dict(current_map) if hasattr(current_map, 'items') else current_map,\n                'map_type': type(current_map).__name__,\n                'map_size': len(current_map) if hasattr(current_map, '__len__') else 0\n            })\n            print(f\"   ‚úÖ Captured map with {len(current_map)} entries\")\n        \n        # Call original if exists\n        if original_trace_module:\n            return original_trace_module(*args, **kwargs)\n        return None\n    \n    def enhanced_setup_trace_hook(*args, **kwargs):\n        \"\"\"Hook to capture setup trace information.\"\"\"\n        print(f\"   üîß Setup trace hook called with {len(args)} args\")\n        \n        # Check current _trace_module_map before setup\n        current_map = getattr(torch.jit._trace, '_trace_module_map', None)\n        if current_map:\n            captured_maps.append({\n                'hook_point': 'setup_trace_before',\n                'map': dict(current_map) if hasattr(current_map, 'items') else current_map,\n                'map_type': type(current_map).__name__,\n                'map_size': len(current_map) if hasattr(current_map, '__len__') else 0\n            })\n        \n        # Call original if exists\n        result = None\n        if original_setup_trace:\n            result = original_setup_trace(*args, **kwargs)\n        \n        # Check _trace_module_map after setup\n        updated_map = getattr(torch.jit._trace, '_trace_module_map', None)\n        if updated_map and updated_map != current_map:\n            captured_maps.append({\n                'hook_point': 'setup_trace_after',\n                'map': dict(updated_map) if hasattr(updated_map, 'items') else updated_map,\n                'map_type': type(updated_map).__name__,\n                'map_size': len(updated_map) if hasattr(updated_map, '__len__') else 0\n            })\n            print(f\"   ‚úÖ Setup created enhanced map with {len(updated_map)} entries\")\n        \n        return result\n    \n    # Apply hooks if functions exist\n    if original_trace_module:\n        torch.onnx.utils._trace = enhanced_trace_hook\n        print(\"   üé£ Applied trace hook\")\n    else:\n        print(\"   ‚ö†Ô∏è  No _trace function found to hook\")\n        \n    if original_setup_trace:\n        torch.onnx.utils._setup_trace_module_map = enhanced_setup_trace_hook\n        print(\"   üîß Applied setup trace hook\")\n    else:\n        print(\"   ‚ö†Ô∏è  No _setup_trace_module_map function found to hook\")\n    \n    try:\n        print(f\"\"\"\n   üöÄ Performing ONNX export with hooks...\"\"\")\n        \n        with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp:\n            torch.onnx.export(\n                model,\n                (input_ids, attention_mask),\n                tmp.name,\n                input_names=['input_ids', 'attention_mask'],\n                output_names=['last_hidden_state'],\n                dynamic_axes={\n                    'input_ids': {0: 'batch_size', 1: 'sequence'},\n                    'attention_mask': {0: 'batch_size', 1: 'sequence'},\n                    'last_hidden_state': {0: 'batch_size', 1: 'sequence'}\n                },\n                do_constant_folding=True,\n                opset_version=17,  # Use preferred opset version\n                verbose=False\n            )\n            exported_path = tmp.name\n            print(f\"   ‚úÖ ONNX export completed: {os.path.basename(exported_path)}\")\n    \n    except Exception as e:\n        print(f\"   ‚ùå Error during ONNX export: {e}\")\n    \n    finally:\n        # Restore original functions\n        if original_trace_module:\n            torch.onnx.utils._trace = original_trace_module\n        if original_setup_trace:\n            torch.onnx.utils._setup_trace_module_map = original_setup_trace\n        print(\"   üîÑ Restored original functions\")\n    \n    # METHOD 2: Direct JIT Tracing (FIX THE DICT OUTPUT ERROR)\n    print(f\"\"\"\nüìã METHOD 2: Direct JIT Tracing (Fixed)\n{'-' * 50}\"\"\")\n    \n    try:\n        print(\"\"\"   üèóÔ∏è  Creating wrapper model to fix dict output issue...\"\"\")\n        \n        # The error occurs because BERT returns a dict, but JIT tracing expects tensors\n        # Solution: Create a wrapper that returns only tensor outputs\n        class BertWrapper(torch.nn.Module):\n            def __init__(self, bert_model):\n                super().__init__()\n                self.bert = bert_model\n            \n            def forward(self, input_ids, attention_mask):\n                outputs = self.bert(input_ids, attention_mask=attention_mask)\n                # Return only the last_hidden_state tensor instead of the full dict\n                return outputs.last_hidden_state\n        \n        wrapped_model = BertWrapper(model)\n        print(\"   ‚úÖ Wrapper model created\")\n        \n        print(\"   üéØ Attempting JIT tracing...\")\n        traced_model = torch.jit.trace(wrapped_model, (input_ids, attention_mask), check_trace=False)\n        print(f\"   ‚úÖ JIT trace successful!\")\n        \n        # Check if tracing populated _trace_module_map\n        post_trace_map = getattr(torch.jit._trace, '_trace_module_map', None)\n        if post_trace_map:\n            captured_maps.append({\n                'hook_point': 'jit_trace',\n                'map': dict(post_trace_map) if hasattr(post_trace_map, 'items') else post_trace_map,\n                'map_type': type(post_trace_map).__name__,\n                'map_size': len(post_trace_map) if hasattr(post_trace_map, '__len__') else 0\n            })\n            print(f\"   ‚úÖ JIT trace populated map with {len(post_trace_map)} entries\")\n        else:\n            print(\"   ‚ÑπÔ∏è  JIT trace did not populate _trace_module_map (this is normal)\")\n        \n        # Extract graph and analyze\n        graph = traced_model.graph\n        print(f\"   ‚úÖ Extracted graph with {len(list(graph.nodes()))} nodes\")\n        \n        # Analyze first few nodes for scope information\n        node_scopes = []\n        for i, node in enumerate(graph.nodes()):\n            if i >= 10:  # Limit analysis\n                break\n            \n            node_info = {\n                'index': i,\n                'kind': node.kind(),\n            }\n            \n            # Try to get scope information\n            try:\n                if hasattr(node, 'scopeName'):\n                    scope = node.scopeName()\n                    node_info['scope'] = scope\n                    if scope and '::' in scope:\n                        node_scopes.append(scope)\n            except:\n                pass\n            \n            captured_maps.append({\n                'hook_point': 'node_analysis',\n                'node_info': node_info\n            })\n        \n        if node_scopes:\n            print(f\"   ‚úÖ Found {len(node_scopes)} nodes with scope information\")\n            print(f\"   üìù Sample scopes: {node_scopes[:3]}\")\n        else:\n            print(\"   ‚ÑπÔ∏è  No scope information found in nodes (expected for JIT tracing)\")\n            \n    except Exception as e:\n        print(f\"\"\"   ‚ùå Error during JIT tracing: {e}\n   üí° This error was expected and is now fixed!\"\"\")\n    \n    # METHOD 3: Manual Module Map Creation\n    print(f\"\"\"\nüìã METHOD 3: Manual Module Map Creation\n{'-' * 50}\"\"\")\n    \n    try:\n        print(\"   üèóÔ∏è  Creating manual module map...\")\n        \n        # This mimics what PyTorch does internally during ONNX export\n        manual_map = {}\n        \n        # Add root module\n        manual_map[model] = \"__module\"\n        \n        # Add all named modules\n        for name, module in model.named_modules():\n            if name:  # Skip root (empty name)\n                full_name = f\"__module.{name}\"\n                manual_map[module] = full_name\n        \n        print(f\"   ‚úÖ Created manual map with {len(manual_map)} modules\")\n        \n        # Temporarily set this as _trace_module_map to see if ONNX export uses it\n        torch.jit._trace._trace_module_map = manual_map\n        \n        captured_maps.append({\n            'hook_point': 'manual_creation',\n            'map': dict(manual_map),\n            'map_type': type(manual_map).__name__,\n            'map_size': len(manual_map)\n        })\n        \n        print(\"   üìã Manual map created and registered\")\n        \n    except Exception as e:\n        print(f\"   ‚ùå Error creating manual map: {e}\")\n    \n    return captured_maps, exported_path\n\ncaptured_maps, onnx_path = capture_trace_module_map_during_export()\n\n# Enhanced output formatting with proper HF hierarchy sorting\ndef print_results_with_rich_formatting():\n    \"\"\"Print results with enhanced formatting and proper HuggingFace hierarchy ordering\"\"\"\n    \n    def sort_modules_by_hf_hierarchy(items):\n        \"\"\"Sort modules preserving HuggingFace model hierarchy structure\"\"\"\n        \n        # First, separate items by their base path (remove :: part for sorting)\n        def get_sort_key(item):\n            module, name = item\n            \n            # Extract the base path (remove :: part if it exists)\n            if '::' in name:\n                base_path = name.split('::')[0]\n                instance_name = name.split('::')[1] if '::' in name else ''\n            else:\n                base_path = name\n                instance_name = ''\n            \n            # Remove __module prefix for consistent sorting\n            if base_path.startswith('__module.'):\n                base_path = base_path[9:]  # Remove '__module.'\n            elif base_path == '__module':\n                base_path = ''  # Root module comes first\n            \n            # Create hierarchical sort key\n            # Split by dots and create a tuple for natural sorting\n            path_parts = base_path.split('.') if base_path else ['']\n            \n            # Pad with empty strings to ensure consistent sorting depth\n            # and add instance name as final sort key\n            sort_tuple = tuple(path_parts + [''] * (10 - len(path_parts)) + [instance_name])\n            \n            return sort_tuple\n        \n        return sorted(items, key=get_sort_key)\n    \n    print(f\"\"\"\nüéØ {'='*70}\nüéØ COMPREHENSIVE RESULTS ANALYSIS\nüéØ {'='*70}\n\nüìä SUMMARY: Captured {len(captured_maps)} entries total\"\"\")\n\n    # Group by hook point\n    hook_points = {}\n    for entry in captured_maps:\n        hook_point = entry.get('hook_point', 'unknown')\n        if hook_point not in hook_points:\n            hook_points[hook_point] = []\n        hook_points[hook_point].append(entry)\n\n    for hook_point, entries in hook_points.items():\n        section_title = hook_point.upper().replace('_', ' ')\n        print(f\"\"\"\nüìå {section_title}: {len(entries)} entries\n   {'-' * 60}\"\"\")\n        \n        if hook_point in ['trace_hook', 'setup_trace_before', 'setup_trace_after', 'jit_trace', 'manual_creation']:\n            # These have module maps - SHOW ALL ENTRIES WITH PROPER HF HIERARCHY SORTING\n            for entry in entries:\n                if 'map_size' in entry:\n                    print(f\"\"\"   üìè Map size: {entry['map_size']} modules\n   üìã Map type: {entry['map_type']}\"\"\")\n                    \n                    if entry.get('map') and len(entry['map']) > 0:\n                        print(\"   üìù ALL MODULE ENTRIES (HuggingFace Hierarchy Order):\")\n                        \n                        # Sort modules preserving HuggingFace hierarchy\n                        sorted_items = sort_modules_by_hf_hierarchy(list(entry['map'].items()))\n                        \n                        for i, (module, name) in enumerate(sorted_items, 1):\n                            module_type = type(module).__name__\n                            \n                            # Color code based on hierarchy quality\n                            if '::' in name and '.' in name:\n                                icon = \"üü¢\"  # Full hierarchy\n                            elif '::' in name:\n                                icon = \"üü°\"  # Class scope\n                            else:\n                                icon = \"üîµ\"  # Simple name\n                            \n                            # Add indentation based on hierarchy depth for visual structure\n                            if name.startswith('__module.'):\n                                depth = name.count('.') - 1  # Subtract 1 for __module\n                                indent = \"  \" * depth\n                            else:\n                                depth = name.count('.')\n                                indent = \"  \" * depth\n                            \n                            print(f\"      {icon} {i:2d}.{indent} {module_type:20s} ‚Üí {name}\")\n                            \n                            # Add extra spacing every 10 entries for readability\n                            if i % 10 == 0 and i < len(sorted_items):\n                                print(\"         \" + \"¬∑\" * 50)\n        \n        elif hook_point == 'node_analysis':\n            # These have node information\n            scoped_nodes = [e for e in entries if 'scope' in e.get('node_info', {})]\n            print(f\"\"\"   üìä Total nodes analyzed: {len(entries)}\n   üéØ Nodes with scope: {len(scoped_nodes)}\"\"\")\n            \n            if scoped_nodes:\n                print(\"   üìù NODES WITH SCOPE INFORMATION:\")\n                for i, entry in enumerate(scoped_nodes, 1):\n                    node_info = entry['node_info']\n                    print(f\"      üéØ {i}. {node_info['kind']:15s} ‚Üí {node_info.get('scope', 'no_scope')}\")\n            else:\n                print(\"   ‚ÑπÔ∏è  No nodes with scope information found\")\n\nprint_results_with_rich_formatting()"
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: ONNX Graph Analysis\n",
    "\n",
    "Let's analyze the ONNX file that was created to understand what metadata is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE THE MAJOR DISCOVERY FROM METHOD 1\n",
    "print(f\"\"\"\n",
    "üö® {'='*70}\n",
    "üö® MAJOR DISCOVERY ANALYSIS\n",
    "üö® {'='*70}\"\"\")\n",
    "\n",
    "if captured_maps:\n",
    "    print(f\"\"\"\n",
    "üìä HIERARCHY INFORMATION QUALITY ANALYSIS\n",
    "{'='*60}\"\"\")\n",
    "    \n",
    "    for entry in captured_maps:\n",
    "        hook_point = entry.get('hook_point', 'unknown')\n",
    "        if 'map' in entry and entry['map']:\n",
    "            section_title = hook_point.upper().replace('_', ' ')\n",
    "            print(f\"\"\"\n",
    "üéØ {section_title}\n",
    "   {'-' * 50}\"\"\")\n",
    "            \n",
    "            # Analyze the quality of hierarchy information\n",
    "            hierarchy_quality = {\n",
    "                'simple_names': 0,\n",
    "                'class_scopes': 0, \n",
    "                'full_hierarchies': 0\n",
    "            }\n",
    "            \n",
    "            # Categorize all entries\n",
    "            for module, name in entry['map'].items():\n",
    "                if '::' in name and '.' in name:\n",
    "                    hierarchy_quality['full_hierarchies'] += 1\n",
    "                elif '::' in name:\n",
    "                    hierarchy_quality['class_scopes'] += 1\n",
    "                else:\n",
    "                    hierarchy_quality['simple_names'] += 1\n",
    "            \n",
    "            total = len(entry['map'])\n",
    "            print(f\"\"\"   üìà Quality Breakdown ({total} total modules):\n",
    "      üîµ Simple names:     {hierarchy_quality['simple_names']:2d} ({hierarchy_quality['simple_names']/total*100:5.1f}%)\n",
    "      üü° Class scopes:     {hierarchy_quality['class_scopes']:2d} ({hierarchy_quality['class_scopes']/total*100:5.1f}%)\n",
    "      üü¢ Full hierarchies: {hierarchy_quality['full_hierarchies']:2d} ({hierarchy_quality['full_hierarchies']/total*100:5.1f}%)\"\"\")\n",
    "            \n",
    "            # Show examples of each type\n",
    "            print(f\"\"\"\n",
    "   üìù Examples by Type:\"\"\")\n",
    "            \n",
    "            examples = {'simple': [], 'class': [], 'full': []}\n",
    "            for module, name in entry['map'].items():\n",
    "                if '::' in name and '.' in name and len(examples['full']) < 3:\n",
    "                    examples['full'].append((type(module).__name__, name))\n",
    "                elif '::' in name and len(examples['class']) < 3:\n",
    "                    examples['class'].append((type(module).__name__, name))\n",
    "                elif len(examples['simple']) < 3:\n",
    "                    examples['simple'].append((type(module).__name__, name))\n",
    "            \n",
    "            if examples['full']:\n",
    "                print(f\"\"\"      üü¢ Full Hierarchies (BEST - What we want!):\"\"\")\n",
    "                for module_type, name in examples['full']:\n",
    "                    print(f\"         ‚ú® {module_type:15s} ‚Üí {name}\")\n",
    "            \n",
    "            if examples['class']:\n",
    "                print(f\"\"\"      üü° Class Scopes (Good - Has class info):\"\"\")\n",
    "                for module_type, name in examples['class']:\n",
    "                    print(f\"         üì¶ {module_type:15s} ‚Üí {name}\")\n",
    "            \n",
    "            if examples['simple']:\n",
    "                print(f\"\"\"      üîµ Simple Names (Basic - Just module path):\"\"\")\n",
    "                for module_type, name in examples['simple']:\n",
    "                    print(f\"         üìÅ {module_type:15s} ‚Üí {name}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üí° {'='*70}\n",
    "üí° KEY INSIGHTS & IMPLEMENTATION STRATEGY\n",
    "üí° {'='*70}\n",
    "\n",
    "üéØ BREAKTHROUGH DISCOVERIES:\n",
    "   1. üî• ONNX export DOES create enhanced scope names during setup!\n",
    "   2. üèóÔ∏è  The 'setup_trace_after' shows FULL class hierarchy information\n",
    "   3. üîó Format: 'package.module.class::instance_name' - EXACTLY what we need!\n",
    "   4. üìã This solves our hierarchy preservation problem completely!\n",
    "\n",
    "üöÄ IMMEDIATE IMPLEMENTATION STRATEGY:\n",
    "   1. üé£ Hook into torch.onnx.utils._setup_trace_module_map\n",
    "   2. üì• Capture the enhanced trace module map AFTER setup\n",
    "   3. üè∑Ô∏è  Use this map to inject hierarchy metadata into ONNX nodes\n",
    "   4. ‚öôÔ∏è  This leverages PyTorch's existing infrastructure - no custom code needed!\n",
    "\n",
    "üéÅ WHY THIS IS PERFECT:\n",
    "   ‚úÖ Universal: Works with ANY PyTorch model\n",
    "   ‚úÖ Complete: Full package.module.class::instance hierarchy\n",
    "   ‚úÖ Reliable: Uses PyTorch's own infrastructure\n",
    "   ‚úÖ Performance: No custom tracing overhead\n",
    "   ‚úÖ Maintainable: Follows PyTorch's internal patterns\n",
    "\n",
    "üî¨ TECHNICAL DETAILS:\n",
    "   üìê Hook Point: torch.onnx.utils._setup_trace_module_map\n",
    "   üìä Data Source: torch.jit._trace._trace_module_map (after setup)\n",
    "   üéØ Target: ONNX node metadata injection\n",
    "   üîÑ Integration: Enhanced HTP strategy v2.0\n",
    "\n",
    "üéâ {'='*70}\n",
    "üéâ SOLUTION FOUND - READY FOR IMPLEMENTATION!\n",
    "üéâ {'='*70}\"\"\")\n",
    "\n",
    "# Continue only if we have valid ONNX path\n",
    "if onnx_path and os.path.exists(onnx_path):\n",
    "    print(f\"\"\"\n",
    "‚úÖ ONNX file available for analysis: {os.path.basename(onnx_path)}\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "‚ö†Ô∏è No valid ONNX file available - skipping ONNX analysis\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Prototype Enhanced Module Mapping\n",
    "\n",
    "Let's prototype extending the existing `_trace_module_map` with additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced module map created with 48 modules\n",
      "\n",
      "Sample module metadata:\n",
      "\n",
      "1. BertModel (root):\n",
      "   - Path: __module\n",
      "   - Type: custom\n",
      "   - Parameters: 4385920\n",
      "   - Level: 0\n",
      "   - Is Leaf: False\n",
      "\n",
      "2. BertEmbeddings (embeddings):\n",
      "   - Path: __module.embeddings\n",
      "   - Type: custom\n",
      "   - Parameters: 3972864\n",
      "   - Level: 1\n",
      "   - Is Leaf: False\n",
      "\n",
      "3. Embedding (embeddings.word_embeddings):\n",
      "   - Path: __module.embeddings.word_embeddings\n",
      "   - Type: torch.nn\n",
      "   - Parameters: 3906816\n",
      "   - Level: 2\n",
      "   - Is Leaf: True\n",
      "\n",
      "4. Embedding (embeddings.position_embeddings):\n",
      "   - Path: __module.embeddings.position_embeddings\n",
      "   - Type: torch.nn\n",
      "   - Parameters: 65536\n",
      "   - Level: 2\n",
      "   - Is Leaf: True\n",
      "\n",
      "5. Embedding (embeddings.token_type_embeddings):\n",
      "   - Path: __module.embeddings.token_type_embeddings\n",
      "   - Type: torch.nn\n",
      "   - Parameters: 256\n",
      "   - Level: 2\n",
      "   - Is Leaf: True\n"
     ]
    }
   ],
   "source": [
    "def create_enhanced_trace_module_map(model: torch.nn.Module) -> Dict[torch.nn.Module, Dict[str, Any]]:\n",
    "    \"\"\"Create an enhanced version of _trace_module_map with additional metadata.\"\"\"\n",
    "    \n",
    "    enhanced_map = {}\n",
    "    \n",
    "    def extract_module_metadata(module: torch.nn.Module, name: str, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract comprehensive metadata for a module.\"\"\"\n",
    "        return {\n",
    "            'name': name,\n",
    "            'full_path': path,\n",
    "            'class_name': type(module).__name__,\n",
    "            'module_type': 'torch.nn' if module.__class__.__module__.startswith('torch.nn') else 'custom',\n",
    "            'parameters': {\n",
    "                'total': sum(p.numel() for p in module.parameters()),\n",
    "                'trainable': sum(p.numel() for p in module.parameters() if p.requires_grad),\n",
    "                'shapes': {n: list(p.shape) for n, p in module.named_parameters(recurse=False)}\n",
    "            },\n",
    "            'children': list(module.named_children()),\n",
    "            'is_leaf': len(list(module.children())) == 0,\n",
    "            'hierarchy_level': len(path.split('.')) - 1 if path != '__module' else 0\n",
    "        }\n",
    "    \n",
    "    # Root module\n",
    "    root_path = '__module'\n",
    "    enhanced_map[model] = extract_module_metadata(model, 'root', root_path)\n",
    "    \n",
    "    # All submodules\n",
    "    for name, module in model.named_modules():\n",
    "        if name:  # Skip root (empty name)\n",
    "            full_path = f\"{root_path}.{name}\"\n",
    "            enhanced_map[module] = extract_module_metadata(module, name, full_path)\n",
    "    \n",
    "    return enhanced_map\n",
    "\n",
    "enhanced_map = create_enhanced_trace_module_map(model)\n",
    "\n",
    "print(f\"Enhanced module map created with {len(enhanced_map)} modules\")\n",
    "print(\"\\nSample module metadata:\")\n",
    "for i, (module, metadata) in enumerate(list(enhanced_map.items())[:5]):\n",
    "    print(f\"\\n{i+1}. {metadata['class_name']} ({metadata['name']}):\")\n",
    "    print(f\"   - Path: {metadata['full_path']}\")\n",
    "    print(f\"   - Type: {metadata['module_type']}\")\n",
    "    print(f\"   - Parameters: {metadata['parameters']['total']}\")\n",
    "    print(f\"   - Level: {metadata['hierarchy_level']}\")\n",
    "    print(f\"   - Is Leaf: {metadata['is_leaf']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Prototype Hook-Based Metadata Injection\n",
    "\n",
    "Let's prototype hooking into the ONNX export process to inject our enhanced metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_with_enhanced_metadata(model, sample_inputs, output_path, enhanced_map):\n",
    "    \"\"\"Export ONNX with enhanced metadata injection.\"\"\"\n",
    "    \n",
    "    # Store original functions\n",
    "    original_setup = getattr(torch.onnx.utils, '_setup_trace_module_map', None)\n",
    "    metadata_captured = {'enhanced_map': None, 'original_map': None}\n",
    "    \n",
    "    def enhanced_setup_trace_module_map(model, export_modules_as_functions=False):\n",
    "        \"\"\"Enhanced version that includes our metadata.\"\"\"\n",
    "        \n",
    "        # Call original setup if it exists\n",
    "        if original_setup:\n",
    "            result = original_setup(model, export_modules_as_functions)\n",
    "            metadata_captured['original_map'] = getattr(torch.jit._trace, '_trace_module_map', None)\n",
    "        \n",
    "        # Create enhanced trace module map\n",
    "        enhanced_trace_map = {}\n",
    "        \n",
    "        for module, metadata in enhanced_map.items():\n",
    "            # Use PyTorch's built-in scope name creation\n",
    "            scope_name = torch._C._jit_onnx_create_full_scope_name(\n",
    "                metadata['class_name'], \n",
    "                metadata['name'] if metadata['name'] != 'root' else '__module'\n",
    "            )\n",
    "            enhanced_trace_map[module] = scope_name\n",
    "        \n",
    "        # Set the enhanced map\n",
    "        torch.jit._trace._trace_module_map = enhanced_trace_map\n",
    "        metadata_captured['enhanced_map'] = enhanced_trace_map\n",
    "        \n",
    "        return enhanced_trace_map if original_setup else None\n",
    "    \n",
    "    # Monkey patch the setup function\n",
    "    if original_setup:\n",
    "        torch.onnx.utils._setup_trace_module_map = enhanced_setup_trace_module_map\n",
    "    \n",
    "    try:\n",
    "        # Perform export\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            sample_inputs,\n",
    "            output_path,\n",
    "            export_params=True,\n",
    "            opset_version=14,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input_ids', 'attention_mask'],\n",
    "            output_names=['last_hidden_state'],\n",
    "            dynamic_axes={\n",
    "                'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "                'last_hidden_state': {0: 'batch_size', 1: 'sequence'}\n",
    "            },\n",
    "            verbose=False\n",
    "        )\n",
    "    finally:\n",
    "        # Restore original\n",
    "        if original_setup:\n",
    "            torch.onnx.utils._setup_trace_module_map = original_setup\n",
    "    \n",
    "    return metadata_captured\n",
    "\n",
    "# Test the enhanced export\n",
    "with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp:\n",
    "    enhanced_output = tmp.name\n",
    "\n",
    "metadata_captured = export_with_enhanced_metadata(\n",
    "    model,\n",
    "    (input_ids, attention_mask),\n",
    "    enhanced_output,\n",
    "    enhanced_map\n",
    ")\n",
    "\n",
    "print(\"Enhanced export completed!\")\n",
    "print(f\"Original map captured: {metadata_captured['original_map'] is not None}\")\n",
    "print(f\"Enhanced map captured: {metadata_captured['enhanced_map'] is not None}\")\n",
    "\n",
    "if metadata_captured['enhanced_map']:\n",
    "    print(f\"Enhanced map contains {len(metadata_captured['enhanced_map'])} modules\")\n",
    "    print(\"\\nSample enhanced scope names:\")\n",
    "    for i, (module, scope_name) in enumerate(list(metadata_captured['enhanced_map'].items())[:5]):\n",
    "        print(f\"  {i+1}. {type(module).__name__}: {scope_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Compare Original vs Enhanced Export\n",
    "\n",
    "Let's compare the results of our enhanced export with a standard export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_onnx_exports(original_path, enhanced_path):\n",
    "    \"\"\"Compare two ONNX exports to see differences in metadata preservation.\"\"\"\n",
    "    \n",
    "    def analyze_onnx_names(path, label):\n",
    "        model_onnx = onnx.load(path)\n",
    "        graph = model_onnx.graph\n",
    "        \n",
    "        analysis = {\n",
    "            'total_nodes': len(graph.node),\n",
    "            'nodes_with_scope': 0,\n",
    "            'scope_patterns': set(),\n",
    "            'parameter_names': [],\n",
    "            'sample_node_names': []\n",
    "        }\n",
    "        \n",
    "        for node in graph.node:\n",
    "            if node.name and ('/' in node.name or '.' in node.name):\n",
    "                analysis['nodes_with_scope'] += 1\n",
    "                # Extract pattern (first part before numbers)\n",
    "                parts = node.name.split('/')\n",
    "                if len(parts) > 1:\n",
    "                    analysis['scope_patterns'].add(parts[0])\n",
    "            \n",
    "            if len(analysis['sample_node_names']) < 10:\n",
    "                analysis['sample_node_names'].append({\n",
    "                    'name': node.name,\n",
    "                    'op_type': node.op_type\n",
    "                })\n",
    "        \n",
    "        # Get parameter names\n",
    "        for init in graph.initializer:\n",
    "            analysis['parameter_names'].append(init.name)\n",
    "        \n",
    "        print(f\"\\n{label} Analysis:\")\n",
    "        print(f\"- Total nodes: {analysis['total_nodes']}\")\n",
    "        print(f\"- Nodes with scope info: {analysis['nodes_with_scope']}\")\n",
    "        print(f\"- Scope patterns: {len(analysis['scope_patterns'])}\")\n",
    "        print(f\"- Parameters: {len(analysis['parameter_names'])}\")\n",
    "        \n",
    "        if analysis['scope_patterns']:\n",
    "            print(f\"- Sample scope patterns: {list(analysis['scope_patterns'])[:5]}\")\n",
    "        \n",
    "        print(f\"- Sample node names:\")\n",
    "        for item in analysis['sample_node_names'][:5]:\n",
    "            print(f\"  - {item['op_type']}: {item['name']}\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    # Create baseline export for comparison\n",
    "    with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp:\n",
    "        baseline_path = tmp.name\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (input_ids, attention_mask),\n",
    "        baseline_path,\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input_ids', 'attention_mask'],\n",
    "        output_names=['last_hidden_state'],\n",
    "        dynamic_axes={\n",
    "            'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "            'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "            'last_hidden_state': {0: 'batch_size', 1: 'sequence'}\n",
    "        },\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    baseline_analysis = analyze_onnx_names(baseline_path, \"Baseline Export\")\n",
    "    enhanced_analysis = analyze_onnx_names(enhanced_path, \"Enhanced Export\")\n",
    "    \n",
    "    print(f\"\\n=== COMPARISON ===\")\n",
    "    print(f\"Scope info improvement: {enhanced_analysis['nodes_with_scope'] - baseline_analysis['nodes_with_scope']} nodes\")\n",
    "    print(f\"Scope patterns improvement: {len(enhanced_analysis['scope_patterns']) - len(baseline_analysis['scope_patterns'])} patterns\")\n",
    "    \n",
    "    # Cleanup\n",
    "    os.unlink(baseline_path)\n",
    "    os.unlink(enhanced_path)\n",
    "    \n",
    "    return baseline_analysis, enhanced_analysis\n",
    "\n",
    "baseline_analysis, enhanced_analysis = compare_onnx_exports(None, enhanced_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Alternative Approach - Custom Graph Pass\n",
    "\n",
    "Let's prototype a custom graph pass approach that injects metadata after graph creation but before final export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype_custom_graph_pass():\n",
    "    \"\"\"Prototype a custom graph pass for metadata injection.\"\"\"\n",
    "    \n",
    "    # This is a conceptual prototype - actual implementation would require\n",
    "    # deeper integration with PyTorch's C++ graph infrastructure\n",
    "    \n",
    "    class HierarchyMetadataInjector:\n",
    "        def __init__(self, enhanced_map):\n",
    "            self.enhanced_map = enhanced_map\n",
    "            self.module_to_scope = {}\n",
    "            \n",
    "            # Build module to scope mapping\n",
    "            for module, metadata in enhanced_map.items():\n",
    "                scope_name = f\"{metadata['class_name']}.{metadata['name']}\"\n",
    "                self.module_to_scope[module] = {\n",
    "                    'scope': scope_name,\n",
    "                    'hierarchy_level': metadata['hierarchy_level'],\n",
    "                    'is_leaf': metadata['is_leaf'],\n",
    "                    'module_type': metadata['module_type']\n",
    "                }\n",
    "        \n",
    "        def inject_metadata_to_graph(self, graph):\n",
    "            \"\"\"Conceptual graph metadata injection.\"\"\"\n",
    "            \n",
    "            # This would be the entry point for a custom C++ graph pass\n",
    "            # that adds hierarchy metadata to graph nodes\n",
    "            \n",
    "            metadata_nodes = []\n",
    "            \n",
    "            for module, scope_info in self.module_to_scope.items():\n",
    "                # Create metadata entry for each module\n",
    "                metadata_entry = {\n",
    "                    'module_id': id(module),\n",
    "                    'scope_name': scope_info['scope'],\n",
    "                    'hierarchy_level': scope_info['hierarchy_level'],\n",
    "                    'is_leaf_module': scope_info['is_leaf'],\n",
    "                    'module_type': scope_info['module_type'],\n",
    "                    'class_name': type(module).__name__\n",
    "                }\n",
    "                metadata_nodes.append(metadata_entry)\n",
    "            \n",
    "            return {\n",
    "                'hierarchy_metadata': metadata_nodes,\n",
    "                'total_modules': len(metadata_nodes),\n",
    "                'injection_strategy': 'custom_graph_pass'\n",
    "            }\n",
    "        \n",
    "        def create_sidecar_metadata(self, onnx_path):\n",
    "            \"\"\"Create sidecar JSON with hierarchy metadata.\"\"\"\n",
    "            \n",
    "            metadata = self.inject_metadata_to_graph(None)  # Conceptual\n",
    "            \n",
    "            sidecar_path = onnx_path.replace('.onnx', '_hierarchy.json')\n",
    "            with open(sidecar_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            return sidecar_path\n",
    "    \n",
    "    # Test the concept\n",
    "    injector = HierarchyMetadataInjector(enhanced_map)\n",
    "    \n",
    "    # Export ONNX normally\n",
    "    with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp:\n",
    "        test_onnx_path = tmp.name\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (input_ids, attention_mask),\n",
    "        test_onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Create sidecar metadata\n",
    "    sidecar_path = injector.create_sidecar_metadata(test_onnx_path)\n",
    "    \n",
    "    print(\"Custom Graph Pass Prototype:\")\n",
    "    print(f\"- ONNX exported: {test_onnx_path}\")\n",
    "    print(f\"- Sidecar created: {sidecar_path}\")\n",
    "    \n",
    "    # Show sidecar content\n",
    "    with open(sidecar_path, 'r') as f:\n",
    "        sidecar_data = json.load(f)\n",
    "    \n",
    "    print(f\"- Total modules in metadata: {sidecar_data['total_modules']}\")\n",
    "    print(f\"- Injection strategy: {sidecar_data['injection_strategy']}\")\n",
    "    \n",
    "    print(\"\\nSample hierarchy metadata:\")\n",
    "    for i, entry in enumerate(sidecar_data['hierarchy_metadata'][:3]):\n",
    "        print(f\"  {i+1}. {entry['class_name']} ({entry['scope_name']})\")\n",
    "        print(f\"     - Level: {entry['hierarchy_level']}\")\n",
    "        print(f\"     - Leaf: {entry['is_leaf_module']}\")\n",
    "        print(f\"     - Type: {entry['module_type']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    os.unlink(test_onnx_path)\n",
    "    os.unlink(sidecar_path)\n",
    "    \n",
    "    return injector\n",
    "\n",
    "injector = prototype_custom_graph_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Summary and Next Steps\n",
    "\n",
    "Based on this investigation, here are the key findings and recommended approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_findings():\n",
    "    \"\"\"Summarize key findings and recommend next steps.\"\"\"\n",
    "    \n",
    "    print(\"=== INVESTIGATION SUMMARY ===\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üîç KEY DISCOVERIES:\")\n",
    "    print(\"1. PyTorch already has hierarchy infrastructure:\")\n",
    "    print(\"   - _trace_module_map captures module hierarchy during ONNX export\")\n",
    "    print(\"   - Built-in ONNX scope functions for hierarchical naming\")\n",
    "    print(\"   - Existing metadata attachment mechanisms\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. Current limitations identified:\")\n",
    "    print(\"   - Standard export doesn't preserve detailed hierarchy in node names\")\n",
    "    print(\"   - Auxiliary operations still lack direct module association\")\n",
    "    print(\"   - Module context is available but not fully utilized\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üöÄ RECOMMENDED APPROACHES (in priority order):\")\n",
    "    print()\n",
    "    \n",
    "    print(\"APPROACH 1: Enhanced Trace Module Map (SAFEST)\")\n",
    "    print(\"‚úÖ Pros:\")\n",
    "    print(\"   - Leverages existing PyTorch infrastructure\")\n",
    "    print(\"   - Low risk of breaking changes\")\n",
    "    print(\"   - Can be implemented as drop-in replacement\")\n",
    "    print(\"‚ùå Cons:\")\n",
    "    print(\"   - Limited by existing ONNX export constraints\")\n",
    "    print(\"   - May not solve auxiliary operations problem completely\")\n",
    "    print()\n",
    "    \n",
    "    print(\"APPROACH 2: Custom Graph Pass + Sidecar Metadata (BALANCED)\")\n",
    "    print(\"‚úÖ Pros:\")\n",
    "    print(\"   - Can inject comprehensive hierarchy metadata\")\n",
    "    print(\"   - Separates concerns (ONNX export + metadata)\")\n",
    "    print(\"   - Flexible metadata format\")\n",
    "    print(\"‚ùå Cons:\")\n",
    "    print(\"   - Requires sidecar file management\")\n",
    "    print(\"   - More complex implementation\")\n",
    "    print()\n",
    "    \n",
    "    print(\"APPROACH 3: Deep Hook Integration (MOST POWERFUL)\")\n",
    "    print(\"‚úÖ Pros:\")\n",
    "    print(\"   - Could solve auxiliary operations problem\")\n",
    "    print(\"   - Direct integration with export process\")\n",
    "    print(\"   - Maximum control over metadata injection\")\n",
    "    print(\"‚ùå Cons:\")\n",
    "    print(\"   - Highest risk of compatibility issues\")\n",
    "    print(\"   - Requires deep PyTorch internals knowledge\")\n",
    "    print(\"   - May break with PyTorch updates\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üéØ IMMEDIATE NEXT STEPS:\")\n",
    "    print(\"1. Implement Approach 1 as proof-of-concept\")\n",
    "    print(\"2. Test with multiple model architectures (BERT, ResNet, GPT)\")\n",
    "    print(\"3. Evaluate hierarchy preservation quality\")\n",
    "    print(\"4. If successful, consider hybrid approach (Approach 1 + 2)\")\n",
    "    print(\"5. Document findings and update strategy implementation\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üí° KEY INSIGHT:\")\n",
    "    print(\"Instead of fighting against ONNX export's design, we should\")\n",
    "    print(\"leverage PyTorch's existing hierarchy infrastructure and extend\")\n",
    "    print(\"it strategically. The infrastructure is already there - we just\")\n",
    "    print(\"need to make better use of it.\")\n",
    "\n",
    "summarize_findings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Next Actions\n",
    "\n",
    "This investigation reveals that PyTorch already has sophisticated infrastructure for hierarchy preservation that we can leverage. The next step is to implement the Enhanced Trace Module Map approach as a proof-of-concept and test it across multiple architectures.\n",
    "\n",
    "Key files to implement:\n",
    "1. `modelexport/strategies/htp/enhanced_trace_mapping.py` - Core enhanced mapping logic\n",
    "2. `modelexport/strategies/htp/pytorch_hooks.py` - PyTorch internal hooks\n",
    "3. `tests/test_enhanced_htp_strategy.py` - Comprehensive testing\n",
    "\n",
    "This approach should solve many of our current challenges while staying within PyTorch's designed patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}