{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchScript Intermediate State Exploration\n",
    "\n",
    "This notebook explores what information is available in TorchScript graph before ONNX conversion,\n",
    "specifically investigating the scope information that contains module hierarchy.\n",
    "\n",
    "## Key Research Questions:\n",
    "1. What scope information is available at the TorchScript level?\n",
    "2. How can we access `node.scopeName()` properly?\n",
    "3. Where exactly is context lost during ONNX conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "import os\n",
    "\n",
    "# Create temp directory for outputs\n",
    "os.makedirs(\"../temp\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: prajjwal1/bert-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: BertModel\n",
      "Input shapes: [('input_ids', torch.Size([1, 4])), ('token_type_ids', torch.Size([1, 4])), ('attention_mask', torch.Size([1, 4]))]\n",
      "‚úÖ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load a small model for exploration\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare inputs\n",
    "text = \"Hello world\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Input shapes: {[(k, v.shape) for k, v in inputs.items()]}\")\n",
    "print(\"‚úÖ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attempt Different Tracing Approaches\n",
    "\n",
    "We'll try multiple methods to trace the model and see which one preserves scope information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying different tracing approaches...\n",
      "\n",
      "1. Trying direct tracing with strict=False...\n",
      "  ‚ùå Direct tracing failed: Type 'Tuple[str, str, str]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
      "\n",
      "2. Trying positional args tracing...\n",
      "  ‚úÖ Positional args tracing successful\n",
      "\n",
      "üéØ Using tracing method: positional_trace\n",
      "Traced model type: <class 'torch.jit._trace.TopLevelTracedModule'>\n",
      "Has graph: True\n"
     ]
    }
   ],
   "source": [
    "# Try multiple tracing approaches\n",
    "traced_model = None\n",
    "tracing_method = None\n",
    "\n",
    "print(\"Trying different tracing approaches...\\n\")\n",
    "\n",
    "# Approach 1: Direct tracing with strict=False (to handle dict output)\n",
    "try:\n",
    "    print(\"1. Trying direct tracing with strict=False...\")\n",
    "    with torch.no_grad():\n",
    "        traced_model = torch.jit.trace(\n",
    "            model, \n",
    "            inputs, \n",
    "            strict=False,\n",
    "            check_trace=False\n",
    "        )\n",
    "    print(\"  ‚úÖ Direct tracing successful\")\n",
    "    tracing_method = \"direct_trace\"\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Direct tracing failed: {e}\")\n",
    "    \n",
    "    # Approach 2: Try with positional args\n",
    "    try:\n",
    "        print(\"\\n2. Trying positional args tracing...\")\n",
    "        with torch.no_grad():\n",
    "            traced_model = torch.jit.trace(\n",
    "                model, \n",
    "                (inputs['input_ids'], inputs['attention_mask']),\n",
    "                strict=False,\n",
    "                check_trace=False\n",
    "            )\n",
    "        print(\"  ‚úÖ Positional args tracing successful\")\n",
    "        tracing_method = \"positional_trace\"\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Positional args tracing failed: {e}\")\n",
    "        \n",
    "        # Approach 3: Wrapper approach\n",
    "        print(\"\\n3. Using wrapper as fallback...\")\n",
    "        class ModelWrapper(nn.Module):\n",
    "            def __init__(self, model):\n",
    "                super().__init__()\n",
    "                self.model = model\n",
    "            \n",
    "            def forward(self, input_ids, attention_mask):\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                return outputs.last_hidden_state\n",
    "        \n",
    "        wrapped_model = ModelWrapper(model)\n",
    "        with torch.no_grad():\n",
    "            traced_model = torch.jit.trace(wrapped_model, (inputs['input_ids'], inputs['attention_mask']))\n",
    "        print(\"  ‚úÖ Wrapper tracing successful\")\n",
    "        tracing_method = \"wrapper_trace\"\n",
    "\n",
    "print(f\"\\nüéØ Using tracing method: {tracing_method}\")\n",
    "print(f\"Traced model type: {type(traced_model)}\")\n",
    "print(f\"Has graph: {hasattr(traced_model, 'graph')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Access TorchScript Graph and Explore Node Information\n",
    "\n",
    "Now let's examine the graph structure and try to extract scope information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing TorchScript graph...\n",
      "\n",
      "Total nodes in graph: 74\n",
      "Graph type: <class 'torch.Graph'>\n",
      "\n",
      "=== Graph String Representation (first 2000 chars) ===\n",
      "graph(%self.1 : __torch__.transformers.models.bert.modeling_bert.___torch_mangle_78.BertModel,\n",
      "      %input_ids : Long(1, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %attention_mask.1 : Long(1, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n",
      "  %pooler : __torch__.transformers.models.bert.modeling_bert.___torch_mangle_77.BertPooler = prim::GetAttr[name=\"pooler\"](%self.1)\n",
      "  %encoder : __torch__.transformers.models.bert.modeling_bert.___torch_mangle_74.BertEncoder = prim::GetAttr[name=\"encoder\"](%self.1)\n",
      "  %embeddings : __torch__.transformers.models.bert.modeling_bert.___torch_mangle_36.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)\n",
      "  %embeddings.3 : __torch__.transformers.models.bert.modeling_bert.___torch_mangle_36.BertEmbeddings = prim::GetAttr[name=\"embeddings\"](%self.1)\n",
      "  %token_type_ids : Tensor = prim::GetAttr[name=\"token_type_ids\"](%embeddings.3)\n",
      "  %187 : int = prim::Constant[value=0]() # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:932:0\n",
      "  %188 : int = aten::size(%input_ids, %187) # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:932:0\n",
      "  %batch_size : Long(device=cpu) = prim::NumToTensor(%188)\n",
      "  %203 : int = aten::Int(%batch_size)\n",
      "  %190 : int = prim::Constant[value=1]() # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:932:0\n",
      "  %191 : int = aten::size(%input_ids, %190) # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:932:0\n",
      "  %seq_length.1 : Long(device=cpu) = prim::NumToTensor(%191)\n",
      "  %278 : int = aten::Int(%seq_length.1)\n",
      "  %204 : int = aten::Int(%seq_length.1)\n",
      "  %198 : int = aten::Int(%seq_length.1)\n",
      "  %193 : int = prim::Constant[value=0]() # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:946:0\n",
      "  %194 : int = prim::Constant[value=0]() # /mnt/d/BYOM/modelexport/.venv/lib/\n",
      "... (truncated, total length: 10464 chars)\n"
     ]
    }
   ],
   "source": [
    "# Access the TorchScript graph\n",
    "print(\"Accessing TorchScript graph...\\n\")\n",
    "graph = traced_model.graph\n",
    "nodes = list(graph.nodes())\n",
    "\n",
    "print(f\"Total nodes in graph: {len(nodes)}\")\n",
    "print(f\"Graph type: {type(graph)}\")\n",
    "\n",
    "# Show graph string representation (this might contain scope info)\n",
    "print(\"\\n=== Graph String Representation (first 2000 chars) ===\")\n",
    "graph_str = str(graph)\n",
    "print(graph_str[:2000])\n",
    "if len(graph_str) > 2000:\n",
    "    print(f\"... (truncated, total length: {len(graph_str)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Node Analysis\n",
    "\n",
    "Let's examine individual nodes and try different methods to extract scope information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring node information...\n",
      "\n",
      "=== Node 0 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 1 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 2 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 3 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 4 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 5 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py(932): forward\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1741): _slow_forward\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1762): _call_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(1279): trace_module\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(696): _trace_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(1002): trace\n",
      "/tmp/ipykernel_203163/940552672.py(26): <module>\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3672): run_code\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3612): run_ast_nodes\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3367): run_cell_async\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3155): _run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3100): run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py(549): run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py(449): do_execute\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(778): execute_request\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py(362): execute_request\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(534): process_one\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/events.py(88): _run\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/base_events.py(1999): _run_once\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/base_events.py(645): run_forever\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py(211): start\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py(739): start\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/traitlets/config/application.py(1075): launch_instance\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      "\n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 6 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py(932): forward\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1741): _slow_forward\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1762): _call_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(1279): trace_module\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(696): _trace_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(1002): trace\n",
      "/tmp/ipykernel_203163/940552672.py(26): <module>\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3672): run_code\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3612): run_ast_nodes\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3367): run_cell_async\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3155): _run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3100): run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py(549): run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py(449): do_execute\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(778): execute_request\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py(362): execute_request\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(534): process_one\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/events.py(88): _run\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/base_events.py(1999): _run_once\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/base_events.py(645): run_forever\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py(211): start\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py(739): start\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/traitlets/config/application.py(1075): launch_instance\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      "\n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 7 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 8 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): \n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "=== Node 9 ===\n",
      "  scopeName(): \n",
      "  scope(): No method\n",
      "  sourceRange(): /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py(932): forward\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1741): _slow_forward\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1762): _call_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py(1751): _wrapped_call_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(1279): trace_module\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(696): _trace_impl\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py(1002): trace\n",
      "/tmp/ipykernel_203163/940552672.py(26): <module>\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3672): run_code\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3612): run_ast_nodes\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3367): run_cell_async\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py(128): _pseudo_sync_runner\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3155): _run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py(3100): run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py(549): run_cell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py(449): do_execute\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(778): execute_request\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py(362): execute_request\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(437): dispatch_shell\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(534): process_one\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py(545): dispatch_queue\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/events.py(88): _run\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/base_events.py(1999): _run_once\n",
      "/home/zhengte/.local/share/uv/python/cpython-3.12.11-linux-aarch64-gnu/lib/python3.12/asyncio/base_events.py(645): run_forever\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py(211): start\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py(739): start\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/traitlets/config/application.py(1075): launch_instance\n",
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      "\n",
      "  debugName(): No method\n",
      "  Available methods (first 10): ['addBlock', 'addInput', 'addOutput', 'attributeNames', 'blocks', 'c', 'c_', 'cconv', 'copyAttributes', 'copyMetadata']\n",
      "\n",
      "‚úÖ Analyzed 10 nodes\n"
     ]
    }
   ],
   "source": [
    "# Explore node information in detail\n",
    "print(\"Exploring node information...\\n\")\n",
    "\n",
    "node_info = []\n",
    "scope_hierarchy = set()\n",
    "\n",
    "for i, node in enumerate(nodes[:10]):  # First 10 nodes for detailed analysis\n",
    "    try:\n",
    "        print(f\"=== Node {i} ===\")\n",
    "        \n",
    "        # Basic node information\n",
    "        info = {\n",
    "            \"index\": i,\n",
    "            \"kind\": str(node.kind()),\n",
    "            \"has_scope\": hasattr(node, 'scopeName'),\n",
    "        }\n",
    "        \n",
    "        # Try different ways to get scope information\n",
    "        scope_methods = {\n",
    "            \"scopeName()\": lambda n: str(n.scopeName()) if hasattr(n, 'scopeName') else \"No method\",\n",
    "            \"scope()\": lambda n: str(n.scope()) if hasattr(n, 'scope') else \"No method\", \n",
    "            \"sourceRange()\": lambda n: str(n.sourceRange()) if hasattr(n, 'sourceRange') else \"No method\",\n",
    "            \"debugName()\": lambda n: str(n.debugName()) if hasattr(n, 'debugName') else \"No method\",\n",
    "        }\n",
    "        \n",
    "        for method_name, method_func in scope_methods.items():\n",
    "            try:\n",
    "                result = method_func(node)\n",
    "                info[method_name] = result\n",
    "                print(f\"  {method_name}: {result}\")\n",
    "            except Exception as e:\n",
    "                info[method_name] = f\"Error: {e}\"\n",
    "                print(f\"  {method_name}: Error - {e}\")\n",
    "        \n",
    "        # Try to get all available methods/attributes\n",
    "        available_methods = [attr for attr in dir(node) if not attr.startswith('_')]\n",
    "        info[\"available_methods\"] = available_methods[:10]  # First 10 to avoid clutter\n",
    "        \n",
    "        print(f\"  Available methods (first 10): {info['available_methods']}\")\n",
    "        \n",
    "        node_info.append(info)\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing node {i}: {e}\")\n",
    "        node_info.append({\"index\": i, \"error\": str(e)})\n",
    "\n",
    "print(f\"‚úÖ Analyzed {len(node_info)} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try to Access the Full Graph IR\n",
    "\n",
    "The error message we saw earlier contained rich scope information. Let's try to access that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access detailed graph representation...\n",
      "\n",
      "Found inlined_graph: <class 'torch.Graph'>\n",
      "Inlined graph has 276 nodes\n",
      "  Inlined Node 0: prim::GetAttr - Scope: \n",
      "  Inlined Node 1: prim::GetAttr - Scope: \n",
      "  Inlined Node 2: prim::GetAttr - Scope: \n",
      "  Inlined Node 3: prim::GetAttr - Scope: \n",
      "  Inlined Node 4: prim::GetAttr - Scope: \n",
      "\n",
      "--- Attempting ONNX export to reveal detailed IR ---\n",
      "ONNX export failed as expected. Analyzing error for scope info...\n",
      "No scope information found in error message.\n",
      "Torch IR graph at exception: graph(%input_ids : Long(1, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %attention_mask.1 : Long(1, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %embeddings.token_type_ids : Long(1, 512, strides=[512, 1], requires_grad=0, device=cpu),\n",
      "      %embeddings.position_ids : Long(1, 512, strides=[512, 1], requires_grad=0, device=cpu),\n",
      "      %embeddings.word_embeddings.weight : Float(30522, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %embeddings.token_type_embeddings.weight : Float(2, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %embeddings.position_embeddings.weight : Float(512, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %embeddings.LayerNorm.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %embeddings.LayerNorm.weight : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.self.query.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.self.query.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.self.key.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.self.key.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.self.value.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.self.value.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.output.dense.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.output.dense.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.output.LayerNorm.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.attention.output.LayerNorm.weight : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.intermediate.dense.bias : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.intermediate.dense.weight : Float(512, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.output.dense.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.output.dense.weight : Float(128, 512, strides=[512, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.output.LayerNorm.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.0.output.LayerNorm.weight : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.self.query.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.self.query.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.self.key.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.self.key.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.self.value.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.self.value.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.output.dense.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.output.dense.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.output.LayerNorm.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.attention.output.LayerNorm.weight : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.intermediate.dense.bias : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.intermediate.dense.weight : Float(512, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.output.dense.bias "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:814: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0xf5e20c173960>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.output.dense.weight : Float(128, 512, strides=[512, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.output.LayerNorm.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %encoder.layer.1.output.LayerNorm.weight : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %pooler.dense.bias : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %pooler.dense.weight : Float(128, 128, strides=[128, 1], requires_grad=0, device=cpu)):\n",
      "  %44 : str = prim::Constant[value=\"pooler_output\"](), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py:1279:0\n",
      "  %45 : str = prim::Constant[value=\"last_hidden_state\"](), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/jit/_trace.py:1279:0\n",
      "  %46 : str = prim::Constant[value=\"none\"](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/activations.py:69:0\n",
      "  %329 : Double(device=cpu) = prim::Constant[value={0}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %48 : int[] = prim::Constant[value=[0, 2, 1, 3]]()\n",
      "  %330 : Long(device=cpu) = prim::Constant[value={64}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %331 : Double(device=cpu) = prim::Constant[value={-3.40282e+38}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %332 : Long(device=cpu) = prim::Constant[value={11}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %333 : Double(device=cpu) = prim::Constant[value={1}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %53 : NoneType = prim::Constant(), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %334 : Long(device=cpu) = prim::Constant[value={6}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %335 : Long(device=cpu) = prim::Constant[value={3}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %336 : Long(device=cpu) = prim::Constant[value={2}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %337 : Double(device=cpu) = prim::Constant[value={0.1}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.dropout.Dropout::dropout\n",
      "  %338 : Bool(device=cpu) = prim::Constant[value={1}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.normalization.LayerNorm::LayerNorm\n",
      "  %339 : Double(device=cpu) = prim::Constant[value={1e-12}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.normalization.LayerNorm::LayerNorm\n",
      "  %60 : int[] = prim::Constant[value=[128]]()\n",
      "  %340 : Long(device=cpu) = prim::Constant[value={128}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.normalization.LayerNorm::LayerNorm\n",
      "  %341 : Long(device=cpu) = prim::Constant[value={-1}](), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::token_type_embeddings\n",
      "  %342 : Bool(device=cpu) = prim::Constant[value={0}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %343 : Long(device=cpu) = prim::Constant[value={9223372036854775807}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %344 : Long(device=cpu) = prim::Constant[value={1}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %345 : Long(device=cpu) = prim::Constant[value={0}](), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %73 : Long(device=cpu) = aten::size(%input_ids, %345), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:932:0\n",
      "  %74 : Long(device=cpu) = aten::size(%input_ids, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:932:0\n",
      "  %76 : Tensor = aten::slice(%embeddings.token_type_ids, %345, %345, %343, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:946:0\n",
      "  %buffered_token_type_ids : Tensor = aten::slice(%76, %344, %345, %74, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:946:0\n",
      "  %78 : int[] = prim::ListConstruct(%73, %74), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %input.1 : Tensor = aten::expand(%buffered_token_type_ids, %78, %342), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:947:0\n",
      "  %87 : Tensor = aten::slice(%embeddings.position_ids, %345, %345, %343, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:164:0\n",
      "  %input.3 : Tensor = aten::slice(%87, %344, %345, %74, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:164:0\n",
      "  %inputs_embeds : Tensor = aten::embedding(%embeddings.word_embeddings.weight, %input_ids, %345, %342, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::word_embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551:0\n",
      "  %token_type_embeddings.1 : Tensor = aten::embedding(%embeddings.token_type_embeddings.weight, %input.1, %341, %342, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::token_type_embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551:0\n",
      "  %position_embeddings.1 : Tensor = aten::embedding(%embeddings.position_embeddings.weight, %input.3, %341, %342, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::position_embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551:0\n",
      "  %embeddings.1 : Tensor = aten::add(%inputs_embeds, %token_type_embeddings.1, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:181:0\n",
      "  %254 : Tensor = aten::add(%embeddings.1, %position_embeddings.1, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:184:0\n",
      "  %input.7 : Tensor = aten::layer_norm(%254, %60, %embeddings.LayerNorm.weight, %embeddings.LayerNorm.bias, %339, %338), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.normalization.LayerNorm::LayerNorm # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2910:0\n",
      "  %hidden_states.1 : Tensor = aten::dropout(%input.7, %337, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.dropout.Dropout::dropout # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425:0\n",
      "  %101 : Long(device=cpu) = aten::size(%attention_mask.1, %345), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:185:0\n",
      "  %102 : Long(device=cpu) = aten::size(%attention_mask.1, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:185:0\n",
      "  %103 : Tensor = aten::slice(%attention_mask.1, %345, %345, %343, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188:0\n",
      "  %104 : Tensor = aten::unsqueeze(%103, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188:0\n",
      "  %105 : Tensor = aten::unsqueeze(%104, %336), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188:0\n",
      "  %106 : Tensor = aten::slice(%105, %335, %345, %343, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188:0\n",
      "  %107 : int[] = prim::ListConstruct(%101, %344, %74, %102), scope: transformers.models.bert.modeling_bert.BertModel::\n",
      "  %108 : Tensor = aten::expand(%106, %107, %342), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188:0\n",
      "  %109 : Tensor = aten::to(%108, %334, %342, %342, %53), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:188:0\n",
      "  %inverted_mask : Tensor = aten::rsub(%109, %333, %344), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/_tensor.py:1098:0\n",
      "  %111 : Tensor = aten::to(%inverted_mask, %332, %342, %342, %53), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:192:0\n",
      "  %attention_mask : Tensor = aten::masked_fill(%inverted_mask, %111, %331), scope: transformers.models.bert.modeling_bert.BertModel:: # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:192:0\n",
      "  %125 : Long(device=cpu) = aten::size(%hidden_states.1, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:360:0\n",
      "  %126 : Long(device=cpu) = aten::size(%hidden_states.1, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:360:0\n",
      "  %x.1 : Tensor = aten::linear(%hidden_states.1, %encoder.layer.0.attention.self.query.weight, %encoder.layer.0.attention.self.query.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self/torch.nn.modules.linear.Linear::query # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %130 : Long(device=cpu) = aten::size(%x.1, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %131 : Long(device=cpu) = aten::size(%x.1, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %132 : int[] = prim::ListConstruct(%130, %131, %336, %330), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %x.3 : Tensor = aten::view(%x.1, %132), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:219:0\n",
      "  %query_layer.1 : Tensor = aten::permute(%x.3, %48), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:220:0\n",
      "  %x.5 : Tensor = aten::linear(%hidden_states.1, %encoder.layer.0.attention.self.key.weight, %encoder.layer.0.attention.self.key.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self/torch.nn.modules.linear.Linear::key # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %138 : Long(device=cpu) = aten::size(%x.5, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %139 : Long(device=cpu) = aten::size(%x.5, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %140 : int[] = prim::ListConstruct(%138, %139, %336, %330), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %x.7 : Tensor = aten::view(%x.5, %140), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:219:0\n",
      "  %key_layer.1 : Tensor = aten::permute(%x.7, %48), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:220:0\n",
      "  %x.9 : Tensor = aten::linear(%hidden_states.1, %encoder.layer.0.attention.self.value.weight, %encoder.layer.0.attention.self.value.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self/torch.nn.modules.linear.Linear::value # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %146 : Long(device=cpu) = aten::size(%x.9, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %147 : Long(device=cpu) = aten::size(%x.9, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %148 : int[] = prim::ListConstruct(%146, %147, %336, %330), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %x.11 : Tensor = aten::view(%x.9, %148), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:219:0\n",
      "  %value_layer.1 : Tensor = aten::permute(%x.11, %48), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:220:0\n",
      "  %attn_output.1 : Tensor = aten::scaled_dot_product_attention(%query_layer.1, %key_layer.1, %value_layer.1, %attention_mask, %329, %342, %53, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:407:0\n",
      "  %attn_output.3 : Tensor = aten::transpose(%attn_output.1, %344, %336), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:416:0\n",
      "  %153 : int[] = prim::ListConstruct(%125, %126, %340), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %input.9 : Tensor = aten::reshape(%attn_output.3, %153), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:417:0\n",
      "  %input.11 : Tensor = aten::linear(%input.9, %encoder.layer.0.attention.output.dense.weight, %encoder.layer.0.attention.output.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %hidden_states.3 : Tensor = aten::dropout(%input.11, %337, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425:0\n",
      "  %input.13 : Tensor = aten::add(%hidden_states.3, %hidden_states.1, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:435:0\n",
      "  %input.15 : Tensor = aten::layer_norm(%input.13, %60, %encoder.layer.0.attention.output.LayerNorm.weight, %encoder.layer.0.attention.output.LayerNorm.bias, %339, %338), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2910:0\n",
      "  %input.17 : Tensor = aten::linear(%input.15, %encoder.layer.0.intermediate.dense.weight, %encoder.layer.0.intermediate.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %input.19 : Tensor = aten::gelu(%input.17, %46), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/activations.py:69:0\n",
      "  %input.21 : Tensor = aten::linear(%input.19, %encoder.layer.0.output.dense.weight, %encoder.layer.0.output.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %hidden_states.5 : Tensor = aten::dropout(%input.21, %337, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425:0\n",
      "  %input.23 : Tensor = aten::add(%hidden_states.5, %input.15, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertOutput::output # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:521:0\n",
      "  %hidden_states.7 : Tensor = aten::layer_norm(%input.23, %60, %encoder.layer.0.output.LayerNorm.weight, %encoder.layer.0.output.LayerNorm.bias, %339, %338), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::0/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2910:0\n",
      "  %188 : Long(device=cpu) = aten::size(%hidden_states.7, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:360:0\n",
      "  %189 : Long(device=cpu) = aten::size(%hidden_states.7, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:360:0\n",
      "  %x.13 : Tensor = aten::linear(%hidden_states.7, %encoder.layer.1.attention.self.query.weight, %encoder.layer.1.attention.self.query.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self/torch.nn.modules.linear.Linear::query # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %193 : Long(device=cpu) = aten::size(%x.13, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %194 : Long(device=cpu) = aten::size(%x.13, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %195 : int[] = prim::ListConstruct(%193, %194, %336, %330), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %x.15 : Tensor = aten::view(%x.13, %195), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:219:0\n",
      "  %query_layer : Tensor = aten::permute(%x.15, %48), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:220:0\n",
      "  %x.17 : Tensor = aten::linear(%hidden_states.7, %encoder.layer.1.attention.self.key.weight, %encoder.layer.1.attention.self.key.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self/torch.nn.modules.linear.Linear::key # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %201 : Long(device=cpu) = aten::size(%x.17, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %202 : Long(device=cpu) = aten::size(%x.17, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %203 : int[] = prim::ListConstruct(%201, %202, %336, %330), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %x.19 : Tensor = aten::view(%x.17, %203), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:219:0\n",
      "  %key_layer : Tensor = aten::permute(%x.19, %48), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:220:0\n",
      "  %x.21 : Tensor = aten::linear(%hidden_states.7, %encoder.layer.1.attention.self.value.weight, %encoder.layer.1.attention.self.value.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self/torch.nn.modules.linear.Linear::value # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %209 : Long(device=cpu) = aten::size(%x.21, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %210 : Long(device=cpu) = aten::size(%x.21, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %211 : int[] = prim::ListConstruct(%209, %210, %336, %330), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %x : Tensor = aten::view(%x.21, %211), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:219:0\n",
      "  %value_layer : Tensor = aten::permute(%x, %48), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:220:0\n",
      "  %attn_output.5 : Tensor = aten::scaled_dot_product_attention(%query_layer, %key_layer, %value_layer, %attention_mask, %329, %342, %53, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:407:0\n",
      "  %attn_output : Tensor = aten::transpose(%attn_output.5, %344, %336), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:416:0\n",
      "  %216 : int[] = prim::ListConstruct(%188, %189, %340), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self\n",
      "  %input.25 : Tensor = aten::reshape(%attn_output, %216), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSdpaSelfAttention::self # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:417:0\n",
      "  %input.27 : Tensor = aten::linear(%input.25, %encoder.layer.1.attention.output.dense.weight, %encoder.layer.1.attention.output.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %hidden_states.9 : Tensor = aten::dropout(%input.27, %337, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.dropout.Dropout::dropout # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425:0\n",
      "  %input.29 : Tensor = aten::add(%hidden_states.9, %hidden_states.7, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:435:0\n",
      "  %input.31 : Tensor = aten::layer_norm(%input.29, %60, %encoder.layer.1.attention.output.LayerNorm.weight, %encoder.layer.1.attention.output.LayerNorm.bias, %339, %338), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2910:0\n",
      "  %input.33 : Tensor = aten::linear(%input.31, %encoder.layer.1.intermediate.dense.weight, %encoder.layer.1.intermediate.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %input.35 : Tensor = aten::gelu(%input.33, %46), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertIntermediate::intermediate/transformers.activations.GELUActivation::intermediate_act_fn # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/activations.py:69:0\n",
      "  %input.37 : Tensor = aten::linear(%input.35, %encoder.layer.1.output.dense.weight, %encoder.layer.1.output.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %hidden_states.11 : Tensor = aten::dropout(%input.37, %337, %342), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.dropout.Dropout::dropout # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1425:0\n",
      "  %input.39 : Tensor = aten::add(%hidden_states.11, %input.31, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertOutput::output # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:521:0\n",
      "  %hidden_states : Tensor = aten::layer_norm(%input.39, %60, %encoder.layer.1.output.LayerNorm.weight, %encoder.layer.1.output.LayerNorm.bias, %339, %338), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::1/transformers.models.bert.modeling_bert.BertOutput::output/torch.nn.modules.normalization.LayerNorm::LayerNorm # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2910:0\n",
      "  %244 : Tensor = aten::slice(%hidden_states, %345, %345, %343, %344), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertPooler::pooler # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:713:0\n",
      "  %input.41 : Tensor = aten::select(%244, %344, %345), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertPooler::pooler # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:713:0\n",
      "  %input : Tensor = aten::linear(%input.41, %pooler.dense.weight, %pooler.dense.bias), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertPooler::pooler/torch.nn.modules.linear.Linear::dense # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %249 : Tensor = aten::tanh(%input), scope: transformers.models.bert.modeling_bert.BertModel::/transformers.models.bert.modeling_bert.BertPooler::pooler/torch.nn.modules.activation.Tanh::activation # /mnt/d/BYOM/modelexport/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:392:0\n",
      "  %250 : Dict(str, Tensor) = prim::DictConstruct(%45, %hidden_states, %44, %249), scope: transformers.models.b"
     ]
    }
   ],
   "source": [
    "# Try to access the full IR graph\n",
    "print(\"Trying to access detailed graph representation...\\n\")\n",
    "\n",
    "# Method 1: Get the inlined graph (this might have scope info)\n",
    "try:\n",
    "    if hasattr(traced_model, 'inlined_graph'):\n",
    "        inlined_graph = traced_model.inlined_graph\n",
    "        print(f\"Found inlined_graph: {type(inlined_graph)}\")\n",
    "        \n",
    "        inlined_nodes = list(inlined_graph.nodes())\n",
    "        print(f\"Inlined graph has {len(inlined_nodes)} nodes\")\n",
    "        \n",
    "        # Check first few nodes for scope info\n",
    "        for i, node in enumerate(inlined_nodes[:5]):\n",
    "            scope_name = str(node.scopeName()) if hasattr(node, 'scopeName') else \"No scope\"\n",
    "            print(f\"  Inlined Node {i}: {node.kind()} - Scope: {scope_name}\")\n",
    "    else:\n",
    "        print(\"No inlined_graph attribute found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing inlined_graph: {e}\")\n",
    "\n",
    "# Method 2: Try to trigger an ONNX export to see the detailed IR in the error\n",
    "print(\"\\n--- Attempting ONNX export to reveal detailed IR ---\")\n",
    "try:\n",
    "    onnx_file = \"../temp/debug_onnx_export.onnx\"\n",
    "    \n",
    "    if tracing_method == \"positional_trace\":\n",
    "        torch.onnx.export(\n",
    "            traced_model,\n",
    "            (inputs['input_ids'], inputs['attention_mask']),\n",
    "            onnx_file,\n",
    "            input_names=['input_ids', 'attention_mask'],\n",
    "            output_names=['output'],\n",
    "            do_constant_folding=False,  # Keep constants separate\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"‚úÖ ONNX export successful (unexpectedly!)\")\n",
    "    else:\n",
    "        print(\"Skipping ONNX export for wrapper method (would fail)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    error_str = str(e)\n",
    "    print(f\"ONNX export failed as expected. Analyzing error for scope info...\")\n",
    "    \n",
    "    # Look for scope information in the error message\n",
    "    if \"scope:\" in error_str:\n",
    "        print(\"\\nüéØ FOUND SCOPE INFORMATION IN ERROR!\")\n",
    "        \n",
    "        # Extract lines containing scope information\n",
    "        scope_lines = [line.strip() for line in error_str.split('\\n') if 'scope:' in line]\n",
    "        \n",
    "        print(f\"Found {len(scope_lines)} lines with scope information:\")\n",
    "        for i, line in enumerate(scope_lines[:10]):  # Show first 10\n",
    "            print(f\"  {i+1}. {line}\")\n",
    "        \n",
    "        if len(scope_lines) > 10:\n",
    "            print(f\"  ... and {len(scope_lines) - 10} more\")\n",
    "            \n",
    "        # Extract unique scopes\n",
    "        unique_scopes = set()\n",
    "        for line in scope_lines:\n",
    "            if 'scope:' in line:\n",
    "                scope_part = line.split('scope:')[1].split('#')[0].strip()\n",
    "                unique_scopes.add(scope_part)\n",
    "        \n",
    "        print(f\"\\nüéØ Unique scopes found: {len(unique_scopes)}\")\n",
    "        for scope in sorted(list(unique_scopes))[:15]:  # Show first 15\n",
    "            print(f\"  {scope}\")\n",
    "    else:\n",
    "        print(\"No scope information found in error message.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Alternative Approach: torch.fx Tracing\n",
    "\n",
    "Let's try torch.fx which might preserve more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying torch.fx symbolic tracing...\n",
      "\n",
      "1. Attempting symbolic tracing...\n",
      "  ‚ùå Symbolic tracing failed: You cannot specify both input_ids and inputs_embeds at the same time\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying torch.fx symbolic tracing...\\n\")\n",
    "\n",
    "try:\n",
    "    import torch.fx as fx\n",
    "    \n",
    "    # Try to symbolically trace the model\n",
    "    print(\"1. Attempting symbolic tracing...\")\n",
    "    \n",
    "    # This might fail for transformers models, but let's try\n",
    "    try:\n",
    "        fx_traced = fx.symbolic_trace(model)\n",
    "        print(\"  ‚úÖ Symbolic tracing successful!\")\n",
    "        \n",
    "        # Explore FX graph\n",
    "        fx_graph = fx_traced.graph\n",
    "        fx_nodes = list(fx_graph.nodes)\n",
    "        \n",
    "        print(f\"  FX graph has {len(fx_nodes)} nodes\")\n",
    "        \n",
    "        # Show first few nodes\n",
    "        for i, node in enumerate(fx_nodes[:10]):\n",
    "            print(f\"    Node {i}: {node.op} - {node.target} - {node.name}\")\n",
    "            if hasattr(node, 'meta') and node.meta:\n",
    "                print(f\"      Meta: {node.meta}\")\n",
    "                \n",
    "        # Try torch.export (PyTorch 2.0+)\n",
    "        print(\"\\n2. Attempting torch.export...\")\n",
    "        try:\n",
    "            exported_program = torch.export.export(model, (inputs['input_ids'], inputs['attention_mask']))\n",
    "            print(\"  ‚úÖ torch.export successful!\")\n",
    "            \n",
    "            export_graph = exported_program.module.graph\n",
    "            export_nodes = list(export_graph.nodes)\n",
    "            \n",
    "            print(f\"  Export graph has {len(export_nodes)} nodes\")\n",
    "            \n",
    "            # Show first few nodes with metadata\n",
    "            for i, node in enumerate(export_nodes[:5]):\n",
    "                print(f\"    Export Node {i}: {node.op} - {node.target}\")\n",
    "                if hasattr(node, 'meta') and node.meta:\n",
    "                    print(f\"      Meta keys: {list(node.meta.keys())}\")\n",
    "                    if 'stack_trace' in node.meta:\n",
    "                        print(f\"      Stack trace available: {len(node.meta['stack_trace'])} frames\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå torch.export failed: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Symbolic tracing failed: {e}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"torch.fx not available in this PyTorch version\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with torch.fx: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results = {\n",
    "    \"model_name\": model_name,\n",
    "    \"tracing_method\": tracing_method,\n",
    "    \"total_nodes\": len(nodes),\n",
    "    \"graph_string_length\": len(str(graph)),\n",
    "    \"node_analysis\": node_info,\n",
    "    \"exploration_summary\": {\n",
    "        \"torch_jit_scope_available\": any(info.get(\"scopeName()\", \"\") != \"\" for info in node_info),\n",
    "        \"methods_attempted\": list(scope_methods.keys()) if 'scope_methods' in locals() else [],\n",
    "        \"graph_string_contains_scope\": \"scope:\" in str(graph),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_file = \"../temp/torchscript_exploration_notebook.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Tracing method: {tracing_method}\")\n",
    "print(f\"Total TorchScript nodes: {len(nodes)}\")\n",
    "print(f\"Graph string length: {len(str(graph))} chars\")\n",
    "print(f\"Contains 'scope:' in graph string: {'scope:' in str(graph)}\")\n",
    "\n",
    "scope_available = any(info.get(\"scopeName()\", \"\") not in [\"\", \"No method\"] for info in node_info)\n",
    "print(f\"Scope information accessible via scopeName(): {scope_available}\")\n",
    "\n",
    "print(\"\\nüéØ KEY FINDINGS:\")\n",
    "if \"scope:\" in str(graph):\n",
    "    print(\"‚úÖ Scope information IS present in the graph string representation\")\n",
    "    print(\"‚úÖ We can extract module hierarchy from TorchScript before ONNX export\")\n",
    "else:\n",
    "    print(\"‚ùå No scope information found in graph representation\")\n",
    "    \n",
    "if scope_available:\n",
    "    print(\"‚úÖ Individual node scope access working\")\n",
    "else:\n",
    "    print(\"‚ùå Individual node scopeName() not working - need alternative approach\")\n",
    "\n",
    "print(\"\\nüí° NEXT STEPS:\")\n",
    "print(\"1. Parse graph string representation to extract scope information\")\n",
    "print(\"2. Implement JIT graph interception before ONNX conversion\")\n",
    "print(\"3. Create hierarchy mapping from TorchScript to ONNX nodes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
