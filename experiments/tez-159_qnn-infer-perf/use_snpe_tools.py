#!/usr/bin/env python3
"""
Use SNPE Tools to Create DLC Model
SNPE (Snapdragon Neural Processing Engine) is part of QNN SDK
Let's try using SNPE tools to convert ONNX to DLC
"""

import os
import sys
import subprocess
import time
import struct
import json
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

QNN_SDK_ROOT = Path("C:/Qualcomm/AIStack/qairt/2.34.0.250424")
OUTPUT_DIR = Path("./snpe_tools_output")
OUTPUT_DIR.mkdir(exist_ok=True)


def create_simple_onnx_model():
    """Create a simple ONNX model for conversion"""
    logger.info("Creating simple ONNX model...")
    
    try:
        import onnx
        from onnx import helper, TensorProto
        
        # Create simplest possible model: [1] -> [1] Identity
        input_tensor = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1])
        output_tensor = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1])
        
        # Identity node
        identity_node = helper.make_node('Identity', ['input'], ['output'], name='identity')
        
        # Create graph
        graph_def = helper.make_graph(
            nodes=[identity_node],
            name='SimpleIdentity',
            inputs=[input_tensor],
            outputs=[output_tensor]
        )
        
        # Create model
        model_def = helper.make_model(graph_def, producer_name='SNPE-Test')
        model_def.opset_import[0].version = 11
        
        # Verify model
        onnx.checker.check_model(model_def)
        
        # Save model
        onnx_path = OUTPUT_DIR / "simple_identity.onnx"
        onnx.save(model_def, str(onnx_path))
        
        logger.info(f"‚úì Created ONNX model: {onnx_path}")
        logger.info(f"  Model size: {onnx_path.stat().st_size} bytes")
        return onnx_path
        
    except Exception as e:
        logger.error(f"Failed to create ONNX model: {e}")
        return None


def try_snpe_onnx_to_dlc(onnx_path):
    """Try using SNPE ONNX to DLC converter"""
    logger.info("Attempting SNPE ONNX to DLC conversion...")
    
    # Try x64 version first (might work on ARM64 Windows)
    snpe_converter = QNN_SDK_ROOT / "bin" / "x86_64-windows-msvc" / "snpe-onnx-to-dlc"
    
    if not snpe_converter.exists():
        logger.error("SNPE ONNX converter not found")
        return None
    
    # DLC output path
    dlc_path = OUTPUT_DIR / "snpe_identity.dlc"
    
    # SNPE conversion command
    cmd = [
        "python",  # SNPE tools are Python scripts
        str(snpe_converter),
        "--input_network", str(onnx_path),
        "--output_path", str(dlc_path),
        "--input_dim", "input", "1"
    ]
    
    # Set up environment for SNPE
    env = os.environ.copy()
    env['SNPE_ROOT'] = str(QNN_SDK_ROOT)
    env['PATH'] = f"{QNN_SDK_ROOT / 'bin' / 'x86_64-windows-msvc'};{env.get('PATH', '')}"
    
    logger.info(f"Running: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60, env=env)
        
        logger.info(f"Return code: {result.returncode}")
        if result.stdout:
            logger.info(f"STDOUT:\n{result.stdout}")
        if result.stderr:
            logger.info(f"STDERR:\n{result.stderr}")
        
        if result.returncode == 0 and dlc_path.exists():
            logger.info(f"‚úÖ SUCCESS: Created DLC with SNPE!")
            logger.info(f"  DLC path: {dlc_path}")
            logger.info(f"  DLC size: {dlc_path.stat().st_size} bytes")
            return dlc_path
        else:
            logger.warning("‚ùå SNPE conversion failed")
            return None
            
    except subprocess.TimeoutExpired:
        logger.error("SNPE conversion timed out")
    except Exception as e:
        logger.error(f"SNPE conversion error: {e}")
    
    return None


def try_snpe_dlc_tools():
    """Try using other SNPE DLC tools"""
    logger.info("Exploring SNPE DLC tools...")
    
    tools_dir = QNN_SDK_ROOT / "bin" / "x86_64-windows-msvc"
    
    # Test snpe-dlc-info on one of the pre-built models
    model_bin = QNN_SDK_ROOT / "examples" / "QNN" / "converter" / "models" / "qnn_model_float.bin"
    
    if model_bin.exists():
        logger.info(f"Analyzing pre-built model with SNPE tools...")
        
        # Try snpe-dlc-info
        dlc_info = tools_dir / "snpe-dlc-info"
        if dlc_info.exists():
            cmd = ["python", str(dlc_info), str(model_bin)]
            
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    logger.info("Model analysis:")
                    logger.info(result.stdout[:500])
                elif "not a valid DLC" in result.stderr:
                    logger.info("Pre-built .bin files are not DLC format")
            except:
                pass
    
    # Check what formats SNPE supports
    logger.info("Available SNPE converters:")
    converters = [
        "snpe-onnx-to-dlc",
        "snpe-tensorflow-to-dlc", 
    ]
    
    for converter in converters:
        converter_path = tools_dir / converter
        if converter_path.exists():
            logger.info(f"  ‚úì {converter}")
            
            # Get help
            try:
                result = subprocess.run(
                    ["python", str(converter_path), "--help"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0 and result.stdout:
                    # Show key options
                    lines = result.stdout.split('\n')
                    for line in lines[:10]:
                        if 'input' in line.lower() or 'output' in line.lower():
                            logger.info(f"    {line.strip()}")
            except:
                pass
        else:
            logger.info(f"  ‚ùå {converter}")


def run_npu_inference_with_snpe_dlc(dlc_path):
    """Test NPU inference using SNPE-created DLC"""
    logger.info(f"Testing NPU inference with SNPE DLC: {dlc_path}")
    
    # Create input data
    input_file = OUTPUT_DIR / "input.raw"
    with open(input_file, 'wb') as f:
        f.write(struct.pack('f', 2.5))  # Single float
    
    # Create input list
    input_list = OUTPUT_DIR / "input_list.txt"
    with open(input_list, 'w') as f:
        f.write(f"input {input_file}\n")
    
    # Test with both backends
    net_run = QNN_SDK_ROOT / "bin" / "aarch64-windows-msvc" / "qnn-net-run.exe"
    backends = {
        'HTP/NPU': QNN_SDK_ROOT / "lib" / "aarch64-windows-msvc" / "QnnHtp.dll",
        'CPU': QNN_SDK_ROOT / "lib" / "aarch64-windows-msvc" / "QnnCpu.dll"
    }
    
    results = {}
    
    for backend_name, backend_dll in backends.items():
        logger.info(f"  Testing {backend_name} backend...")
        
        cmd = [
            str(net_run),
            "--model", str(dlc_path),
            "--backend", str(backend_dll),
            "--input_list", str(input_list),
            "--output_dir", str(OUTPUT_DIR),
            "--perf_profile", "extreme_performance",
            "--profiling_level", "detailed"
        ]
        
        start = time.perf_counter()
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            elapsed_ms = (time.perf_counter() - start) * 1000
            
            if result.returncode == 0:
                logger.info(f"    ‚úÖ {backend_name} SUCCESS!")
                logger.info(f"    Inference time: {elapsed_ms:.2f}ms")
                
                # Check for output files
                outputs = list(OUTPUT_DIR.glob("Result_*"))
                if outputs:
                    logger.info(f"    Generated {len(outputs)} output files")
                    
                    # Read output value
                    with open(outputs[0], 'rb') as f:
                        output_val = struct.unpack('f', f.read(4))[0]
                        logger.info(f"    Output: {output_val} (input was 2.5)")
                
                # Look for profiling data
                profile_files = list(OUTPUT_DIR.glob("*.json"))
                for pf in profile_files:
                    logger.info(f"    Profile data: {pf.name}")
                    
                    # Parse key metrics
                    try:
                        with open(pf) as f:
                            profile = json.load(f)
                            
                        # Extract key metrics
                        if isinstance(profile, dict):
                            if 'executionSummary' in profile:
                                summary = profile['executionSummary']
                                if 'totalInferenceTime' in summary:
                                    logger.info(f"      Total inference: {summary['totalInferenceTime']}Œºs")
                            
                            if 'hardwareAccelerator' in str(profile).lower() or 'htp' in str(profile).lower():
                                logger.info(f"      ‚úì HTP acceleration detected!")
                                
                    except Exception as e:
                        logger.debug(f"Could not parse profile: {e}")
                
                results[backend_name] = {
                    'success': True,
                    'time_ms': elapsed_ms,
                    'outputs': len(outputs)
                }
                
            else:
                logger.warning(f"    ‚ùå {backend_name} failed")
                if result.stderr:
                    logger.debug(f"    Error: {result.stderr[:200]}")
                results[backend_name] = {'success': False}
                
        except subprocess.TimeoutExpired:
            logger.error(f"    {backend_name} timed out")
            results[backend_name] = {'success': False}
        except Exception as e:
            logger.error(f"    {backend_name} error: {e}")
            results[backend_name] = {'success': False}
    
    return results


def main():
    """Main workflow using SNPE tools"""
    logger.info("="*80)
    logger.info("USING SNPE TOOLS TO CREATE DLC MODEL")
    logger.info("="*80)
    
    # Step 1: Explore available tools
    try_snpe_dlc_tools()
    
    # Step 2: Create ONNX model
    onnx_path = create_simple_onnx_model()
    if not onnx_path:
        logger.error("‚ùå Could not create ONNX model")
        return False
    
    # Step 3: Convert ONNX to DLC using SNPE
    dlc_path = try_snpe_onnx_to_dlc(onnx_path)
    if not dlc_path:
        logger.error("‚ùå Could not convert ONNX to DLC")
        return False
    
    # Step 4: Test real NPU inference
    logger.info("\n" + "="*80)
    logger.info("TESTING REAL NPU INFERENCE")
    logger.info("="*80)
    
    results = run_npu_inference_with_snpe_dlc(dlc_path)
    
    # Summary
    logger.info("\n" + "="*80)
    logger.info("RESULTS")
    logger.info("="*80)
    
    if results.get('HTP/NPU', {}).get('success'):
        logger.info("üéâ BREAKTHROUGH: Real NPU inference achieved!")
        logger.info("‚úÖ Successfully created DLC model using SNPE tools!")
        logger.info("‚úÖ Successfully ran neural network on NPU hardware!")
        
        if results.get('CPU', {}).get('success'):
            npu_time = results['HTP/NPU']['time_ms']
            cpu_time = results['CPU']['time_ms']
            speedup = cpu_time / npu_time if npu_time > 0 else 1.0
            
            logger.info(f"\nüìä Performance Comparison:")
            logger.info(f"  NPU inference: {npu_time:.2f}ms")
            logger.info(f"  CPU inference: {cpu_time:.2f}ms")
            logger.info(f"  NPU speedup: {speedup:.2f}x")
        
        logger.info(f"\nüéØ Achievement: All QNN metrics are now unlocked!")
        logger.info(f"We can now measure:")
        logger.info(f"  ‚Ä¢ Inference timing and throughput")
        logger.info(f"  ‚Ä¢ Memory usage and bandwidth") 
        logger.info(f"  ‚Ä¢ HVX/HMX utilization")
        logger.info(f"  ‚Ä¢ Layer-level profiling")
        logger.info(f"  ‚Ä¢ All 36 QNN SDK metrics!")
        
        return True
    else:
        logger.error("‚ùå NPU inference still not working")
        logger.info("SNPE tools may have similar dependency issues")
        return False


if __name__ == "__main__":
    success = main()
    
    if success:
        logger.info("\n" + "üéä"*40)
        logger.info("MISSION ACCOMPLISHED!")
        logger.info("Real NPU inference with DLC model achieved!")
        logger.info("All QNN metrics are now accessible!")
        logger.info("üéä"*40)
    else:
        logger.info("\nüìù SNPE tools also require proper Python environment")
        logger.info("The core issue is Python/DLL compatibility on Windows ARM64")