{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Modules as Functions Analysis\n",
    "\n",
    "This notebook analyzes the `export_modules_as_functions` parameter in `torch.onnx.export()` to understand:\n",
    "\n",
    "1. **What it does**: How it affects ONNX export structure\n",
    "2. **Comparison**: Function=True vs Function=False behavior\n",
    "3. **Hierarchy Preservation**: Whether it helps with our modelexport requirements\n",
    "4. **Limitations**: Why it may not solve our specific use case\n",
    "\n",
    "## Key Research Findings\n",
    "\n",
    "From torch.onnx source code analysis:\n",
    "- `export_modules_as_functions=True` exports PyTorch modules as ONNX local functions\n",
    "- Preserves module boundaries rather than flattening to individual operations\n",
    "- Uses PyTorch's internal `torch.jit._trace._trace_module_map` infrastructure\n",
    "- **Deprecated** but still functional, requires opset_version >= 15\n",
    "- Works at **module level**, not operation level like our HTP strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppress deprecation warnings for clarity\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"../../temp/export_functions_experiment\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Experiment output directory: {output_dir.absolute()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Definition\n",
    "\n",
    "We'll create a hierarchical model with clear module boundaries to observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Simple attention head for testing module boundary preservation.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # Simple scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.embed_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network for testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.activation(self.linear1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Simple transformer block with clear module hierarchy.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention = AttentionHead(embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention with residual connection\n",
    "        attn_out = self.attention(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TestTransformer(nn.Module):\n",
    "    \"\"\"Complete test model with hierarchical structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 64, hidden_dim: int = 128, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(1000, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_proj = nn.Linear(embed_dim, 10)  # 10 classes\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        return self.output_proj(x)\n",
    "\n",
    "# Create model and sample input\n",
    "model = TestTransformer(embed_dim=64, hidden_dim=128, num_layers=2)\n",
    "model.eval()\n",
    "\n",
    "# Sample input: batch_size=2, seq_len=8\n",
    "sample_input = torch.randint(0, 1000, (2, 8))\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Model hierarchy:\")\n",
    "for name, module in model.named_modules():\n",
    "    if name:  # Skip root module\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Export with export_modules_as_functions=False (Default)\n",
    "\n",
    "First, let's export with the default behavior where modules are flattened into individual operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_and_analyze(model, sample_input, export_modules_as_functions, suffix: str):\n",
    "    \"\"\"Export model and analyze the resulting ONNX structure.\"\"\"\n",
    "    \n",
    "    output_path = output_dir / f\"test_model_{suffix}.onnx\"\n",
    "    \n",
    "    print(f\"\\n=== Exporting with export_modules_as_functions={export_modules_as_functions} ===\")\n",
    "    \n",
    "    # Export model\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            sample_input,\n",
    "            output_path,\n",
    "            export_params=True,\n",
    "            opset_version=17,  # Required for local functions\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input_ids'],\n",
    "            output_names=['logits'],\n",
    "            dynamic_axes={\n",
    "                'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                'logits': {0: 'batch_size'}\n",
    "            },\n",
    "            export_modules_as_functions=export_modules_as_functions,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    # Load and analyze ONNX model\n",
    "    onnx_model = onnx.load(str(output_path))\n",
    "    \n",
    "    # Basic statistics\n",
    "    graph = onnx_model.graph\n",
    "    num_nodes = len(graph.node)\n",
    "    num_initializers = len(graph.initializer)\n",
    "    num_functions = len(onnx_model.functions) if hasattr(onnx_model, 'functions') else 0\n",
    "    \n",
    "    print(f\"ONNX Model Statistics:\")\n",
    "    print(f\"  Nodes: {num_nodes}\")\n",
    "    print(f\"  Initializers: {num_initializers}\")\n",
    "    print(f\"  Local Functions: {num_functions}\")\n",
    "    \n",
    "    # Analyze node types\n",
    "    node_types = {}\n",
    "    for node in graph.node:\n",
    "        op_type = node.op_type\n",
    "        node_types[op_type] = node_types.get(op_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nNode Types Distribution:\")\n",
    "    for op_type, count in sorted(node_types.items()):\n",
    "        print(f\"  {op_type}: {count}\")\n",
    "    \n",
    "    # Analyze local functions if present\n",
    "    if num_functions > 0:\n",
    "        print(f\"\\nLocal Functions:\")\n",
    "        for func in onnx_model.functions:\n",
    "            print(f\"  {func.name} (domain: {func.domain})\")\n",
    "            print(f\"    Nodes: {len(func.node)}\")\n",
    "            func_node_types = {}\n",
    "            for node in func.node:\n",
    "                op_type = node.op_type\n",
    "                func_node_types[op_type] = func_node_types.get(op_type, 0) + 1\n",
    "            print(f\"    Node types: {dict(func_node_types)}\")\n",
    "    \n",
    "    # Test model execution\n",
    "    try:\n",
    "        session = ort.InferenceSession(str(output_path))\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        result = session.run(None, {input_name: sample_input.numpy()})\n",
    "        print(f\"\\nModel execution successful!\")\n",
    "        print(f\"Output shape: {result[0].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nModel execution failed: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'path': str(output_path),\n",
    "        'num_nodes': num_nodes,\n",
    "        'num_initializers': num_initializers,\n",
    "        'num_functions': num_functions,\n",
    "        'node_types': node_types\n",
    "    }\n",
    "\n",
    "# Export with default behavior (functions=False)\n",
    "result_false = export_and_analyze(model, sample_input, False, \"functions_false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Export with export_modules_as_functions=True\n",
    "\n",
    "Now let's export with modules preserved as ONNX local functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export with modules as functions (functions=True)\n",
    "result_true = export_and_analyze(model, sample_input, True, \"functions_true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Selective Module Export\n",
    "\n",
    "Test exporting only specific module types as functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export only specific modules as functions\n",
    "selective_modules = {AttentionHead, FeedForward}\n",
    "result_selective = export_and_analyze(model, sample_input, selective_modules, \"functions_selective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Analysis\n",
    "\n",
    "Let's compare the three approaches and analyze the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(result_false, result_true, result_selective):\n",
    "    \"\"\"Compare the three export approaches.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<20} {'Functions=False':<15} {'Functions=True':<15} {'Selective':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Nodes':<20} {result_false['num_nodes']:<15} {result_true['num_nodes']:<15} {result_selective['num_nodes']:<15}\")\n",
    "    print(f\"{'Initializers':<20} {result_false['num_initializers']:<15} {result_true['num_initializers']:<15} {result_selective['num_initializers']:<15}\")\n",
    "    print(f\"{'Local Functions':<20} {result_false['num_functions']:<15} {result_true['num_functions']:<15} {result_selective['num_functions']:<15}\")\n",
    "    \n",
    "    # Analyze the differences\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY OBSERVATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if result_true['num_functions'] > 0:\n",
    "        print(f\"✓ Functions=True successfully created {result_true['num_functions']} local functions\")\n",
    "        print(f\"✓ This represents module-level preservation of hierarchy\")\n",
    "    else:\n",
    "        print(\"✗ Functions=True did not create local functions (possible version/compatibility issue)\")\n",
    "    \n",
    "    if result_selective['num_functions'] > 0:\n",
    "        print(f\"✓ Selective export created {result_selective['num_functions']} functions\")\n",
    "        print(f\"✓ This allows fine-grained control over which modules become functions\")\n",
    "    \n",
    "    # Node count differences\n",
    "    if result_true['num_nodes'] < result_false['num_nodes']:\n",
    "        reduction = result_false['num_nodes'] - result_true['num_nodes']\n",
    "        print(f\"✓ Functions=True reduced main graph nodes by {reduction} ({reduction/result_false['num_nodes']*100:.1f}%)\")\n",
    "        print(f\"✓ These operations were moved into local functions\")\n",
    "    elif result_true['num_nodes'] == result_false['num_nodes']:\n",
    "        print(f\"⚠ No reduction in main graph nodes - functions may not be working as expected\")\n",
    "\n",
    "compare_results(result_false, result_true, result_selective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Relevance to ModelExport Requirements\n",
    "\n",
    "Let's analyze whether `export_modules_as_functions` addresses our hierarchy preservation requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hierarchy_preservation():\n",
    "    \"\"\"Analyze how export_modules_as_functions relates to modelexport requirements.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHY PRESERVATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n🎯 MODELEXPORT PROJECT REQUIREMENTS:\")\n",
    "    print(\"   • Tag individual ONNX operations with their source PyTorch modules\")\n",
    "    print(\"   • Preserve fine-grained operation-to-module mapping\")\n",
    "    print(\"   • Enable traceability from ONNX ops back to original code\")\n",
    "    print(\"   • Support any HuggingFace model universally\")\n",
    "    \n",
    "    print(\"\\n🔍 EXPORT_MODULES_AS_FUNCTIONS BEHAVIOR:\")\n",
    "    print(\"   • Exports entire PyTorch modules as ONNX local functions\")\n",
    "    print(\"   • Preserves module boundaries, not operation boundaries\")\n",
    "    print(\"   • Groups multiple operations within each function\")\n",
    "    print(\"   • Functions contain the actual computational operations\")\n",
    "    \n",
    "    print(\"\\n⚖️  COMPARISON:\")\n",
    "    \n",
    "    print(\"\\n   GRANULARITY:\")\n",
    "    print(\"   • ModelExport HTP: Operation-level (MatMul, Add, etc. → Module)\")\n",
    "    print(\"   • export_modules_as_functions: Module-level (Entire module → Function)\")\n",
    "    \n",
    "    print(\"\\n   STRUCTURE:\")\n",
    "    print(\"   • ModelExport HTP: Flat graph with rich metadata tags\")\n",
    "    print(\"   • export_modules_as_functions: Hierarchical functions containing operations\")\n",
    "    \n",
    "    print(\"\\n   USE CASES:\")\n",
    "    print(\"   • ModelExport HTP: Fine-grained analysis, debugging, custom backends\")\n",
    "    print(\"   • export_modules_as_functions: Module replacement, logical grouping\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"VERDICT: COMPLEMENTARY BUT NOT EQUIVALENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n❌ DOES NOT REPLACE MODELEXPORT HTP STRATEGY:\")\n",
    "    print(\"   1. Different granularity - modules vs operations\")\n",
    "    print(\"   2. No operation-level traceability within functions\")\n",
    "    print(\"   3. Limited metadata propagation for individual ops\")\n",
    "    print(\"   4. Functions obscure internal operation structure\")\n",
    "    \n",
    "    print(\"\\n✅ POTENTIAL COMPLEMENTARY USE:\")\n",
    "    print(\"   1. Could be combined with HTP for dual-level hierarchy\")\n",
    "    print(\"   2. Module-level functions + operation-level tags\")\n",
    "    print(\"   3. Better organization for complex models\")\n",
    "    print(\"   4. Alternative export mode for different use cases\")\n",
    "    \n",
    "    print(\"\\n🔧 TECHNICAL LIMITATIONS:\")\n",
    "    print(\"   1. Deprecated feature - uncertain future support\")\n",
    "    print(\"   2. Requires opset_version >= 15\")\n",
    "    print(\"   3. May not work with all ONNX runtimes\")\n",
    "    print(\"   4. Less control over individual operation metadata\")\n",
    "\n",
    "analyze_hierarchy_preservation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed ONNX Structure Examination\n",
    "\n",
    "Let's examine the actual ONNX structures to understand the differences better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_onnx_structure(onnx_path: str, title: str):\n",
    "    \"\"\"Examine the detailed structure of an ONNX model.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"{title}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = onnx.load(onnx_path)\n",
    "    graph = model.graph\n",
    "    \n",
    "    # Show first few nodes to understand structure\n",
    "    print(f\"\\nFirst 10 nodes in main graph:\")\n",
    "    for i, node in enumerate(graph.node[:10]):\n",
    "        inputs_str = \", \".join(node.input[:2])  # First 2 inputs\n",
    "        if len(node.input) > 2:\n",
    "            inputs_str += f\", ... (+{len(node.input)-2} more)\"\n",
    "        print(f\"  {i+1:2d}. {node.op_type:<15} ({inputs_str})\")\n",
    "    \n",
    "    if len(graph.node) > 10:\n",
    "        print(f\"     ... ({len(graph.node) - 10} more nodes)\")\n",
    "    \n",
    "    # Examine local functions if present\n",
    "    if hasattr(model, 'functions') and len(model.functions) > 0:\n",
    "        print(f\"\\nLocal Functions ({len(model.functions)} total):\")\n",
    "        for i, func in enumerate(model.functions[:3]):  # Show first 3 functions\n",
    "            print(f\"\\n  Function {i+1}: {func.name}\")\n",
    "            print(f\"    Domain: {func.domain}\")\n",
    "            print(f\"    Inputs: {len(func.input)} | Outputs: {len(func.output)}\")\n",
    "            print(f\"    Nodes: {len(func.node)}\")\n",
    "            \n",
    "            # Show function's internal nodes\n",
    "            print(f\"    Internal operations:\")\n",
    "            for j, node in enumerate(func.node[:5]):  # First 5 nodes\n",
    "                print(f\"      {j+1}. {node.op_type}\")\n",
    "            if len(func.node) > 5:\n",
    "                print(f\"         ... (+{len(func.node)-5} more ops)\")\n",
    "        \n",
    "        if len(model.functions) > 3:\n",
    "            print(f\"\\n     ... ({len(model.functions) - 3} more functions)\")\n",
    "    else:\n",
    "        print(f\"\\nNo local functions found.\")\n",
    "\n",
    "# Examine all three exported models\n",
    "examine_onnx_structure(result_false['path'], \"FUNCTIONS=FALSE (Default Behavior)\")\n",
    "examine_onnx_structure(result_true['path'], \"FUNCTIONS=TRUE (Modules as Functions)\")\n",
    "examine_onnx_structure(result_selective['path'], \"SELECTIVE MODULES as Functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusions and Recommendations\n",
    "\n",
    "Based on our comprehensive analysis, let's draw final conclusions about `export_modules_as_functions` relevance to the modelexport project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_conclusions():\n",
    "    \"\"\"Present final conclusions and recommendations.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL CONCLUSIONS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n🎯 MAIN QUESTION: Does export_modules_as_functions help with modelexport requirements?\")\n",
    "    print(\"\\n📝 ANSWER: **PARTIALLY HELPFUL BUT NOT SUFFICIENT**\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"-\"*60)\n",
    "    print(\"WHY IT'S NOT SUFFICIENT FOR MODELEXPORT:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(\"\\n1. 🔍 GRANULARITY MISMATCH:\")\n",
    "    print(\"   • ModelExport needs: Operation → Module mapping (MatMul came from layer.attention.query)\")\n",
    "    print(\"   • export_modules_as_functions: Module → Function grouping (entire module becomes function)\")\n",
    "    print(\"   • Individual operations inside functions lose traceability\")\n",
    "    \n",
    "    print(\"\\n2. 🏗️ ARCHITECTURE DIFFERENCE:\")\n",
    "    print(\"   • ModelExport: Flat graph with rich metadata tags on each operation\")\n",
    "    print(\"   • export_modules_as_functions: Nested functions containing grouped operations\")\n",
    "    print(\"   • Can't trace specific ops within functions back to source code\")\n",
    "    \n",
    "    print(\"\\n3. 🎚️ USE CASE MISMATCH:\")\n",
    "    print(\"   • ModelExport target: Fine-grained debugging, custom backends, operation analysis\")\n",
    "    print(\"   • export_modules_as_functions target: Module replacement, logical organization\")\n",
    "    \n",
    "    print(\"\\n4. 🚫 TECHNICAL LIMITATIONS:\")\n",
    "    print(\"   • Deprecated feature with uncertain future\")\n",
    "    print(\"   • Limited ONNX runtime support\")\n",
    "    print(\"   • Less flexible than custom metadata approach\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"-\"*60)\n",
    "    print(\"POTENTIAL COMPLEMENTARY VALUE:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(\"\\n✅ COULD BE USEFUL AS ADDITIONAL FEATURE:\")\n",
    "    print(\"   • Dual-level hierarchy: Module functions + operation tags\")\n",
    "    print(\"   • Alternative export mode for different use cases\")\n",
    "    print(\"   • Better organization for very complex models\")\n",
    "    print(\"   • Module-level replacement capabilities\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n🎯 CONTINUE WITH CURRENT HTP STRATEGY AS PRIMARY APPROACH\")\n",
    "    print(\"\\n   Reasons:\")\n",
    "    print(\"   ✓ Provides the exact granularity needed (operation-level)\")\n",
    "    print(\"   ✓ Universal approach works with any model\")\n",
    "    print(\"   ✓ Flexible metadata system\")\n",
    "    print(\"   ✓ Better aligned with project requirements\")\n",
    "    \n",
    "    print(\"\\n🔧 CONSIDER export_modules_as_functions AS FUTURE ENHANCEMENT\")\n",
    "    print(\"\\n   Potential use cases:\")\n",
    "    print(\"   • Optional dual-level hierarchy export mode\")\n",
    "    print(\"   • Better organization for extremely complex models\")\n",
    "    print(\"   • Alternative for users who prefer function-based structure\")\n",
    "    print(\"   • Research into hybrid approaches\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"⚡\" * 80)\n",
    "    print(\"EXPERIMENT VALIDATES: HTP strategy addresses different, more granular need\")\n",
    "    print(\"⚡\" * 80)\n",
    "\n",
    "final_conclusions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Experiment Results\n",
    "\n",
    "Let's save the experiment results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment summary\n",
    "experiment_summary = {\n",
    "    \"experiment_date\": \"2025-01-07\",\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"onnx_version\": onnx.__version__,\n",
    "    \"test_model\": \"TestTransformer (hierarchical transformer-like model)\",\n",
    "    \"results\": {\n",
    "        \"functions_false\": result_false,\n",
    "        \"functions_true\": result_true,\n",
    "        \"functions_selective\": result_selective\n",
    "    },\n",
    "    \"conclusions\": {\n",
    "        \"main_finding\": \"export_modules_as_functions provides module-level hierarchy, not operation-level granularity needed by modelexport\",\n",
    "        \"recommendation\": \"Continue with HTP strategy as primary approach; consider export_modules_as_functions as future enhancement\",\n",
    "        \"granularity_difference\": \"export_modules_as_functions: module→function, modelexport: operation→module\",\n",
    "        \"use_case_alignment\": \"Different target use cases - module replacement vs operation traceability\"\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = output_dir / \"experiment_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(experiment_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Experiment results saved to: {summary_path}\")\n",
    "print(f\"\\n📁 All experiment files available in: {output_dir}\")\n",
    "print(f\"\\n🔍 Key finding: export_modules_as_functions operates at module level, not operation level\")\n",
    "print(f\"🎯 Recommendation: Continue with current HTP strategy for operation-level hierarchy preservation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}