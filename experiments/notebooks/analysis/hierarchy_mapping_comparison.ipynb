{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module ↔ Hierarchy Mapping: Static vs Dynamic Approaches\n",
    "\n",
    "This notebook compares the current static approach with the proposed dynamic hook-based approach for creating module-to-hierarchy mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/d/BYOM/modelexport')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load BERT-tiny for demonstration\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Current Approach: Static Analysis\n",
    "\n",
    "Creates hierarchy tags from module paths without execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Static Mapping Examples:\n",
      "================================================================================\n",
      "embeddings                               → /BertModel/BertEmbeddings\n",
      "embeddings.word_embeddings               → /BertModel/BertEmbeddings\n",
      "embeddings.position_embeddings           → /BertModel/BertEmbeddings\n",
      "embeddings.token_type_embeddings         → /BertModel/BertEmbeddings\n",
      "embeddings.LayerNorm                     → /BertModel/BertEmbeddings\n",
      "embeddings.dropout                       → /BertModel/BertEmbeddings\n",
      "encoder                                  → /BertModel/BertEncoder\n",
      "encoder.layer                            → /BertModel/BertEncoder\n",
      "encoder.layer.0                          → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.attention                → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.self           → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.self.query     → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.self.key       → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.self.value     → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.self.dropout   → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.output         → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.output.dense   → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.output.LayerNorm → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.output.dropout → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.intermediate             → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.intermediate.dense       → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.intermediate.intermediate_act_fn → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.output                   → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.output.dense             → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.output.LayerNorm         → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.output.dropout           → /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.1                          → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.attention                → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.self           → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.self.query     → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.self.key       → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.self.value     → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.self.dropout   → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.output         → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.output.dense   → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.output.LayerNorm → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.output.dropout → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.intermediate             → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.intermediate.dense       → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.intermediate.intermediate_act_fn → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.output                   → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.output.dense             → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.output.LayerNorm         → /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.output.dropout           → /BertModel/BertEncoder/BertLayer.1\n",
      "pooler                                   → /BertModel\n",
      "pooler.dense                             → /BertModel\n",
      "pooler.activation                        → /BertModel\n",
      "\n",
      "Total mappings: 47\n"
     ]
    }
   ],
   "source": [
    "def static_hierarchy_mapping(model: nn.Module) -> Dict[str, str]:\n",
    "    \"\"\"Current approach: Static module path → hierarchy tag mapping\"\"\"\n",
    "    \n",
    "    mapping = {}\n",
    "    \n",
    "    # Static analysis - no forward pass needed\n",
    "    for name, module in model.named_modules():\n",
    "        if name:  # Skip root\n",
    "            # Parse path and generate hierarchy tag\n",
    "            hierarchy_tag = generate_hierarchy_tag(name, module)\n",
    "            mapping[name] = hierarchy_tag\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def generate_hierarchy_tag(module_path: str, module: nn.Module) -> str:\n",
    "    \"\"\"Simplified tag generation for demo\"\"\"\n",
    "    \n",
    "    # Simplified version of our algorithm\n",
    "    segments = module_path.split('.')\n",
    "    hierarchy_parts = [\"BertModel\"]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        segment = segments[i]\n",
    "        \n",
    "        if segment == \"encoder\":\n",
    "            hierarchy_parts.append(\"BertEncoder\")\n",
    "        elif segment == \"layer\":\n",
    "            # Skip ModuleList\n",
    "            pass\n",
    "        elif segment.isdigit():\n",
    "            # Instance number\n",
    "            hierarchy_parts.append(f\"BertLayer.{segment}\")\n",
    "        elif segment == \"attention\":\n",
    "            hierarchy_parts.append(\"BertAttention\")\n",
    "        elif segment == \"embeddings\":\n",
    "            hierarchy_parts.append(\"BertEmbeddings\")\n",
    "        # ... etc\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return \"/\" + \"/\".join(hierarchy_parts)\n",
    "\n",
    "# Demo static mapping\n",
    "static_mapping = static_hierarchy_mapping(model)\n",
    "\n",
    "print(\"📊 Static Mapping Examples:\")\n",
    "print(\"=\" * 80)\n",
    "examples = list(static_mapping.items())[:]\n",
    "for path, tag in examples:\n",
    "    print(f\"{path:<40} → {tag}\")\n",
    "    \n",
    "print(f\"\\nTotal mappings: {len(static_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎣 Proposed Approach: Dynamic Hook-Based Mapping\n",
    "\n",
    "Uses forward hooks to capture actual execution and map to ONNX operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎣 Dynamic Mapping with Forward Hooks:\n",
      "================================================================================\n",
      "Executed 45 modules\n",
      "\n",
      "First 10 executed modules:\n",
      " 1. embeddings.word_embeddings          → /BertModel/BertEmbeddings\n",
      " 2. embeddings.token_type_embeddings    → /BertModel/BertEmbeddings\n",
      " 3. embeddings.position_embeddings      → /BertModel/BertEmbeddings\n",
      " 4. embeddings.LayerNorm                → /BertModel/BertEmbeddings\n",
      " 5. embeddings.dropout                  → /BertModel/BertEmbeddings\n",
      " 6. embeddings                          → /BertModel/BertEmbeddings\n",
      " 7. encoder.layer.0.attention.self.query → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      " 8. encoder.layer.0.attention.self.key  → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      " 9. encoder.layer.0.attention.self.value → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "10. encoder.layer.0.attention.self      → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "11. encoder.layer.0.attention.output.dense → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "12. encoder.layer.0.attention.output.dropout → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "13. encoder.layer.0.attention.output.LayerNorm → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "14. encoder.layer.0.attention.output    → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "15. encoder.layer.0.attention           → /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "16. encoder.layer.0.intermediate.dense  → /BertModel/BertEncoder/BertLayer.0\n",
      "17. encoder.layer.0.intermediate.intermediate_act_fn → /BertModel/BertEncoder/BertLayer.0\n",
      "18. encoder.layer.0.intermediate        → /BertModel/BertEncoder/BertLayer.0\n",
      "19. encoder.layer.0.output.dense        → /BertModel/BertEncoder/BertLayer.0\n",
      "20. encoder.layer.0.output.dropout      → /BertModel/BertEncoder/BertLayer.0\n",
      "21. encoder.layer.0.output.LayerNorm    → /BertModel/BertEncoder/BertLayer.0\n",
      "22. encoder.layer.0.output              → /BertModel/BertEncoder/BertLayer.0\n",
      "23. encoder.layer.0                     → /BertModel/BertEncoder/BertLayer.0\n",
      "24. encoder.layer.1.attention.self.query → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "25. encoder.layer.1.attention.self.key  → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "26. encoder.layer.1.attention.self.value → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "27. encoder.layer.1.attention.self      → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "28. encoder.layer.1.attention.output.dense → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "29. encoder.layer.1.attention.output.dropout → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "30. encoder.layer.1.attention.output.LayerNorm → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "31. encoder.layer.1.attention.output    → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "32. encoder.layer.1.attention           → /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "33. encoder.layer.1.intermediate.dense  → /BertModel/BertEncoder/BertLayer.1\n",
      "34. encoder.layer.1.intermediate.intermediate_act_fn → /BertModel/BertEncoder/BertLayer.1\n",
      "35. encoder.layer.1.intermediate        → /BertModel/BertEncoder/BertLayer.1\n",
      "36. encoder.layer.1.output.dense        → /BertModel/BertEncoder/BertLayer.1\n",
      "37. encoder.layer.1.output.dropout      → /BertModel/BertEncoder/BertLayer.1\n",
      "38. encoder.layer.1.output.LayerNorm    → /BertModel/BertEncoder/BertLayer.1\n",
      "39. encoder.layer.1.output              → /BertModel/BertEncoder/BertLayer.1\n",
      "40. encoder.layer.1                     → /BertModel/BertEncoder/BertLayer.1\n",
      "41. encoder                             → /BertModel/BertEncoder\n",
      "42. pooler.dense                        → /BertModel\n",
      "43. pooler.activation                   → /BertModel\n",
      "44. pooler                              → /BertModel\n",
      "45.                                     → /BertModel\n",
      "\n",
      "Total dynamic mappings: 45\n"
     ]
    }
   ],
   "source": [
    "class DynamicHierarchyMapper:\n",
    "    \"\"\"Hook-based dynamic hierarchy mapping inspired by HTP\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.hierarchy_stack = [\"BertModel\"]  # Start with root\n",
    "        self.module_to_hierarchy = {}  # module_id → hierarchy_tag\n",
    "        self.execution_trace = []  # Track execution order\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Create module name mapping\n",
    "        self.module_names = {}\n",
    "        for name, module in model.named_modules():\n",
    "            self.module_names[id(module)] = name\n",
    "        \n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks on all modules\"\"\"\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            hook = module.register_forward_hook(self._create_hook(name, module))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def _create_hook(self, module_name: str, module: nn.Module):\n",
    "        \"\"\"Create a forward hook for a specific module\"\"\"\n",
    "        \n",
    "        def forward_hook(module, input, output):\n",
    "            # Determine hierarchy tag for this module\n",
    "            hierarchy_tag = self._get_current_hierarchy_tag(module_name, module)\n",
    "            \n",
    "            # Record mapping\n",
    "            module_id = id(module)\n",
    "            self.module_to_hierarchy[module_id] = hierarchy_tag\n",
    "            \n",
    "            # Record execution trace\n",
    "            self.execution_trace.append({\n",
    "                'module_name': module_name,\n",
    "                'module_class': module.__class__.__name__,\n",
    "                'hierarchy_tag': hierarchy_tag,\n",
    "                'execution_order': len(self.execution_trace)\n",
    "            })\n",
    "        \n",
    "        return forward_hook\n",
    "    \n",
    "    def _get_current_hierarchy_tag(self, module_name: str, module: nn.Module) -> str:\n",
    "        \"\"\"Generate hierarchy tag based on current execution context\"\"\"\n",
    "        \n",
    "        if not module_name:  # Root module\n",
    "            return \"/BertModel\"\n",
    "        \n",
    "        # Use same logic as static approach but could be enhanced\n",
    "        # with execution context information\n",
    "        return generate_hierarchy_tag(module_name, module)\n",
    "    \n",
    "    def trace_forward_pass(self, *args, **kwargs):\n",
    "        \"\"\"Run forward pass and capture hierarchy mappings\"\"\"\n",
    "        \n",
    "        # Clear previous traces\n",
    "        self.execution_trace.clear()\n",
    "        self.module_to_hierarchy.clear()\n",
    "        \n",
    "        # Run forward pass (this triggers all hooks)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(*args, **kwargs)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_execution_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Get module execution order → hierarchy mapping\"\"\"\n",
    "        \n",
    "        mapping = {}\n",
    "        for trace in self.execution_trace:\n",
    "            mapping[trace['module_name']] = trace['hierarchy_tag']\n",
    "        \n",
    "        return mapping\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "\n",
    "# Demo dynamic mapping\n",
    "dynamic_mapper = DynamicHierarchyMapper(model)\n",
    "\n",
    "print(\"🎣 Dynamic Mapping with Forward Hooks:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Trace forward pass\n",
    "output = dynamic_mapper.trace_forward_pass(inputs['input_ids'], inputs['attention_mask'])\n",
    "dynamic_mapping = dynamic_mapper.get_execution_mapping()\n",
    "\n",
    "print(f\"Executed {len(dynamic_mapper.execution_trace)} modules\")\n",
    "print(\"\\nFirst 10 executed modules:\")\n",
    "for i, trace in enumerate(dynamic_mapper.execution_trace[:]):\n",
    "    print(f\"{i+1:2d}. {trace['module_name']:<35} → {trace['hierarchy_tag']}\")\n",
    "\n",
    "dynamic_mapper.cleanup()\n",
    "print(f\"\\nTotal dynamic mappings: {len(dynamic_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Comparison: Static vs Dynamic\n",
    "\n",
    "Let's compare the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 STATIC vs DYNAMIC COMPARISON\n",
      "================================================================================\n",
      "📊 Coverage Comparison:\n",
      "  Static only:    3 modules\n",
      "  Dynamic only:   1 modules\n",
      "  Both:          44 modules\n",
      "  Total static:  47 modules\n",
      "  Total dynamic: 45 modules\n",
      "\n",
      "🔍 Modules only found in static analysis:\n",
      "  - encoder.layer\n",
      "  - encoder.layer.0.attention.self.dropout\n",
      "  - encoder.layer.1.attention.self.dropout\n",
      "\n",
      "🎯 Modules only found during execution:\n",
      "  - \n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 STATIC vs DYNAMIC COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare coverage\n",
    "static_modules = set(static_mapping.keys())\n",
    "dynamic_modules = set(dynamic_mapping.keys())\n",
    "\n",
    "only_static = static_modules - dynamic_modules\n",
    "only_dynamic = dynamic_modules - static_modules\n",
    "both = static_modules & dynamic_modules\n",
    "\n",
    "print(f\"📊 Coverage Comparison:\")\n",
    "print(f\"  Static only:  {len(only_static):3d} modules\")\n",
    "print(f\"  Dynamic only: {len(only_dynamic):3d} modules\")\n",
    "print(f\"  Both:         {len(both):3d} modules\")\n",
    "print(f\"  Total static: {len(static_modules):3d} modules\")\n",
    "print(f\"  Total dynamic:{len(dynamic_modules):3d} modules\")\n",
    "\n",
    "print(f\"\\n🔍 Modules only found in static analysis:\")\n",
    "for module in sorted(list(only_static)[:10]):\n",
    "    print(f\"  - {module}\")\n",
    "if len(only_static) > 10:\n",
    "    print(f\"  ... and {len(only_static) - 10} more\")\n",
    "\n",
    "print(f\"\\n🎯 Modules only found during execution:\")\n",
    "for module in sorted(list(only_dynamic)[:10]):\n",
    "    print(f\"  - {module}\")\n",
    "if len(only_dynamic) > 10:\n",
    "    print(f\"  ... and {len(only_dynamic) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Advantages of Each Approach\n",
    "\n",
    "### 🏗️ Static Analysis (Current)\n",
    "\n",
    "**Pros:**\n",
    "- ✅ **Fast**: No forward pass execution needed\n",
    "- ✅ **Complete coverage**: Finds ALL modules in model\n",
    "- ✅ **Deterministic**: Always same result\n",
    "- ✅ **Simple**: No hooks or execution tracing\n",
    "- ✅ **Works for unused modules**: Even finds modules that never execute\n",
    "\n",
    "**Cons:**\n",
    "- ❌ **No operation mapping**: Doesn't map to actual ONNX operations\n",
    "- ❌ **No execution context**: Misses dynamic behavior\n",
    "- ❌ **No execution order**: Can't track execution flow\n",
    "\n",
    "### 🎣 Dynamic Hook-Based (Proposed)\n",
    "\n",
    "**Pros:**\n",
    "- ✅ **Real execution**: Captures actual forward pass behavior\n",
    "- ✅ **Operation mapping**: Can map to ONNX operations during export\n",
    "- ✅ **Execution order**: Tracks the order modules are called\n",
    "- ✅ **Dynamic models**: Handles conditional execution\n",
    "- ✅ **Precise**: Only includes actually executed modules\n",
    "\n",
    "**Cons:**\n",
    "- ❌ **Slower**: Requires forward pass execution\n",
    "- ❌ **Hook overhead**: Performance impact during export\n",
    "- ❌ **Incomplete**: Might miss unused modules\n",
    "- ❌ **Complex**: More implementation complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Proposed Hybrid Solution\n",
    "\n",
    "Based on this analysis, I think we should use **both approaches**:\n",
    "\n",
    "### Phase 1: Static Hierarchy Analysis\n",
    "```python\n",
    "# Get complete model structure\n",
    "static_hierarchy = analyze_model_hierarchy(model)\n",
    "```\n",
    "\n",
    "### Phase 2: Dynamic Operation Mapping\n",
    "```python\n",
    "# During ONNX export, use hooks to map operations\n",
    "with DynamicHierarchyMapper(model, static_hierarchy) as mapper:\n",
    "    onnx_model = torch.onnx.export(model, inputs, ...)\n",
    "    operation_tags = mapper.get_operation_hierarchy_map()\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- ✅ **Complete coverage** from static analysis\n",
    "- ✅ **Precise operation tagging** from dynamic hooks\n",
    "- ✅ **Best of both worlds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "Use HYBRID approach:\n",
      "\n",
      "1. 🏗️  Static Analysis Phase:\n",
      "   - Build complete module hierarchy mapping\n",
      "   - Generate hierarchy tags for all modules\n",
      "   - Handle torch.nn filtering\n",
      "\n",
      "2. 🎣  Dynamic Hook Phase (during ONNX export):\n",
      "   - Use forward hooks to capture execution\n",
      "   - Map ONNX operations to executing modules\n",
      "   - Apply hierarchy tags from static analysis\n",
      "\n",
      "This gives us:\n",
      "✅ Complete model understanding (static)\n",
      "✅ Precise operation mapping (dynamic)\n",
      "✅ Both structure and execution info\n"
     ]
    }
   ],
   "source": [
    "print(\"💡 RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Use HYBRID approach:\")\n",
    "print()\n",
    "print(\"1. 🏗️  Static Analysis Phase:\")\n",
    "print(\"   - Build complete module hierarchy mapping\")\n",
    "print(\"   - Generate hierarchy tags for all modules\")\n",
    "print(\"   - Handle torch.nn filtering\")\n",
    "print()\n",
    "print(\"2. 🎣  Dynamic Hook Phase (during ONNX export):\")\n",
    "print(\"   - Use forward hooks to capture execution\")\n",
    "print(\"   - Map ONNX operations to executing modules\")\n",
    "print(\"   - Apply hierarchy tags from static analysis\")\n",
    "print()\n",
    "print(\"This gives us:\")\n",
    "print(\"✅ Complete model understanding (static)\")\n",
    "print(\"✅ Precise operation mapping (dynamic)\")\n",
    "print(\"✅ Both structure and execution info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
