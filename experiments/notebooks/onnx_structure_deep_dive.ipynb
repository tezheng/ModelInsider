{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Structure Deep Dive: export_modules_as_functions Analysis\n",
    "\n",
    "This notebook provides a comprehensive examination of ONNX model structures when using different export options, particularly focusing on `export_modules_as_functions` and its implications for hierarchy preservation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Model Creation](#setup)\n",
    "2. [Export with Different Options](#export)\n",
    "3. [Detailed Structure Analysis](#analysis)\n",
    "4. [Visual Comparison](#visual)\n",
    "5. [Metadata and Attributes](#metadata)\n",
    "6. [Implications for Hierarchy Preservation](#implications)\n",
    "7. [Performance Considerations](#performance)\n",
    "8. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Model Creation <a name=\"setup\"></a>\n",
    "\n",
    "First, let's create a test model with clear hierarchical structure to demonstrate the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "ONNX version: 1.18.0\n",
      "Output directory: /mnt/d/BYOM/modelexport/notebooks/experimental/../../temp/onnx_structure_analysis\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for clarity\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set up paths\n",
    "output_dir = Path(\"../../temp/onnx_structure_analysis\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"Output directory: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexModel(nn.Module):\n",
    "    \"\"\"A more complex model to better demonstrate structural differences.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=10, hidden_dim=20, num_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Self-attention (reshape for attention layer)\n",
    "        features_seq = features.unsqueeze(1)  # Add sequence dimension\n",
    "        attn_out, _ = self.attention(features_seq, features_seq, features_seq)\n",
    "        attn_out = attn_out.squeeze(1)  # Remove sequence dimension\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(attn_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model instance\n",
    "model = ComplexModel()\n",
    "model.eval()\n",
    "\n",
    "# Create sample input\n",
    "sample_input = torch.randn(2, 10)  # batch_size=2, input_dim=10\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export with Different Options <a name=\"export\"></a>\n",
    "\n",
    "Now let's export the model using different configurations of `export_modules_as_functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_with_options(model, sample_input, export_modules_as_functions, suffix):\n",
    "    \"\"\"Export model with specified options and return path.\"\"\"\n",
    "    \n",
    "    output_path = output_dir / f\"model_{suffix}.onnx\"\n",
    "    \n",
    "    print(f\"\\nExporting with export_modules_as_functions={export_modules_as_functions}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            sample_input,\n",
    "            output_path,\n",
    "            export_params=True,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}\n",
    "            },\n",
    "            export_modules_as_functions=export_modules_as_functions,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Exported to: {output_path.name}\")\n",
    "    \n",
    "    # Verify model loads correctly\n",
    "    try:\n",
    "        onnx_model = onnx.load(str(output_path))\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"✓ Model validation passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model validation failed: {e}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Export with different options\n",
    "paths = {\n",
    "    'standard': export_model_with_options(model, sample_input, False, \"standard\"),\n",
    "    'all_functions': export_model_with_options(model, sample_input, True, \"all_functions\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Structure Analysis <a name=\"analysis\"></a>\n",
    "\n",
    "Let's analyze the internal structure of each exported model in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_onnx_structure(onnx_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Perform comprehensive analysis of ONNX model structure.\"\"\"\n",
    "    \n",
    "    model = onnx.load(str(onnx_path))\n",
    "    graph = model.graph\n",
    "    \n",
    "    analysis = {\n",
    "        'file_name': onnx_path.name,\n",
    "        'file_size_mb': onnx_path.stat().st_size / (1024 * 1024),\n",
    "        'graph': {\n",
    "            'inputs': len(graph.input),\n",
    "            'outputs': len(graph.output),\n",
    "            'nodes': len(graph.node),\n",
    "            'initializers': len(graph.initializer),\n",
    "            'value_info': len(graph.value_info)\n",
    "        },\n",
    "        'functions': {\n",
    "            'count': len(model.functions) if hasattr(model, 'functions') else 0,\n",
    "            'details': []\n",
    "        },\n",
    "        'node_types': defaultdict(int),\n",
    "        'attributes': defaultdict(list),\n",
    "        'tensor_shapes': {},\n",
    "        'parameter_count': 0\n",
    "    }\n",
    "    \n",
    "    # Analyze main graph nodes\n",
    "    for node in graph.node:\n",
    "        analysis['node_types'][node.op_type] += 1\n",
    "        \n",
    "        # Collect attributes\n",
    "        for attr in node.attribute:\n",
    "            analysis['attributes'][attr.name].append({\n",
    "                'node': node.name or node.op_type,\n",
    "                'type': attr.type\n",
    "            })\n",
    "    \n",
    "    # Analyze functions if present\n",
    "    if hasattr(model, 'functions') and model.functions:\n",
    "        for func in model.functions:\n",
    "            func_info = {\n",
    "                'name': func.name,\n",
    "                'domain': func.domain,\n",
    "                'inputs': len(func.input),\n",
    "                'outputs': len(func.output),\n",
    "                'nodes': len(func.node),\n",
    "                'node_types': defaultdict(int),\n",
    "                'attributes': len(func.attribute)\n",
    "            }\n",
    "            \n",
    "            # Count node types in function\n",
    "            for node in func.node:\n",
    "                func_info['node_types'][node.op_type] += 1\n",
    "            \n",
    "            func_info['node_types'] = dict(func_info['node_types'])\n",
    "            analysis['functions']['details'].append(func_info)\n",
    "    \n",
    "    # Count parameters\n",
    "    for init in graph.initializer:\n",
    "        shape = [dim for dim in init.dims]\n",
    "        analysis['parameter_count'] += np.prod(shape) if shape else 1\n",
    "        analysis['tensor_shapes'][init.name] = shape\n",
    "    \n",
    "    # Convert defaultdicts to regular dicts\n",
    "    analysis['node_types'] = dict(analysis['node_types'])\n",
    "    analysis['attributes'] = dict(analysis['attributes'])\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze all exported models\n",
    "analyses = {}\n",
    "for name, path in paths.items():\n",
    "    print(f\"\\nAnalyzing {name} export...\")\n",
    "    analyses[name] = analyze_onnx_structure(path)\n",
    "    print(f\"✓ Analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_analysis_summary(analyses: Dict[str, Dict[str, Any]]):\n",
    "    \"\"\"Display a formatted summary of the analyses.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ONNX STRUCTURE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for name, analysis in analyses.items():\n",
    "        print(f\"\\n{name.upper()} EXPORT:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Basic info\n",
    "        print(f\"File: {analysis['file_name']}\")\n",
    "        print(f\"Size: {analysis['file_size_mb']:.2f} MB\")\n",
    "        \n",
    "        # Graph structure\n",
    "        graph = analysis['graph']\n",
    "        print(f\"\\nMain Graph:\")\n",
    "        print(f\"  Inputs: {graph['inputs']}\")\n",
    "        print(f\"  Outputs: {graph['outputs']}\")\n",
    "        print(f\"  Nodes: {graph['nodes']}\")\n",
    "        print(f\"  Initializers: {graph['initializers']}\")\n",
    "        print(f\"  Parameters: {analysis['parameter_count']:,}\")\n",
    "        \n",
    "        # Node types\n",
    "        print(f\"\\nNode Types in Main Graph:\")\n",
    "        for op_type, count in sorted(analysis['node_types'].items()):\n",
    "            print(f\"  {op_type}: {count}\")\n",
    "        \n",
    "        # Functions\n",
    "        if analysis['functions']['count'] > 0:\n",
    "            print(f\"\\nLocal Functions: {analysis['functions']['count']}\")\n",
    "            for func in analysis['functions']['details'][:5]:  # Show first 5\n",
    "                print(f\"  - {func['name']} (domain: {func['domain']})\")\n",
    "                print(f\"    Nodes: {func['nodes']}, I/O: {func['inputs']}/{func['outputs']}\")\n",
    "                print(f\"    Operations: {dict(func['node_types'])}\")\n",
    "\n",
    "display_analysis_summary(analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual Comparison <a name=\"visual\"></a>\n",
    "\n",
    "Let's create visualizations to better understand the structural differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ONNX Export Structure Comparison', fontsize=16)\n",
    "\n",
    "# 1. Graph complexity comparison\n",
    "ax = axes[0, 0]\n",
    "metrics = ['Nodes', 'Functions', 'Initializers']\n",
    "standard_values = [\n",
    "    analyses['standard']['graph']['nodes'],\n",
    "    analyses['standard']['functions']['count'],\n",
    "    analyses['standard']['graph']['initializers']\n",
    "]\n",
    "functions_values = [\n",
    "    analyses['all_functions']['graph']['nodes'],\n",
    "    analyses['all_functions']['functions']['count'],\n",
    "    analyses['all_functions']['graph']['initializers']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, standard_values, width, label='Standard Export', alpha=0.8)\n",
    "ax.bar(x + width/2, functions_values, width, label='Functions Export', alpha=0.8)\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Graph Complexity Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Node type distribution - Standard\n",
    "ax = axes[0, 1]\n",
    "if analyses['standard']['node_types']:\n",
    "    node_types = list(analyses['standard']['node_types'].keys())\n",
    "    node_counts = list(analyses['standard']['node_types'].values())\n",
    "    ax.pie(node_counts, labels=node_types, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title('Node Types - Standard Export')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No nodes in main graph', ha='center', va='center')\n",
    "    ax.set_title('Node Types - Standard Export')\n",
    "\n",
    "# 3. Node type distribution - Functions\n",
    "ax = axes[1, 0]\n",
    "if analyses['all_functions']['node_types']:\n",
    "    node_types = list(analyses['all_functions']['node_types'].keys())\n",
    "    node_counts = list(analyses['all_functions']['node_types'].values())\n",
    "    ax.pie(node_counts, labels=node_types, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title('Node Types - Functions Export (Main Graph)')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Most operations\\nin functions', ha='center', va='center', fontsize=12)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Node Types - Functions Export (Main Graph)')\n",
    "\n",
    "# 4. File size comparison\n",
    "ax = axes[1, 1]\n",
    "export_types = ['Standard', 'Functions']\n",
    "file_sizes = [\n",
    "    analyses['standard']['file_size_mb'],\n",
    "    analyses['all_functions']['file_size_mb']\n",
    "]\n",
    "bars = ax.bar(export_types, file_sizes, alpha=0.8, color=['blue', 'orange'])\n",
    "ax.set_ylabel('File Size (MB)')\n",
    "ax.set_title('File Size Comparison')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, file_sizes):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{size:.3f} MB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata and Attributes <a name=\"metadata\"></a>\n",
    "\n",
    "Let's examine the metadata and attributes in both export modes to understand how hierarchy information is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_metadata_and_attributes(onnx_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Examine metadata and attributes in ONNX model.\"\"\"\n",
    "    \n",
    "    model = onnx.load(str(onnx_path))\n",
    "    \n",
    "    metadata = {\n",
    "        'model_metadata': {},\n",
    "        'graph_metadata': {},\n",
    "        'node_attributes': defaultdict(list),\n",
    "        'function_attributes': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    # Model-level metadata\n",
    "    if hasattr(model, 'metadata_props'):\n",
    "        for prop in model.metadata_props:\n",
    "            metadata['model_metadata'][prop.key] = prop.value\n",
    "    \n",
    "    # Graph-level metadata\n",
    "    if hasattr(model.graph, 'doc_string'):\n",
    "        metadata['graph_metadata']['doc_string'] = model.graph.doc_string\n",
    "    \n",
    "    # Node attributes\n",
    "    for node in model.graph.node:\n",
    "        if node.attribute:\n",
    "            node_info = {\n",
    "                'node_name': node.name or f\"{node.op_type}_unnamed\",\n",
    "                'op_type': node.op_type,\n",
    "                'attributes': {}\n",
    "            }\n",
    "            \n",
    "            for attr in node.attribute:\n",
    "                # Extract attribute value based on type\n",
    "                if attr.type == onnx.AttributeProto.FLOAT:\n",
    "    .               value = attr.f\n",
    "                elif attr.type == onnx.AttributeProto.INT:\n",
    "                    value = attr.i\n",
    "                elif attr.type == onnx.AttributeProto.STRING:\n",
    "                    value = attr.s.decode('utf-8') if attr.s else ''\n",
    "                elif attr.type == onnx.AttributeProto.INTS:\n",
    "                    value = list(attr.ints)\n",
    "                elif attr.type == onnx.AttributeProto.STRINGS:\n",
    "                    value = [s.decode('utf-8') for s in attr.strings]\n",
    "                else:\n",
    "                    value = f\"Type: {attr.type}\"\n",
    "                \n",
    "                node_info['attributes'][attr.name] = value\n",
    "            \n",
    "            metadata['node_attributes'][node.op_type].append(node_info)\n",
    "    \n",
    "    # Function attributes\n",
    "    if hasattr(model, 'functions'):\n",
    "        for func in model.functions:\n",
    "            func_info = {\n",
    "                'name': func.name,\n",
    "                'domain': func.domain,\n",
    "                'attributes': []\n",
    "            }\n",
    "            \n",
    "            if hasattr(func, 'attribute'):\n",
    "                for attr in func.attribute:\n",
    "                    func_info['attributes'].append(attr.name)\n",
    "            \n",
    "            metadata['function_attributes'][func.domain].append(func_info)\n",
    "    \n",
    "    # Convert defaultdicts\n",
    "    metadata['node_attributes'] = dict(metadata['node_attributes'])\n",
    "    metadata['function_attributes'] = dict(metadata['function_attributes'])\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Examine metadata for both exports\n",
    "metadata_analysis = {}\n",
    "for name, path in paths.items():\n",
    "    print(f\"\\nExamining metadata for {name} export...\")\n",
    "    metadata_analysis[name] = examine_metadata_and_attributes(path)\n",
    "\n",
    "# Display interesting findings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METADATA AND ATTRIBUTE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, metadata in metadata_analysis.items():\n",
    "    print(f\"\\n{name.upper()} EXPORT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Model metadata\n",
    "    if metadata['model_metadata']:\n",
    "        print(\"Model Metadata:\")\n",
    "        for key, value in metadata['model_metadata'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"Model Metadata: None\")\n",
    "    \n",
    "    # Sample node attributes\n",
    "    if metadata['node_attributes']:\n",
    "        print(\"\\nSample Node Attributes:\")\n",
    "        for op_type, nodes in list(metadata['node_attributes'].items())[:2]:\n",
    "            print(f\"  {op_type}:\")\n",
    "            if nodes:\n",
    "                sample_node = nodes[0]\n",
    "                for attr_name, attr_value in sample_node['attributes'].items():\n",
    "                    print(f\"    {attr_name}: {attr_value}\")\n",
    "    \n",
    "    # Function information\n",
    "    if metadata['function_attributes']:\n",
    "        print(f\"\\nFunction Domains: {list(metadata['function_attributes'].keys())}\")\n",
    "        total_functions = sum(len(funcs) for funcs in metadata['function_attributes'].values())\n",
    "        print(f\"Total Functions: {total_functions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implications for Hierarchy Preservation <a name=\"implications\"></a>\n",
    "\n",
    "Let's analyze how each export mode affects hierarchy preservation and compare with modelexport's approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hierarchy_preservation():\n",
    "    \"\"\"Analyze hierarchy preservation capabilities of different approaches.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHY PRESERVATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define comparison criteria\n",
    "    criteria = [\n",
    "        \"Operation-level granularity\",\n",
    "        \"Module boundary preservation\",\n",
    "        \"Direct operation traceability\",\n",
    "        \"Metadata flexibility\",\n",
    "        \"Runtime compatibility\",\n",
    "        \"Debugging capability\",\n",
    "        \"Cross-layer analysis\",\n",
    "        \"Parameter attribution\"\n",
    "    ]\n",
    "    \n",
    "    # Score each approach (1-5 scale)\n",
    "    scores = {\n",
    "        \"Standard ONNX\": [5, 1, 5, 2, 5, 4, 3, 4],\n",
    "        \"export_modules_as_functions\": [2, 5, 2, 3, 3, 3, 2, 3],\n",
    "        \"ModelExport HTP\": [5, 4, 5, 5, 5, 5, 5, 5]\n",
    "    }\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\nComparison Matrix (1=Poor, 5=Excellent):\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Criterion':<30} {'Standard':<12} {'Functions':<12} {'HTP':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, criterion in enumerate(criteria):\n",
    "        print(f\"{criterion:<30} {scores['Standard ONNX'][i]:<12} \"\n",
    "              f\"{scores['export_modules_as_functions'][i]:<12} \"\n",
    "              f\"{scores['ModelExport HTP'][i]:<12}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    totals = {name: sum(score) for name, score in scores.items()}\n",
    "    print(f\"{'TOTAL SCORE':<30} {totals['Standard ONNX']:<12} \"\n",
    "          f\"{totals['export_modules_as_functions']:<12} \"\n",
    "          f\"{totals['ModelExport HTP']:<12}\")\n",
    "    \n",
    "    # Detailed analysis\n",
    "    print(\"\\n\\nDETAILED ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. STANDARD ONNX EXPORT:\")\n",
    "    print(\"   Pros:\")\n",
    "    print(\"   • Full operation visibility\")\n",
    "    print(\"   • Direct debugging access\")\n",
    "    print(\"   • Maximum runtime compatibility\")\n",
    "    print(\"   Cons:\")\n",
    "    print(\"   • No hierarchy information\")\n",
    "    print(\"   • Lost module boundaries\")\n",
    "    print(\"   • Difficult to trace operations to source\")\n",
    "    \n",
    "    print(\"\\n2. EXPORT_MODULES_AS_FUNCTIONS:\")\n",
    "    print(\"   Pros:\")\n",
    "    print(\"   • Preserves module boundaries\")\n",
    "    print(\"   • Clear module-level organization\")\n",
    "    print(\"   • Good for module replacement\")\n",
    "    print(\"   Cons:\")\n",
    "    print(\"   • Operations hidden in functions\")\n",
    "    print(\"   • Limited debugging access\")\n",
    "    print(\"   • May have compatibility issues\")\n",
    "    print(\"   • Deprecated feature\")\n",
    "    \n",
    "    print(\"\\n3. MODELEXPORT HTP APPROACH:\")\n",
    "    print(\"   Pros:\")\n",
    "    print(\"   • Operation-level granularity\")\n",
    "    print(\"   • Rich metadata tagging\")\n",
    "    print(\"   • Full debugging capability\")\n",
    "    print(\"   • Preserves hierarchy information\")\n",
    "    print(\"   • Universal compatibility\")\n",
    "    print(\"   Cons:\")\n",
    "    print(\"   • Requires custom implementation\")\n",
    "    print(\"   • Additional metadata overhead\")\n",
    "\n",
    "analyze_hierarchy_preservation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual comparison of approaches\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Radar chart for capability comparison\n",
    "categories = [\n",
    "    \"Op Granularity\",\n",
    "    \"Module Boundaries\",\n",
    "    \"Traceability\",\n",
    "    \"Flexibility\",\n",
    "    \"Compatibility\",\n",
    "    \"Debugging\"\n",
    "]\n",
    "\n",
    "# Scores (normalized to 0-1)\n",
    "standard_scores = [1.0, 0.2, 1.0, 0.4, 1.0, 0.8]\n",
    "functions_scores = [0.4, 1.0, 0.4, 0.6, 0.6, 0.6]\n",
    "htp_scores = [1.0, 0.8, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "# Number of variables\n",
    "N = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize the plot\n",
    "ax1.set_theta_offset(np.pi / 2)\n",
    "ax1.set_theta_direction(-1)\n",
    "\n",
    "# Draw one axis per variable and add labels\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(categories)\n",
    "\n",
    "# Draw ylabels\n",
    "ax1.set_rlabel_position(0)\n",
    "ax1.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax1.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=7)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot data\n",
    "standard_scores += standard_scores[:1]\n",
    "functions_scores += functions_scores[:1]\n",
    "htp_scores += htp_scores[:1]\n",
    "\n",
    "ax1.plot(angles, standard_scores, 'o-', linewidth=2, label='Standard ONNX', color='blue')\n",
    "ax1.fill(angles, standard_scores, alpha=0.25, color='blue')\n",
    "\n",
    "ax1.plot(angles, functions_scores, 'o-', linewidth=2, label='export_modules_as_functions', color='orange')\n",
    "ax1.fill(angles, functions_scores, alpha=0.25, color='orange')\n",
    "\n",
    "ax1.plot(angles, htp_scores, 'o-', linewidth=2, label='ModelExport HTP', color='green')\n",
    "ax1.fill(angles, htp_scores, alpha=0.25, color='green')\n",
    "\n",
    "ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax1.set_title('Capability Comparison', size=14, y=1.08)\n",
    "\n",
    "# Use case suitability\n",
    "ax2.set_title('Use Case Suitability', size=14)\n",
    "\n",
    "use_cases = ['Debugging', 'Analysis', 'Module\\nReplacement', 'Custom\\nBackends', 'Research']\n",
    "standard_suit = [4, 3, 1, 3, 2]\n",
    "functions_suit = [2, 2, 5, 2, 3]\n",
    "htp_suit = [5, 5, 3, 5, 5]\n",
    "\n",
    "x = np.arange(len(use_cases))\n",
    "width = 0.25\n",
    "\n",
    "ax2.bar(x - width, standard_suit, width, label='Standard', alpha=0.8, color='blue')\n",
    "ax2.bar(x, functions_suit, width, label='Functions', alpha=0.8, color='orange')\n",
    "ax2.bar(x + width, htp_suit, width, label='HTP', alpha=0.8, color='green')\n",
    "\n",
    "ax2.set_ylabel('Suitability Score')\n",
    "ax2.set_xlabel('Use Case')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(use_cases)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Considerations <a name=\"performance\"></a>\n",
    "\n",
    "Let's analyze the performance implications of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_onnx_models(paths: Dict[str, Path], num_runs: int = 100):\n",
    "    \"\"\"Benchmark inference performance of different ONNX models.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE BENCHMARKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Create test input\n",
    "    test_input = np.random.randn(10, 10).astype(np.float32)  # Larger batch for better timing\n",
    "    \n",
    "    for name, path in paths.items():\n",
    "        print(f\"\\nBenchmarking {name} export...\")\n",
    "        \n",
    "        try:\n",
    "            # Create inference session\n",
    "            session = ort.InferenceSession(str(path))\n",
    "            \n",
    "            # Get input name\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                _ = session.run(None, {input_name: test_input})\n",
    "            \n",
    "            # Benchmark\n",
    "            import time\n",
    "            times = []\n",
    "            \n",
    "            for _ in range(num_runs):\n",
    "                start = time.perf_counter()\n",
    "                _ = session.run(None, {input_name: test_input})\n",
    "                end = time.perf_counter()\n",
    "                times.append((end - start) * 1000)  # Convert to ms\n",
    "            \n",
    "            results[name] = {\n",
    "                'mean_ms': np.mean(times),\n",
    "                'std_ms': np.std(times),\n",
    "                'min_ms': np.min(times),\n",
    "                'max_ms': np.max(times),\n",
    "                'median_ms': np.median(times)\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ Mean inference time: {results[name]['mean_ms']:.3f} ms\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Benchmarking failed: {e}\")\n",
    "            results[name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = benchmark_onnx_models(paths)\n",
    "\n",
    "# Display results\n",
    "if all(r is not None for r in benchmark_results.values()):\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Export Type':<20} {'Mean (ms)':<12} {'Std (ms)':<12} {'Min (ms)':<12} {'Max (ms)':<12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for name, result in benchmark_results.items():\n",
    "        if result:\n",
    "            print(f\"{name:<20} {result['mean_ms']:<12.3f} {result['std_ms']:<12.3f} \"\n",
    "                  f\"{result['min_ms']:<12.3f} {result['max_ms']:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions <a name=\"conclusions\"></a>\n",
    "\n",
    "Let's summarize our findings and provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conclusions():\n",
    "    \"\"\"Generate comprehensive conclusions from the analysis.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE CONCLUSIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. STRUCTURAL DIFFERENCES:\")\n",
    "    print(\"   • Standard Export: Flat graph with all operations visible\")\n",
    "    print(\"   • Functions Export: Hierarchical structure with module-level functions\")\n",
    "    print(\"   • Key Finding: Functions preserve module boundaries but hide operations\")\n",
    "    \n",
    "    print(\"\\n2. GRANULARITY ANALYSIS:\")\n",
    "    print(\"   • Standard: Operation-level (MatMul, Add, etc.)\")\n",
    "    print(\"   • Functions: Module-level (Linear, Sequential, etc.)\")\n",
    "    print(\"   • ModelExport HTP: Operation-level with module metadata\")\n",
    "    \n",
    "    print(\"\\n3. USE CASE ALIGNMENT:\")\n",
    "    print(\"   Standard Export Best For:\")\n",
    "    print(\"   • Maximum compatibility\")\n",
    "    print(\"   • Direct operation access\")\n",
    "    print(\"   • Performance-critical applications\")\n",
    "    \n",
    "    print(\"\\n   export_modules_as_functions Best For:\")\n",
    "    print(\"   • Module-level manipulation\")\n",
    "    print(\"   • Preserving logical structure\")\n",
    "    print(\"   • Model composition workflows\")\n",
    "    \n",
    "    print(\"\\n   ModelExport HTP Best For:\")\n",
    "    print(\"   • Debugging and analysis\")\n",
    "    print(\"   • Custom backend development\")\n",
    "    print(\"   • Research and experimentation\")\n",
    "    print(\"   • Fine-grained operation tracking\")\n",
    "    \n",
    "    print(\"\\n4. KEY INSIGHTS:\")\n",
    "    print(\"   • export_modules_as_functions operates at different granularity than HTP\")\n",
    "    print(\"   • Neither standard nor functions export provides operation-level hierarchy\")\n",
    "    print(\"   • HTP's metadata approach offers best of both worlds\")\n",
    "    print(\"   • Functions export may have compatibility/deprecation concerns\")\n",
    "    \n",
    "    print(\"\\n5. RECOMMENDATIONS:\")\n",
    "    print(\"   ✓ Continue developing ModelExport HTP as primary strategy\")\n",
    "    print(\"   ✓ Consider export_modules_as_functions for specific use cases only\")\n",
    "    print(\"   ✓ Focus on operation-level tagging for maximum flexibility\")\n",
    "    print(\"   ✓ Ensure compatibility with standard ONNX runtime\")\n",
    "    \n",
    "    print(\"\\n6. FUTURE DIRECTIONS:\")\n",
    "    print(\"   • Hybrid approach: Combine module functions with operation tags\")\n",
    "    print(\"   • Enhanced metadata: Include more context in tags\")\n",
    "    print(\"   • Tool development: Viewers/analyzers for tagged models\")\n",
    "    print(\"   • Performance optimization: Minimize tagging overhead\")\n",
    "\n",
    "generate_conclusions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "analysis_summary = {\n",
    "    'timestamp': str(pd.Timestamp.now()),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'onnx_version': onnx.__version__,\n",
    "    'model_info': {\n",
    "        'class': model.__class__.__name__,\n",
    "        'parameters': sum(p.numel() for p in model.parameters())\n",
    "    },\n",
    "    'export_analysis': analyses,\n",
    "    'benchmark_results': benchmark_results,\n",
    "    'conclusions': {\n",
    "        'structural_difference': 'Functions export creates hierarchical structure',\n",
    "        'granularity_difference': 'Module-level vs operation-level',\n",
    "        'htp_advantage': 'Provides operation-level granularity with hierarchy metadata',\n",
    "        'recommendation': 'Continue with HTP strategy as primary approach'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "summary_path = output_dir / 'onnx_structure_analysis_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Analysis complete! Summary saved to: {summary_path}\")\n",
    "print(f\"\\n🔍 Key Takeaway: export_modules_as_functions and ModelExport HTP serve different purposes\")\n",
    "print(f\"   and operate at different granularities. HTP provides the flexibility needed for\")\n",
    "print(f\"   operation-level analysis while preserving hierarchy information.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
