{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF Module Hierarchy Exploration\n",
    "\n",
    "This notebook explores the sub-module hierarchy of any HuggingFace model and builds semantic hierarchy mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T03:33:24.987727Z",
     "iopub.status.busy": "2025-07-03T03:33:24.987018Z",
     "iopub.status.idle": "2025-07-03T03:34:50.720714Z",
     "shell.execute_reply": "2025-07-03T03:34:50.720262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prajjwal1/bert-tiny...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
      "Model class: BertModel\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "# Configuration - set your model here\n",
    "MODEL_NAME = \"prajjwal1/bert-tiny\"  # Change this to test different models\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model class: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive HF Module Hierarchy Builder\n",
    "\n",
    "Build a semantic hierarchy using HF class names and map to named_modules() paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T03:34:50.722494Z",
     "iopub.status.busy": "2025-07-03T03:34:50.722139Z",
     "iopub.status.idle": "2025-07-03T03:34:50.727509Z",
     "shell.execute_reply": "2025-07-03T03:34:50.727098Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_hf_class(module):\n",
    "    \"\"\"Check if a module is a HuggingFace class\"\"\"\n",
    "    module_path = module.__class__.__module__\n",
    "    return module_path.startswith('transformers')\n",
    "\n",
    "def build_hf_hierarchy_mapping(model):\n",
    "    \"\"\"Recursively build HF module hierarchy mapping\"\"\"\n",
    "    hierarchy_mapping = {}\n",
    "    \n",
    "    def recursive_build(module, current_tag, module_name, parent_children_names=None):\n",
    "        \"\"\"Recursively build hierarchy for a module\"\"\"\n",
    "        \n",
    "        # If this is an HF class, update the tag\n",
    "        if is_hf_class(module):\n",
    "            class_name = module.__class__.__name__\n",
    "            \n",
    "            # Add index if this is a repeated class among siblings\n",
    "            if parent_children_names and module_name:\n",
    "                module_basename = module_name.split('.')[-1]\n",
    "                # Count how many siblings have the same class name\n",
    "                same_class_siblings = []\n",
    "                for sibling_name in parent_children_names:\n",
    "                    if sibling_name == module_basename:\n",
    "                        same_class_siblings.append(sibling_name)\n",
    "                \n",
    "                # If there are multiple siblings with same class, add index\n",
    "                if len(same_class_siblings) > 1 or module_basename.isdigit():\n",
    "                    # Extract index from module name (e.g., \"0\" from \"layer.0\")\n",
    "                    if module_basename.isdigit():\n",
    "                        index = module_basename\n",
    "                        current_tag = f\"{current_tag}/{class_name}.{index}\"\n",
    "                    else:\n",
    "                        current_tag = f\"{current_tag}/{class_name}\"\n",
    "                else:\n",
    "                    current_tag = f\"{current_tag}/{class_name}\"\n",
    "            else:\n",
    "                current_tag = f\"{current_tag}/{class_name}\"\n",
    "        \n",
    "        # Map this module to its hierarchy tag\n",
    "        if module_name:  # Skip root module\n",
    "            hierarchy_mapping[module_name] = current_tag\n",
    "        \n",
    "        # Get children names for indexing\n",
    "        children_names = [name for name, _ in module.named_children()]\n",
    "        \n",
    "        # Recursively process children\n",
    "        for child_name, child_module in module.named_children():\n",
    "            child_full_name = f\"{module_name}.{child_name}\" if module_name else child_name\n",
    "            recursive_build(child_module, current_tag, child_full_name, children_names)\n",
    "    \n",
    "    # Start with root model - use simple class name without duplication\n",
    "    root_class = model.__class__.__name__\n",
    "    initial_tag = f\"/{root_class}\" if is_hf_class(model) else \"\"\n",
    "    \n",
    "    # Skip the root model itself and start with its children to avoid duplication\n",
    "    for child_name, child_module in model.named_children():\n",
    "        recursive_build(child_module, initial_tag, child_name, [name for name, _ in model.named_children()])\n",
    "    \n",
    "    return hierarchy_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T03:34:50.728907Z",
     "iopub.status.busy": "2025-07-03T03:34:50.728714Z",
     "iopub.status.idle": "2025-07-03T03:34:50.731609Z",
     "shell.execute_reply": "2025-07-03T03:34:50.731241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building HF hierarchy mapping...\n",
      "\n",
      "Found 47 module mappings:\n",
      "======================================================================\n",
      "embeddings                               -> /BertModel/BertEmbeddings\n",
      "embeddings.LayerNorm                     -> /BertModel/BertEmbeddings\n",
      "embeddings.dropout                       -> /BertModel/BertEmbeddings\n",
      "embeddings.position_embeddings           -> /BertModel/BertEmbeddings\n",
      "embeddings.token_type_embeddings         -> /BertModel/BertEmbeddings\n",
      "embeddings.word_embeddings               -> /BertModel/BertEmbeddings\n",
      "encoder                                  -> /BertModel/BertEncoder\n",
      "encoder.layer                            -> /BertModel/BertEncoder\n",
      "encoder.layer.0                          -> /BertModel/BertEncoder/BertLayer.0\n",
      "encoder.layer.0.attention                -> /BertModel/BertEncoder/BertLayer.0/BertAttention\n",
      "encoder.layer.0.attention.output         -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput\n",
      "encoder.layer.0.attention.output.LayerNorm -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput\n",
      "encoder.layer.0.attention.output.dense   -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput\n",
      "encoder.layer.0.attention.output.dropout -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput\n",
      "encoder.layer.0.attention.self           -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.0.attention.self.dropout   -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.0.attention.self.key       -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.0.attention.self.query     -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.0.attention.self.value     -> /BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.0.intermediate             -> /BertModel/BertEncoder/BertLayer.0/BertIntermediate\n",
      "encoder.layer.0.intermediate.dense       -> /BertModel/BertEncoder/BertLayer.0/BertIntermediate\n",
      "encoder.layer.0.intermediate.intermediate_act_fn -> /BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation\n",
      "encoder.layer.0.output                   -> /BertModel/BertEncoder/BertLayer.0/BertOutput\n",
      "encoder.layer.0.output.LayerNorm         -> /BertModel/BertEncoder/BertLayer.0/BertOutput\n",
      "encoder.layer.0.output.dense             -> /BertModel/BertEncoder/BertLayer.0/BertOutput\n",
      "encoder.layer.0.output.dropout           -> /BertModel/BertEncoder/BertLayer.0/BertOutput\n",
      "encoder.layer.1                          -> /BertModel/BertEncoder/BertLayer.1\n",
      "encoder.layer.1.attention                -> /BertModel/BertEncoder/BertLayer.1/BertAttention\n",
      "encoder.layer.1.attention.output         -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput\n",
      "encoder.layer.1.attention.output.LayerNorm -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput\n",
      "encoder.layer.1.attention.output.dense   -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput\n",
      "encoder.layer.1.attention.output.dropout -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput\n",
      "encoder.layer.1.attention.self           -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.1.attention.self.dropout   -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.1.attention.self.key       -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.1.attention.self.query     -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.1.attention.self.value     -> /BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention\n",
      "encoder.layer.1.intermediate             -> /BertModel/BertEncoder/BertLayer.1/BertIntermediate\n",
      "encoder.layer.1.intermediate.dense       -> /BertModel/BertEncoder/BertLayer.1/BertIntermediate\n",
      "encoder.layer.1.intermediate.intermediate_act_fn -> /BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation\n",
      "encoder.layer.1.output                   -> /BertModel/BertEncoder/BertLayer.1/BertOutput\n",
      "encoder.layer.1.output.LayerNorm         -> /BertModel/BertEncoder/BertLayer.1/BertOutput\n",
      "encoder.layer.1.output.dense             -> /BertModel/BertEncoder/BertLayer.1/BertOutput\n",
      "encoder.layer.1.output.dropout           -> /BertModel/BertEncoder/BertLayer.1/BertOutput\n",
      "pooler                                   -> /BertModel/BertPooler\n",
      "pooler.activation                        -> /BertModel/BertPooler\n",
      "pooler.dense                             -> /BertModel/BertPooler\n"
     ]
    }
   ],
   "source": [
    "# Build the mapping\n",
    "print(\"Building HF hierarchy mapping...\")\n",
    "hierarchy_mapping = build_hf_hierarchy_mapping(model)\n",
    "\n",
    "print(f\"\\nFound {len(hierarchy_mapping)} module mappings:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for module_name, hierarchy_tag in sorted(hierarchy_mapping.items()):\n",
    "    print(f\"{module_name:40} -> {hierarchy_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: HF vs PyTorch Classes\n",
    "\n",
    "Let's see which modules are HF classes vs standard PyTorch classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T03:34:50.732960Z",
     "iopub.status.busy": "2025-07-03T03:34:50.732567Z",
     "iopub.status.idle": "2025-07-03T03:34:50.736361Z",
     "shell.execute_reply": "2025-07-03T03:34:50.736001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace modules (17):\n",
      "======================================================================\n",
      "embeddings                     -> BertEmbeddings      \n",
      "encoder                        -> BertEncoder         \n",
      "encoder.layer.0                -> BertLayer           \n",
      "encoder.layer.0.attention      -> BertAttention       \n",
      "encoder.layer.0.attention.self -> BertSdpaSelfAttention\n",
      "encoder.layer.0.attention.output -> BertSelfOutput      \n",
      "encoder.layer.0.intermediate   -> BertIntermediate    \n",
      "encoder.layer.0.intermediate.intermediate_act_fn -> GELUActivation      \n",
      "encoder.layer.0.output         -> BertOutput          \n",
      "encoder.layer.1                -> BertLayer           \n",
      "encoder.layer.1.attention      -> BertAttention       \n",
      "encoder.layer.1.attention.self -> BertSdpaSelfAttention\n",
      "encoder.layer.1.attention.output -> BertSelfOutput      \n",
      "encoder.layer.1.intermediate   -> BertIntermediate    \n",
      "encoder.layer.1.intermediate.intermediate_act_fn -> GELUActivation      \n",
      "encoder.layer.1.output         -> BertOutput          \n",
      "pooler                         -> BertPooler          \n",
      "\n",
      "PyTorch modules (30):\n",
      "======================================================================\n",
      "embeddings.word_embeddings     -> Embedding           \n",
      "embeddings.position_embeddings -> Embedding           \n",
      "embeddings.token_type_embeddings -> Embedding           \n",
      "embeddings.LayerNorm           -> LayerNorm           \n",
      "embeddings.dropout             -> Dropout             \n",
      "encoder.layer                  -> ModuleList          \n",
      "encoder.layer.0.attention.self.query -> Linear              \n",
      "encoder.layer.0.attention.self.key -> Linear              \n",
      "encoder.layer.0.attention.self.value -> Linear              \n",
      "encoder.layer.0.attention.self.dropout -> Dropout             \n",
      "... and 20 more PyTorch modules\n"
     ]
    }
   ],
   "source": [
    "# Analyze HF vs PyTorch classes\n",
    "hf_modules = []\n",
    "pytorch_modules = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if name == '':  # Skip root\n",
    "        continue\n",
    "    \n",
    "    if is_hf_class(module):\n",
    "        hf_modules.append((name, module.__class__.__name__, module.__class__.__module__))\n",
    "    else:\n",
    "        pytorch_modules.append((name, module.__class__.__name__, module.__class__.__module__))\n",
    "\n",
    "print(f\"HuggingFace modules ({len(hf_modules)}):\")\n",
    "print(\"=\" * 70)\n",
    "for name, class_name, module_path in hf_modules:\n",
    "    print(f\"{name:30} -> {class_name:20}\")\n",
    "\n",
    "print(f\"\\nPyTorch modules ({len(pytorch_modules)}):\")\n",
    "print(\"=\" * 70)\n",
    "for name, class_name, module_path in pytorch_modules[:10]:  # Show first 10\n",
    "    print(f\"{name:30} -> {class_name:20}\")\n",
    "\n",
    "if len(pytorch_modules) > 10:\n",
    "    print(f\"... and {len(pytorch_modules) - 10} more PyTorch modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Different Models\n",
    "\n",
    "Change the MODEL_NAME variable above to test with different models like:\n",
    "- `\"openai-community/gpt2\"` - GPT-2 model\n",
    "- `\"facebook/opt-125m\"` - OPT model  \n",
    "- `\"google/flan-t5-small\"` - T5 model\n",
    "\n",
    "The hierarchy mapping logic is completely generic and works with any HuggingFace model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
