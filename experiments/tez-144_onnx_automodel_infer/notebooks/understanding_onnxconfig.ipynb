{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding OnnxConfig: From HuggingFace Config to ONNX Export\n",
    "\n",
    "This notebook explains:\n",
    "1. What is OnnxConfig and why it's needed\n",
    "2. The structure and specification of OnnxConfig\n",
    "3. How to generate OnnxConfig from a HuggingFace model config\n",
    "4. Practical examples of creating custom OnnxConfigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is OnnxConfig?\n",
    "\n",
    "**OnnxConfig** is a configuration class that tells the ONNX exporter:\n",
    "- What inputs the model expects (names, shapes, types)\n",
    "- What outputs the model produces\n",
    "- How to generate dummy inputs for tracing\n",
    "- Which axes should be dynamic (variable batch size, sequence length)\n",
    "- ONNX opset version to use\n",
    "\n",
    "Think of it as a \"recipe\" for converting a PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OnnxConfig Specification\n",
    "\n",
    "### Class Hierarchy\n",
    "```\n",
    "OnnxConfig (base class)\n",
    "├── OnnxConfigWithPast (for decoder models with KV cache)\n",
    "├── OnnxSeq2SeqConfig (for encoder-decoder models)\n",
    "└── Model-specific configs (BertOnnxConfig, GPT2OnnxConfig, etc.)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **inputs** property: Defines input names and dynamic axes\n",
    "2. **outputs** property: Defines output names and dynamic axes\n",
    "3. **generate_dummy_inputs()**: Creates example inputs for tracing\n",
    "4. **DEFAULT_ONNX_OPSET**: ONNX operator set version (default: 11)\n",
    "5. **ATOL_FOR_VALIDATION**: Tolerance for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the OnnxConfig interface\n",
    "from typing import Dict, Any, Tuple\n",
    "import torch\n",
    "\n",
    "class OnnxConfigInterface:\n",
    "    \"\"\"Simplified interface showing what OnnxConfig provides\"\"\"\n",
    "    \n",
    "    def __init__(self, config, task=\"default\"):\n",
    "        self.config = config  # HuggingFace PretrainedConfig\n",
    "        self.task = task      # Task like \"text-classification\"\n",
    "    \n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"Define input names and their dynamic axes\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping input names to axis definitions\n",
    "            Example: {\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "            }\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"Define output names and their dynamic axes\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def generate_dummy_inputs(self, \n",
    "                            batch_size: int = 1,\n",
    "                            seq_length: int = 128,\n",
    "                            **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Generate dummy inputs for model tracing\n",
    "        \n",
    "        Returns:\n",
    "            Dict of input tensors\n",
    "            Example: {\n",
    "                \"input_ids\": torch.tensor([[101, 2023, ...]])\n",
    "                \"attention_mask\": torch.tensor([[1, 1, ...]])\n",
    "            }\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to Generate OnnxConfig from HF Config\n",
    "\n",
    "The process involves:\n",
    "1. Load the HuggingFace model config\n",
    "2. Detect the model type and task\n",
    "3. Determine input/output specifications\n",
    "4. Create appropriate OnnxConfig\n",
    "\n",
    "Let's implement this step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load a HuggingFace config\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Example: Load BERT config\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(f\"Model type: {config.model_type}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Num layers: {config.num_hidden_layers}\")\n",
    "print(f\"Architectures: {config.architectures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Detect task from architecture\n",
    "def detect_task_from_config(config):\n",
    "    \"\"\"Detect the task from model config\"\"\"\n",
    "    \n",
    "    # Check architectures field\n",
    "    if hasattr(config, 'architectures') and config.architectures:\n",
    "        arch = config.architectures[0]\n",
    "        \n",
    "        # Map architecture patterns to tasks\n",
    "        if \"ForSequenceClassification\" in arch:\n",
    "            return \"text-classification\"\n",
    "        elif \"ForTokenClassification\" in arch:\n",
    "            return \"token-classification\"\n",
    "        elif \"ForQuestionAnswering\" in arch:\n",
    "            return \"question-answering\"\n",
    "        elif \"ForCausalLM\" in arch or \"LMHead\" in arch:\n",
    "            return \"text-generation\"\n",
    "        elif \"ForMaskedLM\" in arch:\n",
    "            return \"fill-mask\"\n",
    "        elif \"ForImageClassification\" in arch:\n",
    "            return \"image-classification\"\n",
    "    \n",
    "    # Default to feature extraction\n",
    "    return \"feature-extraction\"\n",
    "\n",
    "task = detect_task_from_config(config)\n",
    "print(f\"Detected task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Determine input specifications based on model type\n",
    "def get_input_specs_for_model_type(model_type: str, config):\n",
    "    \"\"\"Get input specifications for a model type\"\"\"\n",
    "    \n",
    "    # Common text model inputs\n",
    "    text_inputs = {\n",
    "        \"bert\": [\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "        \"roberta\": [\"input_ids\", \"attention_mask\"],\n",
    "        \"gpt2\": [\"input_ids\", \"attention_mask\"],\n",
    "        \"t5\": [\"input_ids\", \"attention_mask\", \"decoder_input_ids\"],\n",
    "    }\n",
    "    \n",
    "    # Vision model inputs\n",
    "    vision_inputs = {\n",
    "        \"vit\": [\"pixel_values\"],\n",
    "        \"resnet\": [\"pixel_values\"],\n",
    "    }\n",
    "    \n",
    "    # Get inputs for model type\n",
    "    if model_type in text_inputs:\n",
    "        return text_inputs[model_type]\n",
    "    elif model_type in vision_inputs:\n",
    "        return vision_inputs[model_type]\n",
    "    else:\n",
    "        # Default text inputs\n",
    "        return [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "input_names = get_input_specs_for_model_type(config.model_type, config)\n",
    "print(f\"Input names: {input_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a Universal OnnxConfig\n",
    "\n",
    "Now let's create a universal OnnxConfig that can work with any model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "class UniversalOnnxConfig:\n",
    "    \"\"\"Universal OnnxConfig that works with any HF model\"\"\"\n",
    "    \n",
    "    DEFAULT_ONNX_OPSET = 14  # Modern ONNX opset\n",
    "    \n",
    "    def __init__(self, config, task=\"default\"):\n",
    "        self.config = config\n",
    "        self.task = task if task != \"default\" else self._detect_task()\n",
    "        self.model_type = config.model_type\n",
    "        \n",
    "    def _detect_task(self) -> str:\n",
    "        \"\"\"Auto-detect task from config\"\"\"\n",
    "        if hasattr(self.config, 'architectures') and self.config.architectures:\n",
    "            arch = self.config.architectures[0]\n",
    "            if \"ForSequenceClassification\" in arch:\n",
    "                return \"text-classification\"\n",
    "            elif \"ForCausalLM\" in arch:\n",
    "                return \"text-generation\"\n",
    "        return \"feature-extraction\"\n",
    "    \n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"Define inputs with dynamic axes\"\"\"\n",
    "        \n",
    "        # Base dynamic axes for text\n",
    "        dynamic_axes = {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "        \n",
    "        # Determine input names based on model type\n",
    "        if self.model_type == \"bert\":\n",
    "            return {\n",
    "                \"input_ids\": dynamic_axes,\n",
    "                \"attention_mask\": dynamic_axes,\n",
    "                \"token_type_ids\": dynamic_axes,\n",
    "            }\n",
    "        elif self.model_type in [\"gpt2\", \"llama\", \"mistral\"]:\n",
    "            return {\n",
    "                \"input_ids\": dynamic_axes,\n",
    "                \"attention_mask\": dynamic_axes,\n",
    "            }\n",
    "        elif self.model_type in [\"vit\", \"resnet\"]:\n",
    "            # Vision models\n",
    "            return {\n",
    "                \"pixel_values\": {0: \"batch_size\"},\n",
    "            }\n",
    "        else:\n",
    "            # Default\n",
    "            return {\n",
    "                \"input_ids\": dynamic_axes,\n",
    "                \"attention_mask\": dynamic_axes,\n",
    "            }\n",
    "    \n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"Define outputs with dynamic axes\"\"\"\n",
    "        \n",
    "        if self.task == \"text-classification\":\n",
    "            return {\"logits\": {0: \"batch_size\"}}\n",
    "        elif self.task == \"text-generation\":\n",
    "            return {\n",
    "                \"logits\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            }\n",
    "        elif self.task == \"question-answering\":\n",
    "            return {\n",
    "                \"start_logits\": {0: \"batch_size\"},\n",
    "                \"end_logits\": {0: \"batch_size\"},\n",
    "            }\n",
    "        else:\n",
    "            # Feature extraction\n",
    "            return {\n",
    "                \"last_hidden_state\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            }\n",
    "    \n",
    "    def generate_dummy_inputs(self, \n",
    "                            batch_size: int = 1,\n",
    "                            seq_length: int = 128,\n",
    "                            **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Generate dummy inputs for tracing\"\"\"\n",
    "        \n",
    "        dummy_inputs = {}\n",
    "        \n",
    "        if self.model_type in [\"bert\", \"roberta\", \"gpt2\", \"llama\"]:\n",
    "            # Text models\n",
    "            dummy_inputs[\"input_ids\"] = torch.randint(\n",
    "                0, self.config.vocab_size, (batch_size, seq_length)\n",
    "            )\n",
    "            dummy_inputs[\"attention_mask\"] = torch.ones(\n",
    "                (batch_size, seq_length), dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            if self.model_type == \"bert\":\n",
    "                dummy_inputs[\"token_type_ids\"] = torch.zeros(\n",
    "                    (batch_size, seq_length), dtype=torch.long\n",
    "                )\n",
    "                \n",
    "        elif self.model_type in [\"vit\", \"resnet\"]:\n",
    "            # Vision models\n",
    "            image_size = getattr(self.config, \"image_size\", 224)\n",
    "            num_channels = getattr(self.config, \"num_channels\", 3)\n",
    "            \n",
    "            dummy_inputs[\"pixel_values\"] = torch.randn(\n",
    "                (batch_size, num_channels, image_size, image_size)\n",
    "            )\n",
    "        \n",
    "        return dummy_inputs\n",
    "    \n",
    "    def get_input_names(self) -> List[str]:\n",
    "        \"\"\"Get list of input names\"\"\"\n",
    "        return list(self.inputs.keys())\n",
    "    \n",
    "    def get_output_names(self) -> List[str]:\n",
    "        \"\"\"Get list of output names\"\"\"\n",
    "        return list(self.outputs.keys())\n",
    "    \n",
    "    def get_dynamic_axes(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"Get combined dynamic axes for inputs and outputs\"\"\"\n",
    "        return {**self.inputs, **self.outputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the Universal OnnxConfig\n",
    "\n",
    "Let's see how to use this universal config to export a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: BERT model\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# Load config and model\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Create universal OnnxConfig\n",
    "onnx_config = UniversalOnnxConfig(config)\n",
    "\n",
    "print(\"Task:\", onnx_config.task)\n",
    "print(\"Input names:\", onnx_config.get_input_names())\n",
    "print(\"Output names:\", onnx_config.get_output_names())\n",
    "print(\"\\nInputs with dynamic axes:\")\n",
    "for name, axes in onnx_config.inputs.items():\n",
    "    print(f\"  {name}: {axes}\")\n",
    "\n",
    "# Generate dummy inputs\n",
    "dummy_inputs = onnx_config.generate_dummy_inputs()\n",
    "print(\"\\nDummy input shapes:\")\n",
    "for name, tensor in dummy_inputs.items():\n",
    "    print(f\"  {name}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Export to ONNX using the config\n",
    "import torch\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def export_with_universal_config(model, config, output_path=\"model.onnx\"):\n",
    "    \"\"\"Export a model to ONNX using UniversalOnnxConfig\"\"\"\n",
    "    \n",
    "    # Create OnnxConfig\n",
    "    onnx_config = UniversalOnnxConfig(config)\n",
    "    \n",
    "    # Generate dummy inputs\n",
    "    dummy_inputs = onnx_config.generate_dummy_inputs()\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        tuple(dummy_inputs.values()),  # Convert dict to tuple\n",
    "        output_path,\n",
    "        input_names=onnx_config.get_input_names(),\n",
    "        output_names=onnx_config.get_output_names(),\n",
    "        dynamic_axes=onnx_config.get_dynamic_axes(),\n",
    "        opset_version=onnx_config.DEFAULT_ONNX_OPSET,\n",
    "        do_constant_folding=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model exported to {output_path}\")\n",
    "    print(f\"   File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return onnx_config\n",
    "\n",
    "# Export the model\n",
    "with tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmp:\n",
    "    export_with_universal_config(model, config, tmp.name)\n",
    "    \n",
    "    # Verify the export\n",
    "    import onnx\n",
    "    onnx_model = onnx.load(tmp.name)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"✅ ONNX model validation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Optimum's Approach\n",
    "\n",
    "HuggingFace Optimum uses model-specific OnnxConfig classes. Let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimum's approach (if available)\n",
    "try:\n",
    "    from optimum.exporters.onnx import BertOnnxConfig\n",
    "    \n",
    "    # Model-specific config\n",
    "    optimum_config = BertOnnxConfig(config)\n",
    "    \n",
    "    print(\"Optimum BertOnnxConfig:\")\n",
    "    print(\"  Inputs:\", list(optimum_config.inputs.keys()))\n",
    "    print(\"  Outputs:\", list(optimum_config.outputs.keys()))\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Optimum not installed. To see the comparison, install with:\")\n",
    "    print(\"pip install optimum[exporters]\")\n",
    "\n",
    "print(\"\\nOur UniversalOnnxConfig:\")\n",
    "universal_config = UniversalOnnxConfig(config)\n",
    "print(\"  Inputs:\", universal_config.get_input_names())\n",
    "print(\"  Outputs:\", universal_config.get_output_names())\n",
    "\n",
    "print(\"\\nAdvantage of Universal approach:\")\n",
    "print(\"✅ Works with ANY model without specific implementation\")\n",
    "print(\"✅ Automatically detects task and configuration\")\n",
    "print(\"✅ No need to maintain model-specific classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Handling Special Cases\n",
    "\n",
    "Some models need special handling. Here's how to extend the universal config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedUniversalOnnxConfig(UniversalOnnxConfig):\n",
    "    \"\"\"Extended version with special case handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config, task=\"default\"):\n",
    "        super().__init__(config, task)\n",
    "        self.use_past = self._should_use_past()\n",
    "        \n",
    "    def _should_use_past(self) -> bool:\n",
    "        \"\"\"Check if model uses past key values (for generation)\"\"\"\n",
    "        return (\n",
    "            self.task == \"text-generation\" and \n",
    "            self.model_type in [\"gpt2\", \"llama\", \"mistral\", \"falcon\"]\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        \"\"\"Extended inputs with past key values support\"\"\"\n",
    "        base_inputs = super().inputs\n",
    "        \n",
    "        # Add past key values for generation models\n",
    "        if self.use_past:\n",
    "            num_layers = getattr(self.config, \"n_layer\", \n",
    "                               getattr(self.config, \"num_hidden_layers\", 12))\n",
    "            \n",
    "            for i in range(num_layers):\n",
    "                base_inputs[f\"past_key_values.{i}.key\"] = {\n",
    "                    0: \"batch_size\", 2: \"past_sequence_length\"\n",
    "                }\n",
    "                base_inputs[f\"past_key_values.{i}.value\"] = {\n",
    "                    0: \"batch_size\", 2: \"past_sequence_length\"\n",
    "                }\n",
    "        \n",
    "        return base_inputs\n",
    "    \n",
    "    def generate_dummy_inputs(self, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Extended dummy inputs with special handling\"\"\"\n",
    "        dummy_inputs = super().generate_dummy_inputs(**kwargs)\n",
    "        \n",
    "        # Handle encoder-decoder models (T5, BART)\n",
    "        if self.model_type in [\"t5\", \"bart\", \"mbart\"]:\n",
    "            # Add decoder inputs\n",
    "            batch_size = dummy_inputs[\"input_ids\"].shape[0]\n",
    "            seq_length = dummy_inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            dummy_inputs[\"decoder_input_ids\"] = torch.randint(\n",
    "                0, self.config.vocab_size, (batch_size, seq_length)\n",
    "            )\n",
    "            \n",
    "        # Handle multimodal models (CLIP)\n",
    "        elif self.model_type == \"clip\":\n",
    "            # Add both text and image inputs\n",
    "            batch_size = kwargs.get(\"batch_size\", 1)\n",
    "            seq_length = kwargs.get(\"seq_length\", 77)  # CLIP default\n",
    "            \n",
    "            dummy_inputs[\"input_ids\"] = torch.randint(\n",
    "                0, self.config.vocab_size, (batch_size, seq_length)\n",
    "            )\n",
    "            dummy_inputs[\"attention_mask\"] = torch.ones(\n",
    "                (batch_size, seq_length), dtype=torch.long\n",
    "            )\n",
    "            dummy_inputs[\"pixel_values\"] = torch.randn(\n",
    "                (batch_size, 3, 224, 224)\n",
    "            )\n",
    "            \n",
    "        return dummy_inputs\n",
    "\n",
    "# Test with different model types\n",
    "test_models = [\n",
    "    \"gpt2\",           # Decoder-only\n",
    "    \"t5-small\",       # Encoder-decoder\n",
    "    \"openai/clip-vit-base-patch32\",  # Multimodal\n",
    "]\n",
    "\n",
    "for model_name in test_models:\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        adv_config = AdvancedUniversalOnnxConfig(config)\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Model type: {adv_config.model_type}\")\n",
    "        print(f\"  Task: {adv_config.task}\")\n",
    "        print(f\"  Input names: {adv_config.get_input_names()[:5]}...\")  # First 5\n",
    "        print(f\"  Uses past KV: {adv_config.use_past}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model_name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **OnnxConfig** is essential for ONNX export - it defines:\n",
    "   - Input/output specifications\n",
    "   - Dynamic axes for variable dimensions\n",
    "   - Dummy input generation for tracing\n",
    "\n",
    "2. **From HF Config to OnnxConfig**:\n",
    "   - Detect model type from `config.model_type`\n",
    "   - Detect task from `config.architectures`\n",
    "   - Map model type to input/output specs\n",
    "   - Generate appropriate dummy inputs\n",
    "\n",
    "3. **Universal Approach Benefits**:\n",
    "   - Works with any model without specific implementation\n",
    "   - Automatically detects configuration\n",
    "   - Reduces maintenance burden\n",
    "   - Easy to extend for special cases\n",
    "\n",
    "4. **Optimum's Approach**:\n",
    "   - Uses model-specific OnnxConfig classes\n",
    "   - More precise but requires implementation for each model\n",
    "   - Limited to ~100 supported architectures\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "To make this production-ready:\n",
    "1. Add more model type mappings\n",
    "2. Handle more special cases (vision-language, speech, etc.)\n",
    "3. Add validation and error handling\n",
    "4. Integrate with existing export pipelines\n",
    "5. Test with diverse model architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
