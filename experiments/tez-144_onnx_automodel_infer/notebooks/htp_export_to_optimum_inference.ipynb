{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTP Export to Optimum Inference Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Export a model using HTP exporter CLI\n",
    "2. Copy HuggingFace config alongside ONNX model\n",
    "3. Use Optimum ORTModel for inference\n",
    "\n",
    "## Prerequisites\n",
    "- modelexport package installed\n",
    "- transformers and optimum packages\n",
    "- ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path includes: {sys.path[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check available packages\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModel, ORTModelForFeatureExtraction\n",
    "    print(\"‚úÖ Optimum ONNX Runtime support available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Optimum not installed. Install with: pip install optimum[onnxruntime]\")\n",
    "\n",
    "try:\n",
    "    import onnxruntime\n",
    "    print(f\"‚úÖ ONNX Runtime version: {onnxruntime.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå ONNX Runtime not installed. Install with: pip install onnxruntime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Export BERT-tiny using HTP Exporter CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"prajjwal1/bert-tiny\"\n",
    "EXPORT_DIR = project_root / \"temp\" / \"bert-tiny-htp-export\"\n",
    "ONNX_FILE = EXPORT_DIR / \"model.onnx\"\n",
    "\n",
    "# Create export directory\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Export directory: {EXPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export using HTP exporter CLI\n",
    "export_command = [\n",
    "    sys.executable, \"-m\", \"modelexport\",\n",
    "    \"export\",\n",
    "    MODEL_NAME,\n",
    "    str(ONNX_FILE),\n",
    "    \"--strategy\", \"htp\",\n",
    "    \"--opset\", \"14\",\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Running HTP export command:\")\n",
    "print(\" \".join(export_command))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        export_command,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=project_root\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Export successful!\")\n",
    "        print(\"\\nOutput (last 20 lines):\")\n",
    "        print(\"\\n\".join(result.stdout.split(\"\\n\")[-20:]))\n",
    "    else:\n",
    "        print(\"‚ùå Export failed!\")\n",
    "        print(\"Error output:\")\n",
    "        print(result.stderr)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running export: {e}\")\n",
    "    print(\"\\nAlternative: You can run this command manually:\")\n",
    "    print(f\"cd {project_root}\")\n",
    "    print(\" \".join(export_command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ONNX file was created\n",
    "if ONNX_FILE.exists():\n",
    "    file_size = ONNX_FILE.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úÖ ONNX model exported: {ONNX_FILE}\")\n",
    "    print(f\"   File size: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå ONNX file not found at {ONNX_FILE}\")\n",
    "    print(\"\\nTrying alternative export method...\")\n",
    "    \n",
    "    # Alternative: Direct Python export\n",
    "    from modelexport.conversion.hf_universal_hierarchy_exporter import HuggingFaceUniversalHierarchyExporter\n",
    "    \n",
    "    exporter = HuggingFaceUniversalHierarchyExporter(\n",
    "        model_name=MODEL_NAME,\n",
    "        output_path=str(ONNX_FILE),\n",
    "        strategy=\"htp\",\n",
    "        opset_version=14\n",
    "    )\n",
    "    \n",
    "    success = exporter.export()\n",
    "    if success:\n",
    "        print(\"‚úÖ Alternative export successful!\")\n",
    "    else:\n",
    "        print(\"‚ùå Alternative export also failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Copy HuggingFace Config Alongside ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save HuggingFace config\n",
    "print(f\"üìã Loading HuggingFace config for {MODEL_NAME}...\")\n",
    "\n",
    "# Load the config from HuggingFace\n",
    "hf_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model type: {hf_config.model_type}\")\n",
    "print(f\"Hidden size: {hf_config.hidden_size}\")\n",
    "print(f\"Num layers: {hf_config.num_hidden_layers}\")\n",
    "print(f\"Vocab size: {hf_config.vocab_size}\")\n",
    "\n",
    "# Save config.json in the export directory\n",
    "config_path = EXPORT_DIR / \"config.json\"\n",
    "hf_config.save_pretrained(EXPORT_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ Config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save the tokenizer for convenience\n",
    "print(\"üìù Saving tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(EXPORT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer saved to: {EXPORT_DIR}\")\n",
    "\n",
    "# List all files in export directory\n",
    "print(\"\\nüìÅ Files in export directory:\")\n",
    "for file in EXPORT_DIR.iterdir():\n",
    "    if file.is_file():\n",
    "        size = file.stat().st_size / 1024\n",
    "        print(f\"  - {file.name}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load ONNX Model with Optimum ORTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model using Optimum\n",
    "print(\"üîß Loading ONNX model with Optimum...\")\n",
    "\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "    \n",
    "    # Load the model - it will use config.json automatically\n",
    "    ort_model = ORTModelForFeatureExtraction.from_pretrained(EXPORT_DIR)\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"Model type: {type(ort_model)}\")\n",
    "    print(f\"Config type: {type(ort_model.config)}\")\n",
    "    print(f\"Config model type: {ort_model.config.model_type}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading with Optimum: {e}\")\n",
    "    print(\"\\nTrying direct ONNX Runtime loading...\")\n",
    "    \n",
    "    # Fallback: Direct ONNX Runtime\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    session = ort.InferenceSession(str(ONNX_FILE))\n",
    "    print(\"‚úÖ Loaded with ONNX Runtime directly\")\n",
    "    print(f\"Input names: {[inp.name for inp in session.get_inputs()]}\")\n",
    "    print(f\"Output names: {[out.name for out in session.get_outputs()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inference with Random Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random test texts\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"ONNX models provide efficient inference.\",\n",
    "    \"HuggingFace Optimum makes deployment easy.\",\n",
    "    \"This is a test of the BERT-tiny model exported with HTP.\",\n",
    "    \"Machine learning models can be optimized for production.\"\n",
    "]\n",
    "\n",
    "print(\"üìù Test texts:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "print(\"\\nüî§ Tokenizing inputs...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(EXPORT_DIR)\n",
    "\n",
    "# Tokenize all texts\n",
    "inputs = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Input keys: {list(inputs.keys())}\")\n",
    "print(f\"\\nFirst sequence tokens (first 20):\")\n",
    "print(inputs['input_ids'][0][:20].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with Optimum ORTModel\n",
    "print(\"\\nüöÄ Running inference with Optimum ORTModel...\")\n",
    "\n",
    "try:\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = ort_model(**inputs)\n",
    "    \n",
    "    print(\"‚úÖ Inference successful!\")\n",
    "    \n",
    "    # Check output structure\n",
    "    if hasattr(outputs, 'last_hidden_state'):\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        print(f\"\\nOutput shape: {hidden_states.shape}\")\n",
    "        print(f\"Output type: {type(hidden_states)}\")\n",
    "        \n",
    "        # Get sentence embeddings (mean pooling)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "        \n",
    "        print(f\"\\nSentence embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "        \n",
    "        # Show first embedding (first few dimensions)\n",
    "        print(f\"\\nFirst sentence embedding (first 10 dims):\")\n",
    "        print(embeddings[0][:10].numpy())\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nOutput type: {type(outputs)}\")\n",
    "        if hasattr(outputs, 'logits'):\n",
    "            print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference: {e}\")\n",
    "    print(\"\\nDebug info:\")\n",
    "    print(f\"Model type: {type(ort_model) if 'ort_model' in locals() else 'Not loaded'}\")\n",
    "    print(f\"Input types: {type(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare with Original PyTorch Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original PyTorch model for comparison\n",
    "print(\"üìä Loading original PyTorch model for comparison...\")\n",
    "\n",
    "try:\n",
    "    pytorch_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    pytorch_model.eval()\n",
    "    \n",
    "    # Run inference with PyTorch model\n",
    "    with torch.no_grad():\n",
    "        pytorch_outputs = pytorch_model(**inputs)\n",
    "    \n",
    "    pytorch_hidden = pytorch_outputs.last_hidden_state\n",
    "    \n",
    "    # Compare outputs\n",
    "    if 'hidden_states' in locals():\n",
    "        diff = torch.abs(hidden_states - pytorch_hidden)\n",
    "        max_diff = diff.max().item()\n",
    "        mean_diff = diff.mean().item()\n",
    "        \n",
    "        print(f\"\\nüìà Comparison Results:\")\n",
    "        print(f\"Max difference: {max_diff:.6f}\")\n",
    "        print(f\"Mean difference: {mean_diff:.6f}\")\n",
    "        \n",
    "        if max_diff < 1e-3:\n",
    "            print(\"‚úÖ Excellent match! ONNX model is accurate.\")\n",
    "        elif max_diff < 1e-2:\n",
    "            print(\"‚úÖ Good match! Minor numerical differences.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Larger differences detected. May need investigation.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Could not load PyTorch model for comparison: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, inputs, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark model inference speed.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    avg_time = (end - start) / num_runs * 1000  # ms\n",
    "    return avg_time\n",
    "\n",
    "print(\"‚ö° Performance Benchmark\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Benchmark ONNX model\n",
    "if 'ort_model' in locals():\n",
    "    onnx_time = benchmark_model(ort_model, inputs)\n",
    "    print(f\"ONNX Runtime: {onnx_time:.2f} ms/batch\")\n",
    "\n",
    "# Benchmark PyTorch model\n",
    "if 'pytorch_model' in locals():\n",
    "    pytorch_time = benchmark_model(pytorch_model, inputs)\n",
    "    print(f\"PyTorch: {pytorch_time:.2f} ms/batch\")\n",
    "    \n",
    "    if 'onnx_time' in locals():\n",
    "        speedup = pytorch_time / onnx_time\n",
    "        print(f\"\\nüöÄ Speedup: {speedup:.2f}x\")\n",
    "        if speedup > 1:\n",
    "            print(\"‚úÖ ONNX is faster!\")\n",
    "        else:\n",
    "            print(\"üìù PyTorch is faster (may be due to small model/batch size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **HTP Export**: Used modelexport CLI to export BERT-tiny with hierarchy preservation\n",
    "2. **Config Setup**: Copied HuggingFace config.json alongside ONNX model\n",
    "3. **Optimum Loading**: Loaded ONNX model using ORTModelForFeatureExtraction\n",
    "4. **Inference**: Ran inference with tokenized text inputs\n",
    "5. **Verification**: Compared outputs with original PyTorch model\n",
    "6. **Performance**: Benchmarked ONNX vs PyTorch speed\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- The `config.json` (HuggingFace config) is all that's needed for Optimum inference\n",
    "- No OnnxConfig or ORTConfig required for basic inference\n",
    "- ONNX models can provide significant speedup for inference\n",
    "- The HTP export preserves model hierarchy in ONNX metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (optional)\n",
    "print(\"\\nüßπ Cleanup\")\n",
    "print(f\"Exported model is saved at: {EXPORT_DIR}\")\n",
    "print(\"You can keep it for future use or delete it.\")\n",
    "\n",
    "# Uncomment to delete\n",
    "# import shutil\n",
    "# shutil.rmtree(EXPORT_DIR)\n",
    "# print(\"‚úÖ Cleaned up export directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
