{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Pipeline Integration Demo\n",
    "\n",
    "This notebook demonstrates how to use the enhanced pipeline with ONNXTokenizer for seamless ONNX model integration.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Auto-detecting ONNXTokenizer**: Automatically detects batch_size and sequence_length from ONNX models\n",
    "2. **Enhanced Pipeline**: Uses `data_processor` parameter for any processor type\n",
    "3. **Convenience Functions**: Helper functions for common use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "# Import our enhanced components\n",
    "from enhanced_pipeline import pipeline, create_pipeline\n",
    "from onnx_tokenizer import ONNXTokenizer, create_auto_shape_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Base Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "model_dir = Path(\"../models/bert-tiny-optimum\")\n",
    "\n",
    "# Load base tokenizer and ONNX model\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "onnx_model = ORTModelForFeatureExtraction.from_pretrained(model_dir)\n",
    "\n",
    "print(f\"✅ Loaded model from {model_dir}\")\n",
    "print(f\"✅ Loaded tokenizer: {base_tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Direct ONNXTokenizer Usage (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ONNXTokenizer with auto-detection\n",
    "onnx_tokenizer = ONNXTokenizer(\n",
    "    tokenizer=base_tokenizer,\n",
    "    onnx_model=onnx_model  # Auto-detects shapes from model\n",
    ")\n",
    "\n",
    "print(f\"Auto-detected shapes: {onnx_tokenizer.fixed_batch_size}x{onnx_tokenizer.fixed_sequence_length}\")\n",
    "\n",
    "# Use with enhanced pipeline\n",
    "pipe = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=onnx_model,\n",
    "    data_processor=onnx_tokenizer  # Generic parameter works for any processor!\n",
    ")\n",
    "\n",
    "# Test inference\n",
    "results = pipe([\"Hello world!\", \"This is a test.\"])\n",
    "print(f\"Pipeline results shape: {np.array(results).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using create_pipeline (Full-Featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with all options\n",
    "full_pipe = create_pipeline(\n",
    "    task=\"feature-extraction\",\n",
    "    model=onnx_model,\n",
    "    data_processor=onnx_tokenizer,\n",
    "    device=\"cpu\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Test with different inputs\n",
    "test_texts = [\n",
    "    \"Short text\",\n",
    "    \"This is a longer text that might need truncation or padding based on the model's fixed input size\"\n",
    "]\n",
    "\n",
    "results = full_pipe(test_texts)\n",
    "print(f\"Full pipeline results shape: {np.array(results).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Convenience Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for auto-detection\n",
    "def create_onnx_pipeline(task, model_path, tokenizer_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a pipeline with ONNX model and auto-detecting tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        task: Pipeline task (e.g., \"feature-extraction\")\n",
    "        model_path: Path to ONNX model directory\n",
    "        tokenizer_name: HuggingFace tokenizer identifier\n",
    "        **kwargs: Additional pipeline arguments\n",
    "    \n",
    "    Returns:\n",
    "        Pipeline configured for ONNX inference\n",
    "    \"\"\"\n",
    "    # Load components\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    model = ORTModelForFeatureExtraction.from_pretrained(model_path)\n",
    "    \n",
    "    # Create auto-detecting ONNX tokenizer\n",
    "    onnx_tokenizer = ONNXTokenizer(tokenizer=tokenizer, onnx_model=model)\n",
    "    \n",
    "    # Create enhanced pipeline\n",
    "    return create_pipeline(\n",
    "        task=task,\n",
    "        model=model,\n",
    "        data_processor=onnx_tokenizer,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# Usage example\n",
    "convenience_pipe = create_onnx_pipeline(\n",
    "    task=\"feature-extraction\",\n",
    "    model_path=model_dir,\n",
    "    tokenizer_name=\"prajjwal1/bert-tiny\"\n",
    ")\n",
    "\n",
    "results = convenience_pipe(\"Convenience function test\")\n",
    "print(f\"Convenience pipeline shape: {np.array(results).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Manual Shape Override (When Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you want specific shapes regardless of the model\n",
    "custom_tokenizer = ONNXTokenizer(\n",
    "    tokenizer=base_tokenizer,\n",
    "    onnx_model=onnx_model,  # Model provided for reference\n",
    "    fixed_batch_size=4,     # But override with specific values\n",
    "    fixed_sequence_length=32\n",
    ")\n",
    "\n",
    "print(f\"Custom shapes: {custom_tokenizer.fixed_batch_size}x{custom_tokenizer.fixed_sequence_length}\")\n",
    "\n",
    "# Create pipeline with custom tokenizer\n",
    "custom_pipe = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=onnx_model,\n",
    "    data_processor=custom_tokenizer\n",
    ")\n",
    "\n",
    "# Test with custom shapes\n",
    "results = custom_pipe([\"Test 1\", \"Test 2\", \"Test 3\", \"Test 4\"])\n",
    "print(f\"Custom pipeline shape: {np.array(results).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Before vs After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BEFORE: Standard Pipeline (What doesn't work)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "from transformers import pipeline\n",
    "\n",
    "# ❌ This would fail:\n",
    "pipe = pipeline('feature-extraction', model=model, data_processor=custom_tokenizer)\n",
    "# ERROR: pipeline() got an unexpected keyword argument 'data_processor'\n",
    "\n",
    "# ❌ You'd have to remember different parameter names:\n",
    "text_pipe = pipeline('text-classification', tokenizer=tokenizer, model=model)\n",
    "vision_pipe = pipeline('image-classification', image_processor=processor, model=model)\n",
    "audio_pipe = pipeline('audio-classification', feature_extractor=extractor, model=model)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AFTER: Enhanced Pipeline (What now works)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "from enhanced_pipeline import pipeline\n",
    "\n",
    "# ✅ This works perfectly:\n",
    "pipe = pipeline('feature-extraction', model=model, data_processor=custom_tokenizer)\n",
    "\n",
    "# ✅ Same parameter name for all modalities:\n",
    "text_pipe = pipeline('text-classification', model=model, data_processor=tokenizer)\n",
    "vision_pipe = pipeline('image-classification', model=model, data_processor=processor)\n",
    "audio_pipe = pipeline('audio-classification', model=model, data_processor=extractor)\n",
    "\n",
    "# ✅ Auto-detection makes it even easier:\n",
    "onnx_tokenizer = ONNXTokenizer(base_tokenizer, onnx_model=model)  # Auto-detects shapes\n",
    "pipe = pipeline('feature-extraction', model=model, data_processor=onnx_tokenizer)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test data\n",
    "test_data = [\"This is a test sentence for performance comparison.\"] * 10\n",
    "\n",
    "# Time the enhanced pipeline\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    results = pipe(test_data)\n",
    "enhanced_time = time.time() - start_time\n",
    "\n",
    "print(f\"Enhanced pipeline (10 batches of 10): {enhanced_time:.3f}s\")\n",
    "print(f\"Average per batch: {enhanced_time/10:.3f}s\")\n",
    "print(f\"Results shape consistency: {np.array(results).shape}\")\n",
    "\n",
    "# Demonstrate shape consistency\n",
    "single_result = pipe(\"Single input\")\n",
    "batch_result = pipe([\"Input 1\", \"Input 2\"])\n",
    "print(f\"\\nShape consistency:\")\n",
    "print(f\"Single input shape: {np.array(single_result).shape}\")\n",
    "print(f\"Batch input shape: {np.array(batch_result).shape}\")\n",
    "print(f\"✅ Shapes are consistent thanks to ONNX tokenizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ✅ Key Benefits:\n",
    "\n",
    "1. **Auto-Detection**: No need to manually specify batch_size and sequence_length\n",
    "2. **Generic Interface**: Same `data_processor` parameter for all modalities  \n",
    "3. **Shape Consistency**: Handles variable input sizes with fixed ONNX shapes\n",
    "4. **Drop-in Replacement**: Works as a replacement for standard pipeline\n",
    "5. **Flexible**: Supports manual override when needed\n",
    "\n",
    "### 🎯 Usage Patterns:\n",
    "\n",
    "```python\n",
    "# Most common usage (recommended)\n",
    "onnx_tokenizer = ONNXTokenizer(base_tokenizer, onnx_model=model)\n",
    "pipe = pipeline(\"feature-extraction\", model=model, data_processor=onnx_tokenizer)\n",
    "\n",
    "# Full-featured usage\n",
    "pipe = create_pipeline(\"feature-extraction\", model=model, data_processor=onnx_tokenizer, device=\"cpu\")\n",
    "\n",
    "# Manual override when needed\n",
    "custom_tokenizer = ONNXTokenizer(base_tokenizer, onnx_model=model, fixed_batch_size=8, fixed_sequence_length=64)\n",
    "```\n",
    "\n",
    "This integration makes working with ONNX models as simple as regular HuggingFace pipelines while handling all the shape complexity automatically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}