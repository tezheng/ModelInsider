{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Config Copying with AutoConfig and AutoProcessor\n",
    "\n",
    "This notebook demonstrates the optimal approach for copying configuration files:\n",
    "- Use `AutoConfig.from_pretrained(model_id)` directly\n",
    "- Use `AutoTokenizer.from_pretrained(model_id)` for NLP models\n",
    "- Use `AutoProcessor.from_pretrained(model_id)` for vision/multimodal models\n",
    "\n",
    "**Key insight**: We don't need to load the full model weights, just the config files!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoProcessor, AutoImageProcessor\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Test output directory\n",
    "output_dir = Path(\"../models/config-only-test\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Copy Config Files for BERT (NLP Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_config_files(model_id: str, output_dir: Path):\n",
    "    \"\"\"Copy config files from HF model ID without loading model weights.\"\"\"\n",
    "    \n",
    "    print(f\"üìÅ Copying configs from '{model_id}'\")\n",
    "    \n",
    "    # 1. Config (always required)\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.save_pretrained(output_dir)\n",
    "    print(\"   ‚úÖ config.json\")\n",
    "    \n",
    "    # 2. Tokenizer (for NLP models)\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(\"   ‚úÖ tokenizer files\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ÑπÔ∏è  No tokenizer (vision/audio model)\")\n",
    "    \n",
    "    # 3. Image Processor (for vision models)\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "        processor.save_pretrained(output_dir)\n",
    "        print(\"   ‚úÖ image processor\")\n",
    "    except Exception:\n",
    "        print(\"   ‚ÑπÔ∏è  No image processor (NLP model)\")\n",
    "    \n",
    "    # 4. General Processor (for multimodal)\n",
    "    try:\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        processor.save_pretrained(output_dir)\n",
    "        print(\"   ‚úÖ general processor\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Summary\n",
    "    files = list(output_dir.glob(\"*\"))\n",
    "    total_size = sum(f.stat().st_size for f in files)\n",
    "    print(f\"\\n   üìä {len(files)} files, {total_size/1024:.1f} KB total\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Copying configs from 'prajjwal1/bert-tiny'\n",
      "   ‚úÖ config.json\n",
      "   ‚úÖ tokenizer files\n",
      "   ‚ÑπÔ∏è  No image processor (NLP model)\n",
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 5 files, 922.7 KB total\n",
      "\n",
      "Created files: ['config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ tokenizer files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ÑπÔ∏è  No image processor (NLP model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 5 files, 922.7 KB total\n",
      "\n",
      "Created files: ['config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    }
   ],
   "source": [
    "# Test with BERT\n",
    "bert_dir = output_dir / \"bert-tiny\"\n",
    "bert_dir.mkdir(exist_ok=True)\n",
    "\n",
    "bert_files = copy_config_files(\"prajjwal1/bert-tiny\", bert_dir)\n",
    "print(f\"\\nCreated files: {[f.name for f in bert_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Copy Config Files for Vision Model (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Copying configs from 'google/vit-base-patch16-224'\n",
      "   ‚úÖ config.json\n",
      "   ‚ÑπÔ∏è  No tokenizer (vision/audio model)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ image processor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 2 files, 68.4 KB total\n",
      "\n",
      "Created files: ['config.json', 'preprocessor_config.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 2 files, 68.4 KB total\n",
      "\n",
      "Created files: ['config.json', 'preprocessor_config.json']\n"
     ]
    }
   ],
   "source": [
    "# Test with Vision Transformer\n",
    "vit_dir = output_dir / \"vit-base\"\n",
    "vit_dir.mkdir(exist_ok=True)\n",
    "\n",
    "vit_files = copy_config_files(\"google/vit-base-patch16-224\", vit_dir)\n",
    "print(f\"\\nCreated files: {[f.name for f in vit_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Copy Config Files for Multimodal Model (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Copying configs from 'openai/clip-vit-base-patch32'\n",
      "   ‚úÖ config.json\n",
      "   ‚úÖ tokenizer files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ image processor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 7 files, 4914.1 KB total\n",
      "\n",
      "Created files: ['config.json', 'preprocessor_config.json', 'vocab.json', 'special_tokens_map.json', 'merges.txt', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 7 files, 4914.1 KB total\n",
      "\n",
      "Created files: ['config.json', 'preprocessor_config.json', 'vocab.json', 'special_tokens_map.json', 'merges.txt', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "# Test with CLIP (multimodal)\n",
    "clip_dir = output_dir / \"clip-base\"\n",
    "clip_dir.mkdir(exist_ok=True)\n",
    "\n",
    "clip_files = copy_config_files(\"openai/clip-vit-base-patch32\", clip_dir)\n",
    "print(f\"\\nCreated files: {[f.name for f in clip_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Config Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Config highlights:\n",
      "  Architecture: []\n",
      "  Model type: bert\n",
      "  Hidden size: 128\n",
      "  Num layers: 2\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect the BERT config\n",
    "with open(bert_dir / \"config.json\") as f:\n",
    "    bert_config = json.load(f)\n",
    "\n",
    "print(\"BERT Config highlights:\")\n",
    "print(f\"  Architecture: {bert_config.get('architectures', [])}\")\n",
    "print(f\"  Model type: {bert_config.get('model_type')}\")\n",
    "print(f\"  Hidden size: {bert_config.get('hidden_size')}\")\n",
    "print(f\"  Num layers: {bert_config.get('num_hidden_layers')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Config highlights:\n",
      "  Architecture: ['ViTForImageClassification']\n",
      "  Model type: vit\n",
      "  Image size: 224\n",
      "  Patch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect the ViT config\n",
    "with open(vit_dir / \"config.json\") as f:\n",
    "    vit_config = json.load(f)\n",
    "\n",
    "print(\"ViT Config highlights:\")\n",
    "print(f\"  Architecture: {vit_config.get('architectures', [])}\")\n",
    "print(f\"  Model type: {vit_config.get('model_type')}\")\n",
    "print(f\"  Image size: {vit_config.get('image_size')}\")\n",
    "print(f\"  Patch size: {vit_config.get('patch_size')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test with Existing ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Created mock ONNX export: ../models/config-only-test/simulated-export\n",
      "   Files before: ['config.json', 'special_tokens_map.json', 'model.onnx', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    }
   ],
   "source": [
    "# Simulate adding configs to existing ONNX export\n",
    "import shutil\n",
    "\n",
    "# Create a test directory with mock ONNX file\n",
    "test_export = output_dir / \"simulated-export\"\n",
    "test_export.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a dummy ONNX file (in real scenario, this comes from HTP export)\n",
    "dummy_onnx = test_export / \"model.onnx\"\n",
    "with open(dummy_onnx, \"w\") as f:\n",
    "    f.write(\"# This would be the actual ONNX model from HTP export\\n\")\n",
    "\n",
    "print(f\"1. Created mock ONNX export: {test_export}\")\n",
    "print(f\"   Files before: {[f.name for f in test_export.glob('*')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Adding config files using model ID:\n",
      "üìÅ Copying configs from 'prajjwal1/bert-tiny'\n",
      "   ‚úÖ config.json\n",
      "   ‚úÖ tokenizer files\n",
      "   ‚ÑπÔ∏è  No image processor (NLP model)\n",
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 6 files, 922.7 KB total\n",
      "\n",
      "   Files after: ['config.json', 'special_tokens_map.json', 'model.onnx', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ tokenizer files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ÑπÔ∏è  No image processor (NLP model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ general processor\n",
      "\n",
      "   üìä 6 files, 922.7 KB total\n",
      "\n",
      "   Files after: ['config.json', 'special_tokens_map.json', 'model.onnx', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    }
   ],
   "source": [
    "# Add config files to the existing export\n",
    "print(\"\\n2. Adding config files using model ID:\")\n",
    "copy_config_files(\"prajjwal1/bert-tiny\", test_export)\n",
    "\n",
    "print(f\"\\n   Files after: {[f.name for f in test_export.glob('*')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation for ModelExport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Exporting prajjwal1/bert-tiny with config files\n",
      "   1. Export ONNX with HTP -> model.onnx\n",
      "   2. Copy config files from model ID\n",
      "      ‚úÖ Tokenizer saved\n",
      "      ‚ÑπÔ∏è  No image processor (not a vision model)\n",
      "   3. Validate export\n",
      "   ‚úÖ Export complete: 5 files\n",
      "\n",
      "üéâ Export result: ['config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Tokenizer saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ÑπÔ∏è  No image processor (not a vision model)\n",
      "   3. Validate export\n",
      "   ‚úÖ Export complete: 5 files\n",
      "\n",
      "üéâ Export result: ['config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n"
     ]
    }
   ],
   "source": [
    "def export_with_config(model_id: str, output_dir: Path, onnx_filename: str = \"model.onnx\"):\n",
    "    \"\"\"\n",
    "    Complete export function implementing our \"Always Copy Configuration\" approach.\n",
    "    \n",
    "    This is the function we'll implement in the HTP exporter.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Exporting {model_id} with config files\")\n",
    "    \n",
    "    # Step 1: Export ONNX (this would be our HTP export)\n",
    "    # export_onnx_with_hierarchy(model_id, output_dir / onnx_filename)\n",
    "    print(f\"   1. Export ONNX with HTP -> {onnx_filename}\")\n",
    "    \n",
    "    # Step 2: Copy config files (the focus of this notebook)\n",
    "    print(f\"   2. Copy config files from model ID\")\n",
    "    \n",
    "    # Config (always required)\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.save_pretrained(output_dir)\n",
    "    \n",
    "    # Tokenizer (conditional)\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"      ‚úÖ Tokenizer saved\")\n",
    "    except Exception:\n",
    "        print(f\"      ‚ÑπÔ∏è  No tokenizer (not an NLP model)\")\n",
    "    \n",
    "    # Image processor (conditional)\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "        processor.save_pretrained(output_dir)\n",
    "        print(f\"      ‚úÖ Image processor saved\")\n",
    "    except Exception:\n",
    "        print(f\"      ‚ÑπÔ∏è  No image processor (not a vision model)\")\n",
    "    \n",
    "    # Step 3: Validate\n",
    "    print(f\"   3. Validate export\")\n",
    "    if not (output_dir / \"config.json\").exists():\n",
    "        raise FileNotFoundError(\"config.json not created - export failed!\")\n",
    "    \n",
    "    files = list(output_dir.glob(\"*\"))\n",
    "    print(f\"   ‚úÖ Export complete: {len(files)} files\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Test the complete function\n",
    "demo_export = output_dir / \"complete-export-demo\"\n",
    "demo_export.mkdir(exist_ok=True)\n",
    "\n",
    "result = export_with_config(\"prajjwal1/bert-tiny\", demo_export)\n",
    "print(f\"\\nüéâ Export result: {[f.name for f in result.glob('*')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Config Copying Performance:\n",
      "   Model                            Time (ms) Files Size (KB)\n",
      "   ------------------------------------------------------------\n",
      "   bert-tiny                           2763.1     5    922.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vit-base-patch16-224                1585.5     2     68.4\n",
      "   clip-vit-base-patch3                1164.4     7   4914.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vit-base-patch16-224                1634.7     2     68.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   clip-vit-base-patch3                1117.3     7   4914.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_config_copying():\n",
    "    \"\"\"Benchmark how fast config copying is.\"\"\"\n",
    "    \n",
    "    models_to_test = [\n",
    "        \"prajjwal1/bert-tiny\",\n",
    "        \"google/vit-base-patch16-224\", \n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    ]\n",
    "    \n",
    "    print(\"‚è±Ô∏è  Config Copying Performance:\")\n",
    "    print(\"   Model\".ljust(35), \"Time (ms)\", \"Files\", \"Size (KB)\")\n",
    "    print(\"   \" + \"-\" * 60)\n",
    "    \n",
    "    for model_id in models_to_test:\n",
    "        test_dir = output_dir / f\"benchmark-{model_id.replace('/', '-')}\"\n",
    "        test_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Clean directory\n",
    "        for f in test_dir.glob(\"*\"):\n",
    "            f.unlink()\n",
    "        \n",
    "        # Time the config copying\n",
    "        start_time = time.time()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_id)\n",
    "        config.save_pretrained(test_dir)\n",
    "        \n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            tokenizer.save_pretrained(test_dir)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "            processor.save_pretrained(test_dir)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        elapsed_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Calculate stats\n",
    "        files = list(test_dir.glob(\"*\"))\n",
    "        total_size = sum(f.stat().st_size for f in files)\n",
    "        \n",
    "        model_short = model_id.split('/')[-1][:20]\n",
    "        print(f\"   {model_short.ljust(35)} {elapsed_ms:>6.1f} {len(files):>5} {total_size/1024:>8.1f}\")\n",
    "\n",
    "benchmark_config_copying()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### ‚úÖ Benefits of Config-Only Approach\n",
    "\n",
    "1. **Much Faster**: No need to download/load model weights (GBs)\n",
    "2. **Memory Efficient**: Only loads config files (KBs)\n",
    "3. **Universal**: Works for any HuggingFace model\n",
    "4. **Simple**: Direct API calls, no complex logic\n",
    "\n",
    "### üìù Implementation Pattern\n",
    "\n",
    "```python\n",
    "def add_configs_to_export(model_id: str, output_dir: Path):\n",
    "    # Always copy config.json (required by Optimum)\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    config.save_pretrained(output_dir)\n",
    "    \n",
    "    # Try tokenizer (for NLP models)\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "    except: pass\n",
    "    \n",
    "    # Try image processor (for vision models) \n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "        processor.save_pretrained(output_dir)\n",
    "    except: pass\n",
    "```\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. Integrate this pattern into HTP exporter\n",
    "2. Update CLI to use `export_with_config()` by default\n",
    "3. Add validation tests\n",
    "4. Create production-ready implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
