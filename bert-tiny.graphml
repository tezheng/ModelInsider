<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="graph" attr.name="class_name" attr.type="string" />
  <key id="d1" for="graph" attr.name="module_type" attr.type="string" />
  <key id="d2" for="graph" attr.name="execution_order" attr.type="int" />
  <key id="d3" for="graph" attr.name="traced_tag" attr.type="string" />
  <key id="n0" for="node" attr.name="op_type" attr.type="string" />
  <key id="n1" for="node" attr.name="hierarchy_tag" attr.type="string" />
  <key id="n2" for="node" attr.name="node_attributes" attr.type="string" />
  <key id="n3" for="node" attr.name="name" attr.type="string" />
  <key id="n4" for="node" attr.name="input_names" attr.type="string" />
  <key id="n5" for="node" attr.name="output_names" attr.type="string" />
  <key id="n6" for="node" attr.name="domain" attr.type="string" />
  <key id="e0" for="edge" attr.name="tensor_name" attr.type="string" />
  <key id="t0" for="edge" attr.name="tensor_type" attr.type="string" />
  <key id="t1" for="edge" attr.name="tensor_shape" attr.type="string" />
  <key id="t2" for="edge" attr.name="tensor_data_ref" attr.type="string" />
  <key id="m0" for="graph" attr.name="source_onnx_text" attr.type="string" />
  <key id="m1" for="graph" attr.name="source_htp" attr.type="string" />
  <key id="m2" for="graph" attr.name="format_version" attr.type="string" />
  <key id="m3" for="graph" attr.name="export_timestamp" attr.type="string" />
  <key id="m4" for="graph" attr.name="opset_imports" attr.type="string" />
  <key id="m5" for="graph" attr.name="producer_name" attr.type="string" />
  <key id="m6" for="graph" attr.name="producer_version" attr.type="string" />
  <key id="m7" for="graph" attr.name="model_version" attr.type="string" />
  <key id="m8" for="graph" attr.name="doc_string" attr.type="string" />
  <key id="p0" for="graph" attr.name="parameter_strategy" attr.type="string" />
  <key id="p1" for="graph" attr.name="parameter_file" attr.type="string" />
  <key id="p2" for="graph" attr.name="parameter_checksum" attr.type="string" />
  <key id="p3" for="graph" attr.name="parameter_count" attr.type="string" />
  <key id="g0" for="graph" attr.name="graph_inputs" attr.type="string" />
  <key id="g1" for="graph" attr.name="graph_outputs" attr.type="string" />
  <key id="g2" for="graph" attr.name="value_info" attr.type="string" />
  <key id="g3" for="graph" attr.name="initializers_ref" attr.type="string" />
  <graph id="BertModel" edgedefault="directed">
    <data key="d0">BertModel</data>
    <data key="d1">huggingface</data>
    <data key="d2">0</data>
    <data key="d3">/BertModel</data>
    <data key="m2">1.1</data>
    <data key="m3">2025-08-01T19:08:06.089134</data>
    <data key="m4">[{"domain": "", "version": 17}]</data>
    <data key="m5">pytorch</data>
    <data key="m6">2.7.1</data>
    <data key="m7">0</data>
    <data key="m8" />
    <data key="p0">sidecar</data>
    <data key="p1">bert-tiny.onnxdata</data>
    <data key="p2">sha256:bdd7767842aac2d95518c6aec9bd42f9f5df30265769ab532b933686f86fcbc5</data>
    <data key="g0">[{"name": "input_ids", "type": "int64", "shape": [2, 16]}, {"name": "attention_mask", "type": "int64", "shape": [2, 16]}, {"name": "token_type_ids", "type": "int64", "shape": [2, 16]}]</data>
    <data key="g1">[{"name": "last_hidden_state", "type": "float32", "shape": [2, 16, 128]}, {"name": "pooler_output", "type": "float32", "shape": [2, 128]}]</data>
    <data key="g2">[]</data>
    <data key="g3">bert-tiny.onnxdata</data>
    <node id="embeddings">
      <data key="n0">BertEmbeddings</data>
      <data key="n1">/BertModel/BertEmbeddings</data>
      <data key="n2">{"module_type": "torch_module", "execution_order": 1}</data>
      <data key="n3">embeddings</data>
      <graph id="embeddings::" edgedefault="directed">
        <data key="d0">BertEmbeddings</data>
        <data key="d1">torch_module</data>
        <data key="d2">1</data>
        <data key="d3">/BertModel/BertEmbeddings</data>
        <node id="_embeddings_Constant">
          <data key="n0">Constant</data>
          <data key="n1">/BertModel/BertEmbeddings</data>
          <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [], "data_b64": "AAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEmbeddings"}</data>
          <data key="n3">/embeddings/Constant</data>
          <data key="n4">[]</data>
          <data key="n5">["/embeddings/Constant_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_Constant_1">
          <data key="n0">Constant</data>
          <data key="n1">/BertModel/BertEmbeddings</data>
          <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1, 16], "data_b64": "AAAAAAAAAAABAAAAAAAAAAIAAAAAAAAAAwAAAAAAAAAEAAAAAAAAAAUAAAAAAAAABgAAAAAAAAAHAAAAAAAAAAgAAAAAAAAACQAAAAAAAAAKAAAAAAAAAAsAAAAAAAAADAAAAAAAAAANAAAAAAAAAA4AAAAAAAAADwAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEmbeddings"}</data>
          <data key="n3">/embeddings/Constant_1</data>
          <data key="n4">[]</data>
          <data key="n5">["/embeddings/Constant_1_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_word_embeddings_Gather">
          <data key="n0">Gather</data>
          <data key="n1">/BertModel/BertEmbeddings/Embedding</data>
          <data key="n2">{"hierarchy_tag": "/BertModel/BertEmbeddings/Embedding"}</data>
          <data key="n3">/embeddings/word_embeddings/Gather</data>
          <data key="n4">["embeddings.word_embeddings.weight", "input_ids"]</data>
          <data key="n5">["/embeddings/word_embeddings/Gather_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_token_type_embeddings_Gather">
          <data key="n0">Gather</data>
          <data key="n1">/BertModel/BertEmbeddings/Embedding</data>
          <data key="n2">{"hierarchy_tag": "/BertModel/BertEmbeddings/Embedding"}</data>
          <data key="n3">/embeddings/token_type_embeddings/Gather</data>
          <data key="n4">["embeddings.token_type_embeddings.weight", "token_type_ids"]</data>
          <data key="n5">["/embeddings/token_type_embeddings/Gather_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_Add">
          <data key="n0">Add</data>
          <data key="n1">/BertModel/BertEmbeddings</data>
          <data key="n2">{"hierarchy_tag": "/BertModel/BertEmbeddings"}</data>
          <data key="n3">/embeddings/Add</data>
          <data key="n4">["/embeddings/word_embeddings/Gather_output_0", "/embeddings/token_type_embeddings/Gather_output_0"]</data>
          <data key="n5">["/embeddings/Add_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_position_embeddings_Gather">
          <data key="n0">Gather</data>
          <data key="n1">/BertModel/BertEmbeddings/Embedding</data>
          <data key="n2">{"hierarchy_tag": "/BertModel/BertEmbeddings/Embedding"}</data>
          <data key="n3">/embeddings/position_embeddings/Gather</data>
          <data key="n4">["embeddings.position_embeddings.weight", "/embeddings/Constant_1_output_0"]</data>
          <data key="n5">["/embeddings/position_embeddings/Gather_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_Add_1">
          <data key="n0">Add</data>
          <data key="n1">/BertModel/BertEmbeddings</data>
          <data key="n2">{"hierarchy_tag": "/BertModel/BertEmbeddings"}</data>
          <data key="n3">/embeddings/Add_1</data>
          <data key="n4">["/embeddings/Add_output_0", "/embeddings/position_embeddings/Gather_output_0"]</data>
          <data key="n5">["/embeddings/Add_1_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_embeddings_LayerNorm_LayerNormalization">
          <data key="n0">LayerNormalization</data>
          <data key="n1">/BertModel/BertEmbeddings/LayerNorm</data>
          <data key="n2">{"axis": -1, "epsilon": 9.999999960041972e-13, "hierarchy_tag": "/BertModel/BertEmbeddings/LayerNorm"}</data>
          <data key="n3">/embeddings/LayerNorm/LayerNormalization</data>
          <data key="n4">["/embeddings/Add_1_output_0", "embeddings.LayerNorm.weight", "embeddings.LayerNorm.bias"]</data>
          <data key="n5">["/embeddings/LayerNorm/LayerNormalization_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="embeddings.position_embeddings">
          <data key="n0">Embedding</data>
          <data key="n1">/BertModel/BertEmbeddings/Embedding</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 4}</data>
          <data key="n3">embeddings.position_embeddings</data>
          <graph id="embeddings.position_embeddings::" edgedefault="directed">
            <data key="d0">Embedding</data>
            <data key="d1">torch_module</data>
            <data key="d2">4</data>
            <data key="d3">/BertModel/BertEmbeddings/Embedding</data>
          </graph>
        </node>
        <node id="embeddings.LayerNorm">
          <data key="n0">LayerNorm</data>
          <data key="n1">/BertModel/BertEmbeddings/LayerNorm</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 5}</data>
          <data key="n3">embeddings.LayerNorm</data>
          <graph id="embeddings.LayerNorm::" edgedefault="directed">
            <data key="d0">LayerNorm</data>
            <data key="d1">torch_module</data>
            <data key="d2">5</data>
            <data key="d3">/BertModel/BertEmbeddings/LayerNorm</data>
          </graph>
        </node>
        <node id="embeddings.dropout">
          <data key="n0">Dropout</data>
          <data key="n1">/BertModel/BertEmbeddings/Dropout</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 6}</data>
          <data key="n3">embeddings.dropout</data>
          <graph id="embeddings.dropout::" edgedefault="directed">
            <data key="d0">Dropout</data>
            <data key="d1">torch_module</data>
            <data key="d2">6</data>
            <data key="d3">/BertModel/BertEmbeddings/Dropout</data>
          </graph>
        </node>
      </graph>
    </node>
    <node id="encoder">
      <data key="n0">BertEncoder</data>
      <data key="n1">/BertModel/BertEncoder</data>
      <data key="n2">{"module_type": "torch_module", "execution_order": 7}</data>
      <data key="n3">encoder</data>
      <graph id="encoder::" edgedefault="directed">
        <data key="d0">BertEncoder</data>
        <data key="d1">torch_module</data>
        <data key="d2">7</data>
        <data key="d3">/BertModel/BertEncoder</data>
        <node id="encoder.layer.0">
          <data key="n0">BertLayer</data>
          <data key="n1">/BertModel/BertEncoder/BertLayer.0</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 8}</data>
          <data key="n3">encoder.layer.0</data>
          <graph id="encoder.layer.0::" edgedefault="directed">
            <data key="d0">BertLayer</data>
            <data key="d1">torch_module</data>
            <data key="d2">8</data>
            <data key="d3">/BertModel/BertEncoder/BertLayer.0</data>
            <node id="_encoder_layer_0_output_Add">
              <data key="n0">Add</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput</data>
              <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertOutput"}</data>
              <data key="n3">/encoder/layer.0/output/Add</data>
              <data key="n4">["/encoder/layer.0/output/dense/Add_output_0", "/encoder/layer.0/attention/output/LayerNorm/LayerNormalization_output_0"]</data>
              <data key="n5">["/encoder/layer.0/output/Add_output_0"]</data>
              <data key="n6" />
            </node>
            <node id="encoder.layer.0.attention">
              <data key="n0">BertAttention</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention</data>
              <data key="n2">{"module_type": "torch_module", "execution_order": 9}</data>
              <data key="n3">encoder.layer.0.attention</data>
              <graph id="encoder.layer.0.attention::" edgedefault="directed">
                <data key="d0">BertAttention</data>
                <data key="d1">torch_module</data>
                <data key="d2">9</data>
                <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention</data>
                <node id="_encoder_layer_0_attention_self_Constant">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAAQAAAAAAAAAAIAAAAAAAAAQAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Reshape">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Reshape</data>
                  <data key="n4">["/encoder/layer.0/attention/self/query/Add_output_0", "/encoder/layer.0/attention/self/Constant_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Reshape_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Transpose">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 1, 3], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Transpose</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Reshape_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Transpose_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Constant_1">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAAQAAAAAAAAAAIAAAAAAAAAQAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant_1</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Reshape_1">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Reshape_1</data>
                  <data key="n4">["/encoder/layer.0/attention/self/key/Add_output_0", "/encoder/layer.0/attention/self/Constant_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Reshape_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Constant_2">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAAQAAAAAAAAAAIAAAAAAAAAQAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant_2</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Reshape_2">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Reshape_2</data>
                  <data key="n4">["/encoder/layer.0/attention/self/value/Add_output_0", "/encoder/layer.0/attention/self/Constant_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Reshape_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Transpose_1">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 1, 3], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Transpose_1</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Reshape_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Transpose_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Shape">
                  <data key="n0">Shape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Shape</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Transpose_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Shape_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Constant_3">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "//////////8=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant_3</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_3_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Constant_4">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "/////////38=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant_4</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_4_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Slice">
                  <data key="n0">Slice</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Slice</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Shape_output_0", "/encoder/layer.0/attention/self/Constant_3_output_0", "/encoder/layer.0/attention/self/Constant_4_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Slice_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Cast">
                  <data key="n0">Cast</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"to": 1, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Cast</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Slice_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Cast_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Sqrt">
                  <data key="n0">Sqrt</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Sqrt</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Cast_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Sqrt_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Constant_5">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [1], "data_b64": "AACAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant_5</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_5_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Div">
                  <data key="n0">Div</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Div</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Constant_5_output_0", "/encoder/layer.0/attention/self/Sqrt_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Div_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Cast_1">
                  <data key="n0">Cast</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"to": 1, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Cast_1</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Div_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Cast_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Transpose_2">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 3, 1], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Transpose_2</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Reshape_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Transpose_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Sqrt_1">
                  <data key="n0">Sqrt</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Sqrt_1</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Cast_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Sqrt_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Mul">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Mul</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Transpose_output_0", "/encoder/layer.0/attention/self/Sqrt_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Mul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Sqrt_2">
                  <data key="n0">Sqrt</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Sqrt_2</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Cast_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Sqrt_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Mul_1">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Mul_1</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Transpose_2_output_0", "/encoder/layer.0/attention/self/Sqrt_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Mul_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_MatMul">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/MatMul</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Mul_output_0", "/encoder/layer.0/attention/self/Mul_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/MatMul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Add</data>
                  <data key="n4">["/encoder/layer.0/attention/self/MatMul_output_0", "/Where_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Softmax">
                  <data key="n0">Softmax</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"axis": -1, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Softmax</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Add_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Softmax_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_MatMul_1">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/MatMul_1</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Softmax_output_0", "/encoder/layer.0/attention/self/Transpose_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/MatMul_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Transpose_3">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 1, 3], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Transpose_3</data>
                  <data key="n4">["/encoder/layer.0/attention/self/MatMul_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Transpose_3_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Constant_6">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [3], "data_b64": "AgAAAAAAAAAQAAAAAAAAAIAAAAAAAAAA", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Constant_6</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Constant_6_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_self_Reshape_3">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.0/attention/self/Reshape_3</data>
                  <data key="n4">["/encoder/layer.0/attention/self/Transpose_3_output_0", "/encoder/layer.0/attention/self/Constant_6_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/self/Reshape_3_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_attention_output_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput"}</data>
                  <data key="n3">/encoder/layer.0/attention/output/Add</data>
                  <data key="n4">["/encoder/layer.0/attention/output/dense/Add_output_0", "/embeddings/LayerNorm/LayerNormalization_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/attention/output/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="encoder.layer.0.attention.self">
                  <data key="n0">BertSdpaSelfAttention</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 10}</data>
                  <data key="n3">encoder.layer.0.attention.self</data>
                  <graph id="encoder.layer.0.attention.self::" edgedefault="directed">
                    <data key="d0">BertSdpaSelfAttention</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">10</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention</data>
                    <node id="_encoder_layer_0_attention_self_query_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/self/query/MatMul</data>
                      <data key="n4">["/embeddings/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_290"]</data>
                      <data key="n5">["/encoder/layer.0/attention/self/query/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_self_query_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/self/query/Add</data>
                      <data key="n4">["encoder.layer.0.attention.self.query.bias", "/encoder/layer.0/attention/self/query/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.0/attention/self/query/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_self_key_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/self/key/MatMul</data>
                      <data key="n4">["/embeddings/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_296"]</data>
                      <data key="n5">["/encoder/layer.0/attention/self/key/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_self_key_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/self/key/Add</data>
                      <data key="n4">["encoder.layer.0.attention.self.key.bias", "/encoder/layer.0/attention/self/key/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.0/attention/self/key/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_self_value_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/self/value/MatMul</data>
                      <data key="n4">["/embeddings/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_302"]</data>
                      <data key="n5">["/encoder/layer.0/attention/self/value/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_self_value_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/self/value/Add</data>
                      <data key="n4">["encoder.layer.0.attention.self.value.bias", "/encoder/layer.0/attention/self/value/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.0/attention/self/value/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="encoder.layer.0.attention.self.value">
                      <data key="n0">Linear</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 13}</data>
                      <data key="n3">encoder.layer.0.attention.self.value</data>
                      <graph id="encoder.layer.0.attention.self.value::" edgedefault="directed">
                        <data key="d0">Linear</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">13</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSdpaSelfAttention/Linear</data>
                      </graph>
                    </node>
                  </graph>
                </node>
                <node id="encoder.layer.0.attention.output">
                  <data key="n0">BertSelfOutput</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 14}</data>
                  <data key="n3">encoder.layer.0.attention.output</data>
                  <graph id="encoder.layer.0.attention.output::" edgedefault="directed">
                    <data key="d0">BertSelfOutput</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">14</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput</data>
                    <node id="_encoder_layer_0_attention_output_dense_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/output/dense/MatMul</data>
                      <data key="n4">["/encoder/layer.0/attention/self/Reshape_3_output_0", "onnx::MatMul_312"]</data>
                      <data key="n5">["/encoder/layer.0/attention/output/dense/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_output_dense_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Linear"}</data>
                      <data key="n3">/encoder/layer.0/attention/output/dense/Add</data>
                      <data key="n4">["encoder.layer.0.attention.output.dense.bias", "/encoder/layer.0/attention/output/dense/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.0/attention/output/dense/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_0_attention_output_LayerNorm_LayerNormalization">
                      <data key="n0">LayerNormalization</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/LayerNorm</data>
                      <data key="n2">{"axis": -1, "epsilon": 9.999999960041972e-13, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/LayerNorm"}</data>
                      <data key="n3">/encoder/layer.0/attention/output/LayerNorm/LayerNormalization</data>
                      <data key="n4">["/encoder/layer.0/attention/output/Add_output_0", "encoder.layer.0.attention.output.LayerNorm.weight", "encoder.layer.0.attention.output.LayerNorm.bias"]</data>
                      <data key="n5">["/encoder/layer.0/attention/output/LayerNorm/LayerNormalization_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="encoder.layer.0.attention.output.dense">
                      <data key="n0">Linear</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Linear</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 15}</data>
                      <data key="n3">encoder.layer.0.attention.output.dense</data>
                      <graph id="encoder.layer.0.attention.output.dense::" edgedefault="directed">
                        <data key="d0">Linear</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">15</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Linear</data>
                      </graph>
                    </node>
                    <node id="encoder.layer.0.attention.output.dropout">
                      <data key="n0">Dropout</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Dropout</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 16}</data>
                      <data key="n3">encoder.layer.0.attention.output.dropout</data>
                      <graph id="encoder.layer.0.attention.output.dropout::" edgedefault="directed">
                        <data key="d0">Dropout</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">16</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/Dropout</data>
                      </graph>
                    </node>
                    <node id="encoder.layer.0.attention.output.LayerNorm">
                      <data key="n0">LayerNorm</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/LayerNorm</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 17}</data>
                      <data key="n3">encoder.layer.0.attention.output.LayerNorm</data>
                      <graph id="encoder.layer.0.attention.output.LayerNorm::" edgedefault="directed">
                        <data key="d0">LayerNorm</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">17</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertAttention/BertSelfOutput/LayerNorm</data>
                      </graph>
                    </node>
                  </graph>
                </node>
              </graph>
            </node>
            <node id="encoder.layer.0.intermediate">
              <data key="n0">BertIntermediate</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate</data>
              <data key="n2">{"module_type": "torch_module", "execution_order": 18}</data>
              <data key="n3">encoder.layer.0.intermediate</data>
              <graph id="encoder.layer.0.intermediate::" edgedefault="directed">
                <data key="d0">BertIntermediate</data>
                <data key="d1">torch_module</data>
                <data key="d2">18</data>
                <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertIntermediate</data>
                <node id="_encoder_layer_0_intermediate_dense_MatMul">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/Linear"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/dense/MatMul</data>
                  <data key="n4">["/encoder/layer.0/attention/output/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_313"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/dense/MatMul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_dense_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/Linear"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/dense/Add</data>
                  <data key="n4">["encoder.layer.0.intermediate.dense.bias", "/encoder/layer.0/intermediate/dense/MatMul_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/dense/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Constant">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "8wS1Pw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Constant</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Constant_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Div">
                  <data key="n0">Div</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Div</data>
                  <data key="n4">["/encoder/layer.0/intermediate/dense/Add_output_0", "/encoder/layer.0/intermediate/intermediate_act_fn/Constant_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Div_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Erf">
                  <data key="n0">Erf</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Erf</data>
                  <data key="n4">["/encoder/layer.0/intermediate/intermediate_act_fn/Div_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Erf_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Constant_1">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "AACAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Constant_1</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Constant_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Add</data>
                  <data key="n4">["/encoder/layer.0/intermediate/intermediate_act_fn/Erf_output_0", "/encoder/layer.0/intermediate/intermediate_act_fn/Constant_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Mul">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Mul</data>
                  <data key="n4">["/encoder/layer.0/intermediate/dense/Add_output_0", "/encoder/layer.0/intermediate/intermediate_act_fn/Add_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Mul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Constant_2">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "AAAAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Constant_2</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Constant_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_intermediate_intermediate_act_fn_Mul_1">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.0/intermediate/intermediate_act_fn/Mul_1</data>
                  <data key="n4">["/encoder/layer.0/intermediate/intermediate_act_fn/Mul_output_0", "/encoder/layer.0/intermediate/intermediate_act_fn/Constant_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/intermediate/intermediate_act_fn/Mul_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="encoder.layer.0.intermediate.dense">
                  <data key="n0">Linear</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/Linear</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 19}</data>
                  <data key="n3">encoder.layer.0.intermediate.dense</data>
                  <graph id="encoder.layer.0.intermediate.dense::" edgedefault="directed">
                    <data key="d0">Linear</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">19</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/Linear</data>
                  </graph>
                </node>
                <node id="encoder.layer.0.intermediate.intermediate_act_fn">
                  <data key="n0">GELUActivation</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 20}</data>
                  <data key="n3">encoder.layer.0.intermediate.intermediate_act_fn</data>
                  <graph id="encoder.layer.0.intermediate.intermediate_act_fn::" edgedefault="directed">
                    <data key="d0">GELUActivation</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">20</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertIntermediate/GELUActivation</data>
                  </graph>
                </node>
              </graph>
            </node>
            <node id="encoder.layer.0.output">
              <data key="n0">BertOutput</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput</data>
              <data key="n2">{"module_type": "torch_module", "execution_order": 21}</data>
              <data key="n3">encoder.layer.0.output</data>
              <graph id="encoder.layer.0.output::" edgedefault="directed">
                <data key="d0">BertOutput</data>
                <data key="d1">torch_module</data>
                <data key="d2">21</data>
                <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertOutput</data>
                <node id="_encoder_layer_0_output_dense_MatMul">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertOutput/Linear"}</data>
                  <data key="n3">/encoder/layer.0/output/dense/MatMul</data>
                  <data key="n4">["/encoder/layer.0/intermediate/intermediate_act_fn/Mul_1_output_0", "onnx::MatMul_314"]</data>
                  <data key="n5">["/encoder/layer.0/output/dense/MatMul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_output_dense_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertOutput/Linear"}</data>
                  <data key="n3">/encoder/layer.0/output/dense/Add</data>
                  <data key="n4">["encoder.layer.0.output.dense.bias", "/encoder/layer.0/output/dense/MatMul_output_0"]</data>
                  <data key="n5">["/encoder/layer.0/output/dense/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_0_output_LayerNorm_LayerNormalization">
                  <data key="n0">LayerNormalization</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput/LayerNorm</data>
                  <data key="n2">{"axis": -1, "epsilon": 9.999999960041972e-13, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.0/BertOutput/LayerNorm"}</data>
                  <data key="n3">/encoder/layer.0/output/LayerNorm/LayerNormalization</data>
                  <data key="n4">["/encoder/layer.0/output/Add_output_0", "encoder.layer.0.output.LayerNorm.weight", "encoder.layer.0.output.LayerNorm.bias"]</data>
                  <data key="n5">["/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="encoder.layer.0.output.dense">
                  <data key="n0">Linear</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput/Linear</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 22}</data>
                  <data key="n3">encoder.layer.0.output.dense</data>
                  <graph id="encoder.layer.0.output.dense::" edgedefault="directed">
                    <data key="d0">Linear</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">22</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertOutput/Linear</data>
                  </graph>
                </node>
                <node id="encoder.layer.0.output.dropout">
                  <data key="n0">Dropout</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput/Dropout</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 23}</data>
                  <data key="n3">encoder.layer.0.output.dropout</data>
                  <graph id="encoder.layer.0.output.dropout::" edgedefault="directed">
                    <data key="d0">Dropout</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">23</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertOutput/Dropout</data>
                  </graph>
                </node>
                <node id="encoder.layer.0.output.LayerNorm">
                  <data key="n0">LayerNorm</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.0/BertOutput/LayerNorm</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 24}</data>
                  <data key="n3">encoder.layer.0.output.LayerNorm</data>
                  <graph id="encoder.layer.0.output.LayerNorm::" edgedefault="directed">
                    <data key="d0">LayerNorm</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">24</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.0/BertOutput/LayerNorm</data>
                  </graph>
                </node>
              </graph>
            </node>
          </graph>
        </node>
        <node id="encoder.layer.1">
          <data key="n0">BertLayer</data>
          <data key="n1">/BertModel/BertEncoder/BertLayer.1</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 25}</data>
          <data key="n3">encoder.layer.1</data>
          <graph id="encoder.layer.1::" edgedefault="directed">
            <data key="d0">BertLayer</data>
            <data key="d1">torch_module</data>
            <data key="d2">25</data>
            <data key="d3">/BertModel/BertEncoder/BertLayer.1</data>
            <node id="_encoder_layer_1_output_Add">
              <data key="n0">Add</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput</data>
              <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertOutput"}</data>
              <data key="n3">/encoder/layer.1/output/Add</data>
              <data key="n4">["/encoder/layer.1/output/dense/Add_output_0", "/encoder/layer.1/attention/output/LayerNorm/LayerNormalization_output_0"]</data>
              <data key="n5">["/encoder/layer.1/output/Add_output_0"]</data>
              <data key="n6" />
            </node>
            <node id="encoder.layer.1.attention">
              <data key="n0">BertAttention</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention</data>
              <data key="n2">{"module_type": "torch_module", "execution_order": 26}</data>
              <data key="n3">encoder.layer.1.attention</data>
              <graph id="encoder.layer.1.attention::" edgedefault="directed">
                <data key="d0">BertAttention</data>
                <data key="d1">torch_module</data>
                <data key="d2">26</data>
                <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention</data>
                <node id="_encoder_layer_1_attention_self_Constant">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAAQAAAAAAAAAAIAAAAAAAAAQAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Reshape">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Reshape</data>
                  <data key="n4">["/encoder/layer.1/attention/self/query/Add_output_0", "/encoder/layer.1/attention/self/Constant_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Reshape_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Transpose">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 1, 3], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Transpose</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Reshape_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Transpose_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Constant_1">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAAQAAAAAAAAAAIAAAAAAAAAQAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant_1</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Reshape_1">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Reshape_1</data>
                  <data key="n4">["/encoder/layer.1/attention/self/key/Add_output_0", "/encoder/layer.1/attention/self/Constant_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Reshape_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Constant_2">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAAQAAAAAAAAAAIAAAAAAAAAQAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant_2</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Reshape_2">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Reshape_2</data>
                  <data key="n4">["/encoder/layer.1/attention/self/value/Add_output_0", "/encoder/layer.1/attention/self/Constant_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Reshape_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Transpose_1">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 1, 3], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Transpose_1</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Reshape_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Transpose_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Shape">
                  <data key="n0">Shape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Shape</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Transpose_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Shape_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Constant_3">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "//////////8=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant_3</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_3_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Constant_4">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "/////////38=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant_4</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_4_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Slice">
                  <data key="n0">Slice</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Slice</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Shape_output_0", "/encoder/layer.1/attention/self/Constant_3_output_0", "/encoder/layer.1/attention/self/Constant_4_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Slice_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Cast">
                  <data key="n0">Cast</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"to": 1, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Cast</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Slice_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Cast_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Sqrt">
                  <data key="n0">Sqrt</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Sqrt</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Cast_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Sqrt_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Constant_5">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [1], "data_b64": "AACAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant_5</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_5_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Div">
                  <data key="n0">Div</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Div</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Constant_5_output_0", "/encoder/layer.1/attention/self/Sqrt_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Div_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Cast_1">
                  <data key="n0">Cast</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"to": 1, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Cast_1</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Div_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Cast_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Transpose_2">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 3, 1], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Transpose_2</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Reshape_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Transpose_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Sqrt_1">
                  <data key="n0">Sqrt</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Sqrt_1</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Cast_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Sqrt_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Mul">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Mul</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Transpose_output_0", "/encoder/layer.1/attention/self/Sqrt_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Mul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Sqrt_2">
                  <data key="n0">Sqrt</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Sqrt_2</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Cast_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Sqrt_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Mul_1">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Mul_1</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Transpose_2_output_0", "/encoder/layer.1/attention/self/Sqrt_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Mul_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_MatMul">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/MatMul</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Mul_output_0", "/encoder/layer.1/attention/self/Mul_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/MatMul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Add</data>
                  <data key="n4">["/encoder/layer.1/attention/self/MatMul_output_0", "/Where_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Softmax">
                  <data key="n0">Softmax</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"axis": -1, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Softmax</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Add_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Softmax_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_MatMul_1">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/MatMul_1</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Softmax_output_0", "/encoder/layer.1/attention/self/Transpose_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/MatMul_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Transpose_3">
                  <data key="n0">Transpose</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"perm": [0, 2, 1, 3], "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Transpose_3</data>
                  <data key="n4">["/encoder/layer.1/attention/self/MatMul_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Transpose_3_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Constant_6">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [3], "data_b64": "AgAAAAAAAAAQAAAAAAAAAIAAAAAAAAAA", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Constant_6</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Constant_6_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_self_Reshape_3">
                  <data key="n0">Reshape</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"allowzero": 0, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention"}</data>
                  <data key="n3">/encoder/layer.1/attention/self/Reshape_3</data>
                  <data key="n4">["/encoder/layer.1/attention/self/Transpose_3_output_0", "/encoder/layer.1/attention/self/Constant_6_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/self/Reshape_3_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_attention_output_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput"}</data>
                  <data key="n3">/encoder/layer.1/attention/output/Add</data>
                  <data key="n4">["/encoder/layer.1/attention/output/dense/Add_output_0", "/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/attention/output/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="encoder.layer.1.attention.self">
                  <data key="n0">BertSdpaSelfAttention</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 27}</data>
                  <data key="n3">encoder.layer.1.attention.self</data>
                  <graph id="encoder.layer.1.attention.self::" edgedefault="directed">
                    <data key="d0">BertSdpaSelfAttention</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">27</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention</data>
                    <node id="_encoder_layer_1_attention_self_query_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/self/query/MatMul</data>
                      <data key="n4">["/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_315"]</data>
                      <data key="n5">["/encoder/layer.1/attention/self/query/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_self_query_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/self/query/Add</data>
                      <data key="n4">["encoder.layer.1.attention.self.query.bias", "/encoder/layer.1/attention/self/query/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.1/attention/self/query/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_self_key_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/self/key/MatMul</data>
                      <data key="n4">["/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_321"]</data>
                      <data key="n5">["/encoder/layer.1/attention/self/key/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_self_key_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/self/key/Add</data>
                      <data key="n4">["encoder.layer.1.attention.self.key.bias", "/encoder/layer.1/attention/self/key/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.1/attention/self/key/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_self_value_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/self/value/MatMul</data>
                      <data key="n4">["/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_327"]</data>
                      <data key="n5">["/encoder/layer.1/attention/self/value/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_self_value_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/self/value/Add</data>
                      <data key="n4">["encoder.layer.1.attention.self.value.bias", "/encoder/layer.1/attention/self/value/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.1/attention/self/value/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="encoder.layer.1.attention.self.value">
                      <data key="n0">Linear</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 30}</data>
                      <data key="n3">encoder.layer.1.attention.self.value</data>
                      <graph id="encoder.layer.1.attention.self.value::" edgedefault="directed">
                        <data key="d0">Linear</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">30</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSdpaSelfAttention/Linear</data>
                      </graph>
                    </node>
                  </graph>
                </node>
                <node id="encoder.layer.1.attention.output">
                  <data key="n0">BertSelfOutput</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 31}</data>
                  <data key="n3">encoder.layer.1.attention.output</data>
                  <graph id="encoder.layer.1.attention.output::" edgedefault="directed">
                    <data key="d0">BertSelfOutput</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">31</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput</data>
                    <node id="_encoder_layer_1_attention_output_dense_MatMul">
                      <data key="n0">MatMul</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/output/dense/MatMul</data>
                      <data key="n4">["/encoder/layer.1/attention/self/Reshape_3_output_0", "onnx::MatMul_337"]</data>
                      <data key="n5">["/encoder/layer.1/attention/output/dense/MatMul_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_output_dense_Add">
                      <data key="n0">Add</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Linear</data>
                      <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Linear"}</data>
                      <data key="n3">/encoder/layer.1/attention/output/dense/Add</data>
                      <data key="n4">["encoder.layer.1.attention.output.dense.bias", "/encoder/layer.1/attention/output/dense/MatMul_output_0"]</data>
                      <data key="n5">["/encoder/layer.1/attention/output/dense/Add_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="_encoder_layer_1_attention_output_LayerNorm_LayerNormalization">
                      <data key="n0">LayerNormalization</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/LayerNorm</data>
                      <data key="n2">{"axis": -1, "epsilon": 9.999999960041972e-13, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/LayerNorm"}</data>
                      <data key="n3">/encoder/layer.1/attention/output/LayerNorm/LayerNormalization</data>
                      <data key="n4">["/encoder/layer.1/attention/output/Add_output_0", "encoder.layer.1.attention.output.LayerNorm.weight", "encoder.layer.1.attention.output.LayerNorm.bias"]</data>
                      <data key="n5">["/encoder/layer.1/attention/output/LayerNorm/LayerNormalization_output_0"]</data>
                      <data key="n6" />
                    </node>
                    <node id="encoder.layer.1.attention.output.dense">
                      <data key="n0">Linear</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Linear</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 32}</data>
                      <data key="n3">encoder.layer.1.attention.output.dense</data>
                      <graph id="encoder.layer.1.attention.output.dense::" edgedefault="directed">
                        <data key="d0">Linear</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">32</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Linear</data>
                      </graph>
                    </node>
                    <node id="encoder.layer.1.attention.output.dropout">
                      <data key="n0">Dropout</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Dropout</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 33}</data>
                      <data key="n3">encoder.layer.1.attention.output.dropout</data>
                      <graph id="encoder.layer.1.attention.output.dropout::" edgedefault="directed">
                        <data key="d0">Dropout</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">33</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/Dropout</data>
                      </graph>
                    </node>
                    <node id="encoder.layer.1.attention.output.LayerNorm">
                      <data key="n0">LayerNorm</data>
                      <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/LayerNorm</data>
                      <data key="n2">{"module_type": "torch_module", "execution_order": 34}</data>
                      <data key="n3">encoder.layer.1.attention.output.LayerNorm</data>
                      <graph id="encoder.layer.1.attention.output.LayerNorm::" edgedefault="directed">
                        <data key="d0">LayerNorm</data>
                        <data key="d1">torch_module</data>
                        <data key="d2">34</data>
                        <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertAttention/BertSelfOutput/LayerNorm</data>
                      </graph>
                    </node>
                  </graph>
                </node>
              </graph>
            </node>
            <node id="encoder.layer.1.intermediate">
              <data key="n0">BertIntermediate</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate</data>
              <data key="n2">{"module_type": "torch_module", "execution_order": 35}</data>
              <data key="n3">encoder.layer.1.intermediate</data>
              <graph id="encoder.layer.1.intermediate::" edgedefault="directed">
                <data key="d0">BertIntermediate</data>
                <data key="d1">torch_module</data>
                <data key="d2">35</data>
                <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertIntermediate</data>
                <node id="_encoder_layer_1_intermediate_dense_MatMul">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/Linear"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/dense/MatMul</data>
                  <data key="n4">["/encoder/layer.1/attention/output/LayerNorm/LayerNormalization_output_0", "onnx::MatMul_338"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/dense/MatMul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_dense_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/Linear"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/dense/Add</data>
                  <data key="n4">["encoder.layer.1.intermediate.dense.bias", "/encoder/layer.1/intermediate/dense/MatMul_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/dense/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Constant">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "8wS1Pw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Constant</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Constant_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Div">
                  <data key="n0">Div</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Div</data>
                  <data key="n4">["/encoder/layer.1/intermediate/dense/Add_output_0", "/encoder/layer.1/intermediate/intermediate_act_fn/Constant_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Div_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Erf">
                  <data key="n0">Erf</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Erf</data>
                  <data key="n4">["/encoder/layer.1/intermediate/intermediate_act_fn/Div_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Erf_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Constant_1">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "AACAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Constant_1</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Constant_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Add</data>
                  <data key="n4">["/encoder/layer.1/intermediate/intermediate_act_fn/Erf_output_0", "/encoder/layer.1/intermediate/intermediate_act_fn/Constant_1_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Mul">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Mul</data>
                  <data key="n4">["/encoder/layer.1/intermediate/dense/Add_output_0", "/encoder/layer.1/intermediate/intermediate_act_fn/Add_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Mul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Constant_2">
                  <data key="n0">Constant</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "AAAAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Constant_2</data>
                  <data key="n4">[]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Constant_2_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_intermediate_intermediate_act_fn_Mul_1">
                  <data key="n0">Mul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation"}</data>
                  <data key="n3">/encoder/layer.1/intermediate/intermediate_act_fn/Mul_1</data>
                  <data key="n4">["/encoder/layer.1/intermediate/intermediate_act_fn/Mul_output_0", "/encoder/layer.1/intermediate/intermediate_act_fn/Constant_2_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/intermediate/intermediate_act_fn/Mul_1_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="encoder.layer.1.intermediate.dense">
                  <data key="n0">Linear</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/Linear</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 36}</data>
                  <data key="n3">encoder.layer.1.intermediate.dense</data>
                  <graph id="encoder.layer.1.intermediate.dense::" edgedefault="directed">
                    <data key="d0">Linear</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">36</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/Linear</data>
                  </graph>
                </node>
                <node id="encoder.layer.1.intermediate.intermediate_act_fn">
                  <data key="n0">GELUActivation</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 37}</data>
                  <data key="n3">encoder.layer.1.intermediate.intermediate_act_fn</data>
                  <graph id="encoder.layer.1.intermediate.intermediate_act_fn::" edgedefault="directed">
                    <data key="d0">GELUActivation</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">37</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertIntermediate/GELUActivation</data>
                  </graph>
                </node>
              </graph>
            </node>
            <node id="encoder.layer.1.output">
              <data key="n0">BertOutput</data>
              <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput</data>
              <data key="n2">{"module_type": "torch_module", "execution_order": 38}</data>
              <data key="n3">encoder.layer.1.output</data>
              <graph id="encoder.layer.1.output::" edgedefault="directed">
                <data key="d0">BertOutput</data>
                <data key="d1">torch_module</data>
                <data key="d2">38</data>
                <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertOutput</data>
                <node id="_encoder_layer_1_output_dense_MatMul">
                  <data key="n0">MatMul</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertOutput/Linear"}</data>
                  <data key="n3">/encoder/layer.1/output/dense/MatMul</data>
                  <data key="n4">["/encoder/layer.1/intermediate/intermediate_act_fn/Mul_1_output_0", "onnx::MatMul_339"]</data>
                  <data key="n5">["/encoder/layer.1/output/dense/MatMul_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_output_dense_Add">
                  <data key="n0">Add</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput/Linear</data>
                  <data key="n2">{"hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertOutput/Linear"}</data>
                  <data key="n3">/encoder/layer.1/output/dense/Add</data>
                  <data key="n4">["encoder.layer.1.output.dense.bias", "/encoder/layer.1/output/dense/MatMul_output_0"]</data>
                  <data key="n5">["/encoder/layer.1/output/dense/Add_output_0"]</data>
                  <data key="n6" />
                </node>
                <node id="_encoder_layer_1_output_LayerNorm_LayerNormalization">
                  <data key="n0">LayerNormalization</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput/LayerNorm</data>
                  <data key="n2">{"axis": -1, "epsilon": 9.999999960041972e-13, "hierarchy_tag": "/BertModel/BertEncoder/BertLayer.1/BertOutput/LayerNorm"}</data>
                  <data key="n3">/encoder/layer.1/output/LayerNorm/LayerNormalization</data>
                  <data key="n4">["/encoder/layer.1/output/Add_output_0", "encoder.layer.1.output.LayerNorm.weight", "encoder.layer.1.output.LayerNorm.bias"]</data>
                  <data key="n5">["last_hidden_state"]</data>
                  <data key="n6" />
                </node>
                <node id="encoder.layer.1.output.dense">
                  <data key="n0">Linear</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput/Linear</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 39}</data>
                  <data key="n3">encoder.layer.1.output.dense</data>
                  <graph id="encoder.layer.1.output.dense::" edgedefault="directed">
                    <data key="d0">Linear</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">39</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertOutput/Linear</data>
                  </graph>
                </node>
                <node id="encoder.layer.1.output.dropout">
                  <data key="n0">Dropout</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput/Dropout</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 40}</data>
                  <data key="n3">encoder.layer.1.output.dropout</data>
                  <graph id="encoder.layer.1.output.dropout::" edgedefault="directed">
                    <data key="d0">Dropout</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">40</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertOutput/Dropout</data>
                  </graph>
                </node>
                <node id="encoder.layer.1.output.LayerNorm">
                  <data key="n0">LayerNorm</data>
                  <data key="n1">/BertModel/BertEncoder/BertLayer.1/BertOutput/LayerNorm</data>
                  <data key="n2">{"module_type": "torch_module", "execution_order": 41}</data>
                  <data key="n3">encoder.layer.1.output.LayerNorm</data>
                  <graph id="encoder.layer.1.output.LayerNorm::" edgedefault="directed">
                    <data key="d0">LayerNorm</data>
                    <data key="d1">torch_module</data>
                    <data key="d2">41</data>
                    <data key="d3">/BertModel/BertEncoder/BertLayer.1/BertOutput/LayerNorm</data>
                  </graph>
                </node>
              </graph>
            </node>
          </graph>
        </node>
      </graph>
    </node>
    <node id="pooler">
      <data key="n0">BertPooler</data>
      <data key="n1">/BertModel/BertPooler</data>
      <data key="n2">{"module_type": "torch_module", "execution_order": 42}</data>
      <data key="n3">pooler</data>
      <graph id="pooler::" edgedefault="directed">
        <data key="d0">BertPooler</data>
        <data key="d1">torch_module</data>
        <data key="d2">42</data>
        <data key="d3">/BertModel/BertPooler</data>
        <node id="_pooler_Gather">
          <data key="n0">Gather</data>
          <data key="n1">/BertModel/BertPooler</data>
          <data key="n2">{"axis": 1, "hierarchy_tag": "/BertModel/BertPooler"}</data>
          <data key="n3">/pooler/Gather</data>
          <data key="n4">["last_hidden_state", "/embeddings/Constant_output_0"]</data>
          <data key="n5">["/pooler/Gather_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_pooler_dense_Gemm">
          <data key="n0">Gemm</data>
          <data key="n1">/BertModel/BertPooler/Linear</data>
          <data key="n2">{"alpha": 1.0, "beta": 1.0, "transB": 1, "hierarchy_tag": "/BertModel/BertPooler/Linear"}</data>
          <data key="n3">/pooler/dense/Gemm</data>
          <data key="n4">["/pooler/Gather_output_0", "pooler.dense.weight", "pooler.dense.bias"]</data>
          <data key="n5">["/pooler/dense/Gemm_output_0"]</data>
          <data key="n6" />
        </node>
        <node id="_pooler_activation_Tanh">
          <data key="n0">Tanh</data>
          <data key="n1">/BertModel/BertPooler/Tanh</data>
          <data key="n2">{"hierarchy_tag": "/BertModel/BertPooler/Tanh"}</data>
          <data key="n3">/pooler/activation/Tanh</data>
          <data key="n4">["/pooler/dense/Gemm_output_0"]</data>
          <data key="n5">["pooler_output"]</data>
          <data key="n6" />
        </node>
        <node id="pooler.dense">
          <data key="n0">Linear</data>
          <data key="n1">/BertModel/BertPooler/Linear</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 43}</data>
          <data key="n3">pooler.dense</data>
          <graph id="pooler.dense::" edgedefault="directed">
            <data key="d0">Linear</data>
            <data key="d1">torch_module</data>
            <data key="d2">43</data>
            <data key="d3">/BertModel/BertPooler/Linear</data>
          </graph>
        </node>
        <node id="pooler.activation">
          <data key="n0">Tanh</data>
          <data key="n1">/BertModel/BertPooler/Tanh</data>
          <data key="n2">{"module_type": "torch_module", "execution_order": 44}</data>
          <data key="n3">pooler.activation</data>
          <graph id="pooler.activation::" edgedefault="directed">
            <data key="d0">Tanh</data>
            <data key="d1">torch_module</data>
            <data key="d2">44</data>
            <data key="d3">/BertModel/BertPooler/Tanh</data>
          </graph>
        </node>
      </graph>
    </node>
    <node id="_Constant">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "AQAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Unsqueeze">
      <data key="n0">Unsqueeze</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Unsqueeze</data>
      <data key="n4">["attention_mask", "/Constant_output_0"]</data>
      <data key="n5">["/Unsqueeze_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Constant_1">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "AgAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant_1</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_1_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Unsqueeze_1">
      <data key="n0">Unsqueeze</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Unsqueeze_1</data>
      <data key="n4">["/Unsqueeze_output_0", "/Constant_1_output_0"]</data>
      <data key="n5">["/Unsqueeze_1_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Constant_2">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [4], "data_b64": "AgAAAAAAAAABAAAAAAAAABAAAAAAAAAAEAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant_2</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_2_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Constant_3">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "BAAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant_3</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_3_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_ConstantOfShape">
      <data key="n0">ConstantOfShape</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [1], "data_b64": "AQAAAAAAAAA=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/ConstantOfShape</data>
      <data key="n4">["/Constant_3_output_0"]</data>
      <data key="n5">["/ConstantOfShape_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Constant_4">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 7, "dims": [], "data_b64": "//////////8=", "numpy_dtype": "int64"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant_4</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_4_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Mul">
      <data key="n0">Mul</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Mul</data>
      <data key="n4">["/ConstantOfShape_output_0", "/Constant_4_output_0"]</data>
      <data key="n5">["/Mul_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Equal">
      <data key="n0">Equal</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Equal</data>
      <data key="n4">["/Constant_2_output_0", "/Mul_output_0"]</data>
      <data key="n5">["/Equal_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Where">
      <data key="n0">Where</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Where</data>
      <data key="n4">["/Equal_output_0", "/ConstantOfShape_output_0", "/Constant_2_output_0"]</data>
      <data key="n5">["/Where_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Expand">
      <data key="n0">Expand</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Expand</data>
      <data key="n4">["/Unsqueeze_1_output_0", "/Where_output_0"]</data>
      <data key="n5">["/Expand_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Cast">
      <data key="n0">Cast</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"to": 1, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Cast</data>
      <data key="n4">["/Expand_output_0"]</data>
      <data key="n5">["/Cast_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Constant_5">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "AACAPw==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant_5</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_5_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Sub">
      <data key="n0">Sub</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Sub</data>
      <data key="n4">["/Constant_5_output_0", "/Cast_output_0"]</data>
      <data key="n5">["/Sub_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Cast_1">
      <data key="n0">Cast</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"to": 9, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Cast_1</data>
      <data key="n4">["/Sub_output_0"]</data>
      <data key="n5">["/Cast_1_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Cast_2">
      <data key="n0">Cast</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"to": 9, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Cast_2</data>
      <data key="n4">["/Cast_1_output_0"]</data>
      <data key="n5">["/Cast_2_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Constant_6">
      <data key="n0">Constant</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"value": {"tensor_type": "onnx_tensor", "name": "", "data_type": 1, "dims": [], "data_b64": "//9//w==", "numpy_dtype": "float32"}, "hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Constant_6</data>
      <data key="n4">[]</data>
      <data key="n5">["/Constant_6_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="_Where_1">
      <data key="n0">Where</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{"hierarchy_tag": "/BertModel"}</data>
      <data key="n3">/Where_1</data>
      <data key="n4">["/Cast_2_output_0", "/Constant_6_output_0", "/Sub_output_0"]</data>
      <data key="n5">["/Where_1_output_0"]</data>
      <data key="n6" />
    </node>
    <node id="input_input_ids">
      <data key="n0">Input</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{}</data>
      <data key="n3">input_ids</data>
      <!-- input_ids: [2, 16] -->
    </node>
    <node id="input_attention_mask">
      <data key="n0">Input</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{}</data>
      <data key="n3">attention_mask</data>
      <!-- attention_mask: [2, 16] -->
    </node>
    <node id="input_token_type_ids">
      <data key="n0">Input</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{}</data>
      <data key="n3">token_type_ids</data>
      <!-- token_type_ids: [2, 16] -->
    </node>
    <node id="output_last_hidden_state">
      <data key="n0">Output</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{}</data>
      <data key="n3">last_hidden_state</data>
      <!-- last_hidden_state: [2, 16, 128] -->
    </node>
    <node id="output_pooler_output">
      <data key="n0">Output</data>
      <data key="n1">/BertModel</data>
      <data key="n2">{}</data>
      <data key="n3">pooler_output</data>
      <!-- pooler_output: [2, 128] -->
    </node>
    <edge source="input_input_ids" target="_embeddings_word_embeddings_Gather">
      <data key="e0">input_ids</data>
      <data key="t0">int64</data>
      <data key="t1">[2, 16]</data>
      <data key="t2" />
    </edge>
    <edge source="input_token_type_ids" target="_embeddings_token_type_embeddings_Gather">
      <data key="e0">token_type_ids</data>
      <data key="t0">int64</data>
      <data key="t1">[2, 16]</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_word_embeddings_Gather" target="_embeddings_Add">
      <data key="e0">/embeddings/word_embeddings/Gather_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_token_type_embeddings_Gather" target="_embeddings_Add">
      <data key="e0">/embeddings/token_type_embeddings/Gather_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_Constant_1" target="_embeddings_position_embeddings_Gather">
      <data key="e0">/embeddings/Constant_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_Add" target="_embeddings_Add_1">
      <data key="e0">/embeddings/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_position_embeddings_Gather" target="_embeddings_Add_1">
      <data key="e0">/embeddings/position_embeddings/Gather_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_Add_1" target="_embeddings_LayerNorm_LayerNormalization">
      <data key="e0">/embeddings/Add_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="input_attention_mask" target="_Unsqueeze">
      <data key="e0">attention_mask</data>
      <data key="t0">int64</data>
      <data key="t1">[2, 16]</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant" target="_Unsqueeze">
      <data key="e0">/Constant_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Unsqueeze" target="_Unsqueeze_1">
      <data key="e0">/Unsqueeze_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_1" target="_Unsqueeze_1">
      <data key="e0">/Constant_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_3" target="_ConstantOfShape">
      <data key="e0">/Constant_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_ConstantOfShape" target="_Mul">
      <data key="e0">/ConstantOfShape_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_4" target="_Mul">
      <data key="e0">/Constant_4_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_2" target="_Equal">
      <data key="e0">/Constant_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Mul" target="_Equal">
      <data key="e0">/Mul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Equal" target="_Where">
      <data key="e0">/Equal_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_ConstantOfShape" target="_Where">
      <data key="e0">/ConstantOfShape_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_2" target="_Where">
      <data key="e0">/Constant_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Unsqueeze_1" target="_Expand">
      <data key="e0">/Unsqueeze_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Where" target="_Expand">
      <data key="e0">/Where_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Expand" target="_Cast">
      <data key="e0">/Expand_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_5" target="_Sub">
      <data key="e0">/Constant_5_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Cast" target="_Sub">
      <data key="e0">/Cast_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Sub" target="_Cast_1">
      <data key="e0">/Sub_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Cast_1" target="_Cast_2">
      <data key="e0">/Cast_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Cast_2" target="_Where_1">
      <data key="e0">/Cast_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Constant_6" target="_Where_1">
      <data key="e0">/Constant_6_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Sub" target="_Where_1">
      <data key="e0">/Sub_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_LayerNorm_LayerNormalization" target="_encoder_layer_0_attention_self_query_MatMul">
      <data key="e0">/embeddings/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_query_MatMul" target="_encoder_layer_0_attention_self_query_Add">
      <data key="e0">/encoder/layer.0/attention/self/query/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_query_Add" target="_encoder_layer_0_attention_self_Reshape">
      <data key="e0">/encoder/layer.0/attention/self/query/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant" target="_encoder_layer_0_attention_self_Reshape">
      <data key="e0">/encoder/layer.0/attention/self/Constant_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Reshape" target="_encoder_layer_0_attention_self_Transpose">
      <data key="e0">/encoder/layer.0/attention/self/Reshape_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_LayerNorm_LayerNormalization" target="_encoder_layer_0_attention_self_key_MatMul">
      <data key="e0">/embeddings/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_key_MatMul" target="_encoder_layer_0_attention_self_key_Add">
      <data key="e0">/encoder/layer.0/attention/self/key/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_key_Add" target="_encoder_layer_0_attention_self_Reshape_1">
      <data key="e0">/encoder/layer.0/attention/self/key/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant_1" target="_encoder_layer_0_attention_self_Reshape_1">
      <data key="e0">/encoder/layer.0/attention/self/Constant_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_LayerNorm_LayerNormalization" target="_encoder_layer_0_attention_self_value_MatMul">
      <data key="e0">/embeddings/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_value_MatMul" target="_encoder_layer_0_attention_self_value_Add">
      <data key="e0">/encoder/layer.0/attention/self/value/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_value_Add" target="_encoder_layer_0_attention_self_Reshape_2">
      <data key="e0">/encoder/layer.0/attention/self/value/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant_2" target="_encoder_layer_0_attention_self_Reshape_2">
      <data key="e0">/encoder/layer.0/attention/self/Constant_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Reshape_2" target="_encoder_layer_0_attention_self_Transpose_1">
      <data key="e0">/encoder/layer.0/attention/self/Reshape_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Transpose" target="_encoder_layer_0_attention_self_Shape">
      <data key="e0">/encoder/layer.0/attention/self/Transpose_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Shape" target="_encoder_layer_0_attention_self_Slice">
      <data key="e0">/encoder/layer.0/attention/self/Shape_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant_3" target="_encoder_layer_0_attention_self_Slice">
      <data key="e0">/encoder/layer.0/attention/self/Constant_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant_4" target="_encoder_layer_0_attention_self_Slice">
      <data key="e0">/encoder/layer.0/attention/self/Constant_4_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Slice" target="_encoder_layer_0_attention_self_Cast">
      <data key="e0">/encoder/layer.0/attention/self/Slice_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Cast" target="_encoder_layer_0_attention_self_Sqrt">
      <data key="e0">/encoder/layer.0/attention/self/Cast_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant_5" target="_encoder_layer_0_attention_self_Div">
      <data key="e0">/encoder/layer.0/attention/self/Constant_5_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Sqrt" target="_encoder_layer_0_attention_self_Div">
      <data key="e0">/encoder/layer.0/attention/self/Sqrt_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Div" target="_encoder_layer_0_attention_self_Cast_1">
      <data key="e0">/encoder/layer.0/attention/self/Div_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Reshape_1" target="_encoder_layer_0_attention_self_Transpose_2">
      <data key="e0">/encoder/layer.0/attention/self/Reshape_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Cast_1" target="_encoder_layer_0_attention_self_Sqrt_1">
      <data key="e0">/encoder/layer.0/attention/self/Cast_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Transpose" target="_encoder_layer_0_attention_self_Mul">
      <data key="e0">/encoder/layer.0/attention/self/Transpose_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Sqrt_1" target="_encoder_layer_0_attention_self_Mul">
      <data key="e0">/encoder/layer.0/attention/self/Sqrt_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Cast_1" target="_encoder_layer_0_attention_self_Sqrt_2">
      <data key="e0">/encoder/layer.0/attention/self/Cast_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Transpose_2" target="_encoder_layer_0_attention_self_Mul_1">
      <data key="e0">/encoder/layer.0/attention/self/Transpose_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Sqrt_2" target="_encoder_layer_0_attention_self_Mul_1">
      <data key="e0">/encoder/layer.0/attention/self/Sqrt_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Mul" target="_encoder_layer_0_attention_self_MatMul">
      <data key="e0">/encoder/layer.0/attention/self/Mul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Mul_1" target="_encoder_layer_0_attention_self_MatMul">
      <data key="e0">/encoder/layer.0/attention/self/Mul_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_MatMul" target="_encoder_layer_0_attention_self_Add">
      <data key="e0">/encoder/layer.0/attention/self/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Where_1" target="_encoder_layer_0_attention_self_Add">
      <data key="e0">/Where_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Add" target="_encoder_layer_0_attention_self_Softmax">
      <data key="e0">/encoder/layer.0/attention/self/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Softmax" target="_encoder_layer_0_attention_self_MatMul_1">
      <data key="e0">/encoder/layer.0/attention/self/Softmax_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Transpose_1" target="_encoder_layer_0_attention_self_MatMul_1">
      <data key="e0">/encoder/layer.0/attention/self/Transpose_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_MatMul_1" target="_encoder_layer_0_attention_self_Transpose_3">
      <data key="e0">/encoder/layer.0/attention/self/MatMul_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Transpose_3" target="_encoder_layer_0_attention_self_Reshape_3">
      <data key="e0">/encoder/layer.0/attention/self/Transpose_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Constant_6" target="_encoder_layer_0_attention_self_Reshape_3">
      <data key="e0">/encoder/layer.0/attention/self/Constant_6_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_self_Reshape_3" target="_encoder_layer_0_attention_output_dense_MatMul">
      <data key="e0">/encoder/layer.0/attention/self/Reshape_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_output_dense_MatMul" target="_encoder_layer_0_attention_output_dense_Add">
      <data key="e0">/encoder/layer.0/attention/output/dense/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_output_dense_Add" target="_encoder_layer_0_attention_output_Add">
      <data key="e0">/encoder/layer.0/attention/output/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_LayerNorm_LayerNormalization" target="_encoder_layer_0_attention_output_Add">
      <data key="e0">/embeddings/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_output_Add" target="_encoder_layer_0_attention_output_LayerNorm_LayerNormalization">
      <data key="e0">/encoder/layer.0/attention/output/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_output_LayerNorm_LayerNormalization" target="_encoder_layer_0_intermediate_dense_MatMul">
      <data key="e0">/encoder/layer.0/attention/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_dense_MatMul" target="_encoder_layer_0_intermediate_dense_Add">
      <data key="e0">/encoder/layer.0/intermediate/dense/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_dense_Add" target="_encoder_layer_0_intermediate_intermediate_act_fn_Div">
      <data key="e0">/encoder/layer.0/intermediate/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Constant" target="_encoder_layer_0_intermediate_intermediate_act_fn_Div">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Constant_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Div" target="_encoder_layer_0_intermediate_intermediate_act_fn_Erf">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Div_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Erf" target="_encoder_layer_0_intermediate_intermediate_act_fn_Add">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Erf_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Constant_1" target="_encoder_layer_0_intermediate_intermediate_act_fn_Add">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Constant_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_dense_Add" target="_encoder_layer_0_intermediate_intermediate_act_fn_Mul">
      <data key="e0">/encoder/layer.0/intermediate/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Add" target="_encoder_layer_0_intermediate_intermediate_act_fn_Mul">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Mul" target="_encoder_layer_0_intermediate_intermediate_act_fn_Mul_1">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Mul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Constant_2" target="_encoder_layer_0_intermediate_intermediate_act_fn_Mul_1">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Constant_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_intermediate_intermediate_act_fn_Mul_1" target="_encoder_layer_0_output_dense_MatMul">
      <data key="e0">/encoder/layer.0/intermediate/intermediate_act_fn/Mul_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_dense_MatMul" target="_encoder_layer_0_output_dense_Add">
      <data key="e0">/encoder/layer.0/output/dense/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_dense_Add" target="_encoder_layer_0_output_Add">
      <data key="e0">/encoder/layer.0/output/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_attention_output_LayerNorm_LayerNormalization" target="_encoder_layer_0_output_Add">
      <data key="e0">/encoder/layer.0/attention/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_Add" target="_encoder_layer_0_output_LayerNorm_LayerNormalization">
      <data key="e0">/encoder/layer.0/output/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_LayerNorm_LayerNormalization" target="_encoder_layer_1_attention_self_query_MatMul">
      <data key="e0">/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_query_MatMul" target="_encoder_layer_1_attention_self_query_Add">
      <data key="e0">/encoder/layer.1/attention/self/query/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_query_Add" target="_encoder_layer_1_attention_self_Reshape">
      <data key="e0">/encoder/layer.1/attention/self/query/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant" target="_encoder_layer_1_attention_self_Reshape">
      <data key="e0">/encoder/layer.1/attention/self/Constant_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Reshape" target="_encoder_layer_1_attention_self_Transpose">
      <data key="e0">/encoder/layer.1/attention/self/Reshape_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_LayerNorm_LayerNormalization" target="_encoder_layer_1_attention_self_key_MatMul">
      <data key="e0">/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_key_MatMul" target="_encoder_layer_1_attention_self_key_Add">
      <data key="e0">/encoder/layer.1/attention/self/key/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_key_Add" target="_encoder_layer_1_attention_self_Reshape_1">
      <data key="e0">/encoder/layer.1/attention/self/key/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant_1" target="_encoder_layer_1_attention_self_Reshape_1">
      <data key="e0">/encoder/layer.1/attention/self/Constant_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_LayerNorm_LayerNormalization" target="_encoder_layer_1_attention_self_value_MatMul">
      <data key="e0">/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_value_MatMul" target="_encoder_layer_1_attention_self_value_Add">
      <data key="e0">/encoder/layer.1/attention/self/value/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_value_Add" target="_encoder_layer_1_attention_self_Reshape_2">
      <data key="e0">/encoder/layer.1/attention/self/value/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant_2" target="_encoder_layer_1_attention_self_Reshape_2">
      <data key="e0">/encoder/layer.1/attention/self/Constant_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Reshape_2" target="_encoder_layer_1_attention_self_Transpose_1">
      <data key="e0">/encoder/layer.1/attention/self/Reshape_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Transpose" target="_encoder_layer_1_attention_self_Shape">
      <data key="e0">/encoder/layer.1/attention/self/Transpose_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Shape" target="_encoder_layer_1_attention_self_Slice">
      <data key="e0">/encoder/layer.1/attention/self/Shape_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant_3" target="_encoder_layer_1_attention_self_Slice">
      <data key="e0">/encoder/layer.1/attention/self/Constant_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant_4" target="_encoder_layer_1_attention_self_Slice">
      <data key="e0">/encoder/layer.1/attention/self/Constant_4_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Slice" target="_encoder_layer_1_attention_self_Cast">
      <data key="e0">/encoder/layer.1/attention/self/Slice_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Cast" target="_encoder_layer_1_attention_self_Sqrt">
      <data key="e0">/encoder/layer.1/attention/self/Cast_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant_5" target="_encoder_layer_1_attention_self_Div">
      <data key="e0">/encoder/layer.1/attention/self/Constant_5_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Sqrt" target="_encoder_layer_1_attention_self_Div">
      <data key="e0">/encoder/layer.1/attention/self/Sqrt_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Div" target="_encoder_layer_1_attention_self_Cast_1">
      <data key="e0">/encoder/layer.1/attention/self/Div_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Reshape_1" target="_encoder_layer_1_attention_self_Transpose_2">
      <data key="e0">/encoder/layer.1/attention/self/Reshape_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Cast_1" target="_encoder_layer_1_attention_self_Sqrt_1">
      <data key="e0">/encoder/layer.1/attention/self/Cast_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Transpose" target="_encoder_layer_1_attention_self_Mul">
      <data key="e0">/encoder/layer.1/attention/self/Transpose_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Sqrt_1" target="_encoder_layer_1_attention_self_Mul">
      <data key="e0">/encoder/layer.1/attention/self/Sqrt_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Cast_1" target="_encoder_layer_1_attention_self_Sqrt_2">
      <data key="e0">/encoder/layer.1/attention/self/Cast_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Transpose_2" target="_encoder_layer_1_attention_self_Mul_1">
      <data key="e0">/encoder/layer.1/attention/self/Transpose_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Sqrt_2" target="_encoder_layer_1_attention_self_Mul_1">
      <data key="e0">/encoder/layer.1/attention/self/Sqrt_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Mul" target="_encoder_layer_1_attention_self_MatMul">
      <data key="e0">/encoder/layer.1/attention/self/Mul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Mul_1" target="_encoder_layer_1_attention_self_MatMul">
      <data key="e0">/encoder/layer.1/attention/self/Mul_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_MatMul" target="_encoder_layer_1_attention_self_Add">
      <data key="e0">/encoder/layer.1/attention/self/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_Where_1" target="_encoder_layer_1_attention_self_Add">
      <data key="e0">/Where_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Add" target="_encoder_layer_1_attention_self_Softmax">
      <data key="e0">/encoder/layer.1/attention/self/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Softmax" target="_encoder_layer_1_attention_self_MatMul_1">
      <data key="e0">/encoder/layer.1/attention/self/Softmax_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Transpose_1" target="_encoder_layer_1_attention_self_MatMul_1">
      <data key="e0">/encoder/layer.1/attention/self/Transpose_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_MatMul_1" target="_encoder_layer_1_attention_self_Transpose_3">
      <data key="e0">/encoder/layer.1/attention/self/MatMul_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Transpose_3" target="_encoder_layer_1_attention_self_Reshape_3">
      <data key="e0">/encoder/layer.1/attention/self/Transpose_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Constant_6" target="_encoder_layer_1_attention_self_Reshape_3">
      <data key="e0">/encoder/layer.1/attention/self/Constant_6_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_self_Reshape_3" target="_encoder_layer_1_attention_output_dense_MatMul">
      <data key="e0">/encoder/layer.1/attention/self/Reshape_3_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_output_dense_MatMul" target="_encoder_layer_1_attention_output_dense_Add">
      <data key="e0">/encoder/layer.1/attention/output/dense/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_output_dense_Add" target="_encoder_layer_1_attention_output_Add">
      <data key="e0">/encoder/layer.1/attention/output/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_0_output_LayerNorm_LayerNormalization" target="_encoder_layer_1_attention_output_Add">
      <data key="e0">/encoder/layer.0/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_output_Add" target="_encoder_layer_1_attention_output_LayerNorm_LayerNormalization">
      <data key="e0">/encoder/layer.1/attention/output/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_output_LayerNorm_LayerNormalization" target="_encoder_layer_1_intermediate_dense_MatMul">
      <data key="e0">/encoder/layer.1/attention/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_dense_MatMul" target="_encoder_layer_1_intermediate_dense_Add">
      <data key="e0">/encoder/layer.1/intermediate/dense/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_dense_Add" target="_encoder_layer_1_intermediate_intermediate_act_fn_Div">
      <data key="e0">/encoder/layer.1/intermediate/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Constant" target="_encoder_layer_1_intermediate_intermediate_act_fn_Div">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Constant_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Div" target="_encoder_layer_1_intermediate_intermediate_act_fn_Erf">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Div_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Erf" target="_encoder_layer_1_intermediate_intermediate_act_fn_Add">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Erf_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Constant_1" target="_encoder_layer_1_intermediate_intermediate_act_fn_Add">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Constant_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_dense_Add" target="_encoder_layer_1_intermediate_intermediate_act_fn_Mul">
      <data key="e0">/encoder/layer.1/intermediate/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Add" target="_encoder_layer_1_intermediate_intermediate_act_fn_Mul">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Mul" target="_encoder_layer_1_intermediate_intermediate_act_fn_Mul_1">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Mul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Constant_2" target="_encoder_layer_1_intermediate_intermediate_act_fn_Mul_1">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Constant_2_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_intermediate_intermediate_act_fn_Mul_1" target="_encoder_layer_1_output_dense_MatMul">
      <data key="e0">/encoder/layer.1/intermediate/intermediate_act_fn/Mul_1_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_output_dense_MatMul" target="_encoder_layer_1_output_dense_Add">
      <data key="e0">/encoder/layer.1/output/dense/MatMul_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_output_dense_Add" target="_encoder_layer_1_output_Add">
      <data key="e0">/encoder/layer.1/output/dense/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_attention_output_LayerNorm_LayerNormalization" target="_encoder_layer_1_output_Add">
      <data key="e0">/encoder/layer.1/attention/output/LayerNorm/LayerNormalization_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_output_Add" target="_encoder_layer_1_output_LayerNorm_LayerNormalization">
      <data key="e0">/encoder/layer.1/output/Add_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_output_LayerNorm_LayerNormalization" target="_pooler_Gather">
      <data key="e0">last_hidden_state</data>
      <data key="t0">float32</data>
      <data key="t1">[2, 16, 128]</data>
      <data key="t2" />
    </edge>
    <edge source="_embeddings_Constant" target="_pooler_Gather">
      <data key="e0">/embeddings/Constant_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_pooler_Gather" target="_pooler_dense_Gemm">
      <data key="e0">/pooler/Gather_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_pooler_dense_Gemm" target="_pooler_activation_Tanh">
      <data key="e0">/pooler/dense/Gemm_output_0</data>
      <data key="t0" />
      <data key="t1">null</data>
      <data key="t2" />
    </edge>
    <edge source="_encoder_layer_1_output_LayerNorm_LayerNormalization" target="output_last_hidden_state">
      <data key="e0">last_hidden_state</data>
      <data key="t0">float32</data>
      <data key="t1">[2, 16, 128]</data>
      <data key="t2" />
    </edge>
    <edge source="_pooler_activation_Tanh" target="output_pooler_output">
      <data key="e0">pooler_output</data>
      <data key="t0">float32</data>
      <data key="t1">[2, 128]</data>
      <data key="t2" />
    </edge>
  </graph>
</graphml>